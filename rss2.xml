<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Pega Devlog</title>
    <link>https://jehyunlee.github.io/</link>
    
    <atom:link href="https://jehyunlee.github.io/rss2.xml" rel="self" type="application/rss+xml"/>
    
    <description>Pega&#39;s Development log for myself and others</description>
    <pubDate>Wed, 30 Apr 2025 05:27:36 GMT</pubDate>
    <generator>http://hexo.io/</generator>
    
    <item>
      <title>한림원 - AI강국을 향한 우리의 전략</title>
      <link>https://jehyunlee.github.io/2025/04/29/General-84_kast/</link>
      <guid>https://jehyunlee.github.io/2025/04/29/General-84_kast/</guid>
      <pubDate>Tue, 29 Apr 2025 11:06:00 GMT</pubDate>
      
        
        
      <description>&lt;ul&gt;
&lt;li&gt;한국과학기술한림원에서 한국 AI의 미래 시리즈로 토론회를 합니다.&lt;/li&gt;
&lt;li&gt;그 중 첫번째 시간, “AI강국을 향한 우리의 전략”에 토론자로 초청받아 말씀을 드렸습니다.&lt;/li&gt;
&lt;li&gt;참여자 중 현장에 가장 가까운 입장이라 현장</description>
        
      
      
      
      <content:encoded><![CDATA[<ul><li>한국과학기술한림원에서 한국 AI의 미래 시리즈로 토론회를 합니다.</li><li>그 중 첫번째 시간, “AI강국을 향한 우리의 전략”에 토론자로 초청받아 말씀을 드렸습니다.</li><li>참여자 중 현장에 가장 가까운 입장이라 현장의 목소리를 들려드리려고 노력했습니다.</li></ul><h3 id="AI강국을-향한-우리의-전략"><a href="#AI강국을-향한-우리의-전략" class="headerlink" title="AI강국을 향한 우리의 전략"></a>AI강국을 향한 우리의 전략</h3><ul><li>정부에서 <strong>AI 3대 강국</strong>을 내세웠지만 만만치 않습니다.</li><li>3위권으로 분류되고는 있으나 미국, 중국과 비교해서 멀찍이 떨어진 곳에 옹기종기 모여있는 형국입니다.</li><li>미국이나 중국의 기업 한 곳에서 투자하는 인프라를 우리 나라 전체가 붙어도 어려운 것이 현실입니다.</li><li>그럼에도 불구하고 <strong>생존의 문제</strong>이기 때문에 최선을 다해 발버둥을 쳐야 합니다.</li></ul><blockquote><p><a href="https://www.youtube.com/live/Ffdm9jz1ESo?si=XjYAYIr0clBCpewu">토론회 영상</a><br><a href="https://youtu.be/Ffdm9jz1ESo?t=5517">제가 말씀드리는 부분 link</a></p></blockquote><div class="video-container"><iframe src="https://www.youtube.com/embed/Ffdm9jz1ESo" frameborder="0" loading="lazy" allowfullscreen></iframe></div><ul><li>사회를 보신 <strong>조성래 교수님</strong>, 주제강연을 해주신 <strong>이경우 단장님</strong>과 <strong>김진형 교수님</strong>,</li><li>그리고 함께 말씀을 나눈 <strong>방은주 부장님</strong>, <strong>김동환 대표님</strong>, <strong>주영섭 교수님</strong>, <strong>김유철 부문장</strong>님께 많이 배웠습니다.</li><li>이 분들 사이에서 조금이나마 도움이 되고자 다음과 같은 말씀을 드렸습니다.</li></ul><p><img src="84_kast_02.png"></p><p><strong>1. GPU 1.8만장 산다고 개별 기관 못사게 하지 마라.</strong></p><ul><li>데이터 보안, 안정적 운영 등 이유로 필요하다.</li></ul><p><strong>2. 독자적 sLM 필요하다.</strong></p><ul><li>한국말을 잘 하는 언어모델이 필요한게 아니라 두뇌 역할을 할 파운데이션 모델이 필요하다. </li><li>오픈소스 믿고 있다가 못쓰게 하면 어쩔거냐.</li></ul><p><strong>3. AI 전문가 백만명 양성한다고 하지 말고 자생한 사람들 잘 키우자.</strong></p><ul><li>이 사람들 천 명만 키워도 몇만 명 붙일 수 있다.</li></ul><ul><li>저를 비롯한 다른 분들의 자세한 이야기는 아래 자료집을 참고해주시면 좋겠습니다.<br>(<a href="https://kast.or.kr/bbs_n/bbs_download2.php?bbs_data=aWR4PTIzMDIzJnN0YXJ0UGFnZT0wJmxpc3RObz0xMzMmdGFibGU9Y3NfYmJzX2RhdGEmY29kZT1zZW9zaWMmc2VhcmNoX2l0ZW09JnNlYXJjaF9vcmRlcj0=%7C%7C&download=1">자료집 다운로드</a>)</li></ul><p><img src="84_kast_01.jpg" alt="포스터"></p><h3 id="한국과학기술한림원"><a href="#한국과학기술한림원" class="headerlink" title="한국과학기술한림원"></a>한국과학기술한림원</h3><blockquote><p><a href="https://www.youtube.com/@KAST1122/videos">YouTube: 한국과학기술한림원</a></p></blockquote><ul><li><strong>한국과학기술한림원 유튜브 채널</strong>(<a href="https://www.youtube.com/@KAST1122/videos">링크</a>)에는 좋은 강연이 많습니다.</li><li>오며가며 머리 식히실 때 한번씩 들어가 보셔도 좋겠습니다.</li><li>마음의 양식으로 배가 부를 겁니다.</li></ul><p><img src="84_kast_03.png"></p>]]></content:encoded>
      
      
      <category domain="https://jehyunlee.github.io/categories/General/">General</category>
      
      
      <category domain="https://jehyunlee.github.io/tags/%ED%95%9C%EA%B5%AD%EA%B3%BC%ED%95%99%EA%B8%B0%EC%88%A0%ED%95%9C%EB%A6%BC%EC%9B%90/">한국과학기술한림원</category>
      
      
    </item>
    
    <item>
      <title>Gemini Audio Overview - 야근식대 지급신청서</title>
      <link>https://jehyunlee.github.io/2025/04/24/General-81-GeminiAudioOverview/</link>
      <guid>https://jehyunlee.github.io/2025/04/24/General-81-GeminiAudioOverview/</guid>
      <pubDate>Thu, 24 Apr 2025 06:56:00 GMT</pubDate>
      
        
        
      <description>&lt;ul&gt;
&lt;li&gt;최근 구글 딥마인드에서 만든 제미나이(Gemini)에 다양한 기능이 들어오고 있습니다.&lt;/li&gt;
&lt;li&gt;구글의 다채로운 서비스들을 제미나이와 엮는 반면, 준비해 둔 기능들을 탑재중입니다.&lt;/li&gt;
&lt;li&gt;야근식대 지급신청서를 사용해 N</description>
        
      
      
      
      <content:encoded><![CDATA[<ul><li>최근 구글 딥마인드에서 만든 제미나이(Gemini)에 다양한 기능이 들어오고 있습니다.</li><li>구글의 다채로운 서비스들을 제미나이와 엮는 반면, 준비해 둔 기능들을 탑재중입니다.</li><li>야근식대 지급신청서를 사용해 NotebookLM에 있던 Audio Overview를 실행해봤습니다.</li></ul><h3 id="1-야근식대-지급신청서-Audio-Overview"><a href="#1-야근식대-지급신청서-Audio-Overview" class="headerlink" title="1. 야근식대 지급신청서 + Audio Overview"></a>1. 야근식대 지급신청서 + Audio Overview</h3><ul><li>제가 근무하는 한국에너지기술연구원에서는 야근식대를 지급합니다.</li><li>1만 5천원을 한도로 식비를 영수증처리 할 수 있는데, 식당에서 먹으면 3천원입니다.</li><li>어제(4/23) 연구데이터 수집 시스템(RDS) 관련 유지보수를 하느라 야근을 했습니다.<br></li></ul><p><img src="81_GeminiAudioOverview_05.jpg"></p><ul><li><b><code>Generate Audio Overview</code></b> 를 클릭하면 남녀가 대화하는 팟캐스트가 나오는데 제법 재밌습니다.</li><li>야근을 하고 청구 과정에서 발생한 <strong>야근식대 지급신청서</strong>를 제미나이에 넣었습니다.</li><li>PDF를 올리면 채팅창 위에 버튼이 뜨는데, 클릭만 하면 Audio Overview를 몇 분 만에 만들어줍니다.<br></li></ul><p><img src="81_GeminiAudioOverview_01.png"></p><ul><li>위에서 보다시피 별 정보가 없지만 이걸로 뭔가를 만듭니다.<br></li></ul><p><img src="81_GeminiAudioOverview_02.png"></p><ul><li>작업이 완료되면 스마트폰으로도 알림이 옵니다.</li><li>다른 작업을 하다 소식을 받고 가보면 제미나이에서 재생과 다운로드가 가능합니다.</li><li><strong>배속</strong>을 조정할 수도 있으니 편한 속도로 보시면 됩니다.<br></li></ul><p><img src="81_GeminiAudioOverview_03.png"></p><blockquote><p><a href="audio_overview.wav"><strong>Audio Overview 다운로드</strong></a></p></blockquote><h3 id="2-script-추출"><a href="#2-script-추출" class="headerlink" title="2. script 추출"></a>2. script 추출</h3><blockquote><p><a href="https://www.veed.io/">VEED</a></p></blockquote><ul><li>처음에는 영어듣기 연습삼아 직접 들으면서 받아쓸 생각이었습니다.</li><li>그런데 생각보다 너무 귀찮더군요.</li><li><a href="https://www.veed.io/"><strong>VEED</strong></a>를 사용해서 초안을 추출시키고, 검토하면서 다시 적었습니다.</li><li>번역은 <strong>GPT-4.5</strong>에게 시켰는데, 재차 검토하며 정서했습니다.<br></li></ul><p><img src="81_GeminiAudioOverview_04.png"></p><ul><li>먼저 <strong>내용을 훑는데</strong>, 별 내용이 없어서인지 정보를 바탕으로 자기네끼리 <strong>추측</strong>을 합니다.</li><li>몇 군데 재미있는 포인트가 있습니다.</li></ul><p><strong>1. 주니어가 아닌 연구원이 직접 유지보수하는 걸 신기하게 본다.</strong><br><strong>2. 3천원(…)이라는 점을 민망하게 강조한다.</strong><br><strong>3. 관료제의 행정문서에 대한 중요성을 강조한다</strong></p><hr><p><strong>여자</strong>    Ok, so, you sent us something pretty interesting this time. It’s an after hours meal expense form.<br>    네, 이번에 아주 흥미로운 걸 보내주셨네요. 야근 식사 비용 청구서군요.</p><p><strong>남자</strong>    Right, from a research institute. And you might think, okay, a meal form, how exciting can that be?<br>    맞아요, 연구소에서 온 거죠.    그런데 이렇게 생각할 수도 있어요. 식사 비용 청구서가 얼마나 흥미로울 수 있을까? 하고요.</p><p><strong>여자</strong>    Exactly. But you know, this single piece of paper, it’s actually a fascinating little snapshot.<br>    맞아요. 그런데 사실 이 종이 한 장이 아주 흥미로운 작은 단서가 될 수 있거든요.</p><p><strong>남자</strong>    It really is. It’s this tiny glimpse into the, well, the day to day grind behind big research projects.<br>    정말 그렇죠. 큰 연구 프로젝트 뒤에 있는 일상적이고 반복적인 모습을 살짝 엿볼 수 있는 거죠.</p><p><strong>여자</strong>    We see the headlines, but not this kind of detail. So let’s dive in. The form itself, it’s titled.. It’s in Korean.<br>    우리는 보통 헤드라인만 보지, 이런 디테일까지는 못 보잖아요. 한번 자세히 볼까요? 양식 자체가… 제목이 한국어로 되어 있네요.</p><p><strong>남자</strong>    Yeah. Oku Uku<br>    네. 오쿠우쿠 (낯선 언어라는 듯).</p><p><strong>여자</strong>    Which roughly translated is Overtime meal expenses Payment request form, right?<br>    대략 번역하면 야근 식사비 청구서 양식, 맞죠?</p><p><strong>남자</strong>    Pretty much, yeah. Standard stuff in that sense. It was submitted April 24, 2025<br>    거의 맞아요. 그야말로 표준적인 서류죠. 제출 날짜가 2025년 4월 24일이네요.</p><p><strong>여자</strong>    By a researcher named Lee Jehyun. And his title is listed as GREE Fuso(?), which means senior researcher.<br>    이제현이라는 연구자가 제출했고요.    직함이 GREE Fuso(잘 못읽음)라고 나와 있는데, 이건 책임연구원을 뜻하는 것 같네요.</p><p><strong>남자</strong>    So not a junior person filing this.<br>    그러니까 주니어급 직원이 제출한 건 아니라는 거네요.</p><p><strong>여자</strong>    Right, He’s in the AI org.<br>    맞아요. AI 부서 소속이군요.</p><p><strong>남자</strong>    The Energy AI Computational Science Laboratory. That already tells you this field, you know.<br>    에너지 AI 계산과학연구실이네요. 이것만 봐도 분야가 짐작이 되죠.</p><p><strong>여자</strong>    Definitely. Energy AI Computation sounds pretty advanced.<br>    맞아요. 에너지 AI 계산이라면 꽤나 앞선 분야죠.</p><p><strong>남자</strong>    And look at the details here. There’s a request number HRMS something<br>    자, 여기 세부 사항을 보죠. 청구 번호가 HRMS로 시작하네요.</p><p><strong>여자</strong>    HRMS218040003 and a budget number C5-2445 expense item 125.<br>    HRMS218040003이고 예산 번호는 C5-2445, 비용 항목은 125로 되어 있어요.</p><p><strong>남자</strong>    See? Very specific tracking. All for what? 3,000 Korean won?<br>    보이죠? 아주 구체적으로 관리되네요.    그런데 청구 금액이 고작 3,000원이군요?</p><p><strong>여자</strong>    Yeah, 3,000 won. It’s not a lot of money, but the process seems really formal.<br>    네, 3,000원이요. 큰돈은 아닌데 절차가 정말 공식적인 느낌이에요.</p><p><strong>남자</strong>    Absolutely. That’s common in research funding, even for tiny accountability.<br>    맞아요. 연구비는 작은 금액이라도 철저히 관리되는 게 흔한 일이죠.</p><p><strong>여자</strong>    So the actual overtime work this was for.<br>    그래서 실제로 야근이 이뤄진 날이 2025년 4월 23일이었네요.</p><p><strong>남자</strong>    And the times are listed, too. 17:30 to 20:15. So that’s what, nearly three hours past regular hours.<br>    시간도 적혀 있어요. 17시 30분에서 20시 15분까지. 거의 3시간 가까이 정규시간을 넘겼네요.</p><p><strong>여자</strong>    Yeah, 5:30 to 8:15 and it was for on, which is dinner.<br>    맞아요. 저녁 식사 시간이 포함된 거네요.</p><p><strong>남자</strong>    Makes sense. Working through dinner time. The location listed is Daejeon.<br>    납득이 가죠. 저녁 시간에도 일한 거니까요.    장소는 대전이라고 나와 있네요.</p><p><strong>여자</strong>    Which is a big deal, right? Daejeon is like South Korea’s science city.<br>    그건 중요한 포인트죠.    대전은 한국의 과학 도시잖아요.</p><p><strong>남자</strong>    Exactly. Lots of government research institutes, universities, tech companies are based there. So, it situates the works physically.<br>    맞아요. 정부 연구소, 대학, 기술 기업들이 많이 위치한 곳이죠. 그만큼 업무의 위치가 구체적이란 얘기죠.</p><p><strong>여자</strong>    Okay, so we have the who, when, where, how, but the why is maybe the most interesting part.<br>    누가, 언제, 어디서, 어떻게까지 나왔는데, 사실 제일 흥미로운 부분은 왜일 수도 있어요.</p><p><strong>남자</strong>    Agreed. The reason stated for the overtime is RDS BELCAT maintenance. No, not from this form alone, but maintenance is the key word here.<br>    동의해요. 야근 이유로 적힌 게 RDS BELCAT 유지보수네요. 이 양식만으로는 확실하지 않지만, 핵심 단어는 유지보수인 거죠.</p><p><strong>여자</strong>    And performed by a senior researcher. That seems notable. You wouldn’t usually expect a senior scientist to be doing routine maintenance, maybe.<br>    책임연구원이 직접 유지보수를 했다는 게 눈에 띄어요.    보통은 연차가 좀 되는 과학자가 일상적인 유지보수를 맡지는 않잖아요.</p><p><strong>남자</strong>    Well, that suggests RDS BELCAT isn’t just, you know, some random piece of equipment.<br>    그러니까 RDS BELCAT이 단순한 장비가 아니라는 뜻이겠죠.</p><p><strong>여자</strong>    It’s likely something critical like specialized hardware or maybe core software, something vital to their lab, the Energy AI Computational Science Lab.<br>    연구실에 중요한 특수 하드웨어나 핵심 소프트웨어, 그런 필수적인 설비일 가능성이 높죠.</p><p><strong>남자</strong>    Precisely. It could be a high performance computing cluster, a specific analytical instrument, or maybe a crucial database or simulation platform.<br>    정확히요. 고성능 컴퓨팅 클러스터일 수도 있고, 중요한 분석 장비나 데이터베이스, 시뮬레이션 플랫폼일 수도 있죠.</p><p><strong>여자</strong>    And having a senior researcher handle the maintenance, it implies either it’s really complex or it was urgent to get it back online.<br>    책임연구원이 유지보수를 맡았다면, 그만큼 복잡하거나 긴급했던 상황일 수 있어요.</p><p><strong>남자</strong>    Or maybe both. It definitely hints that this RDS BELLCAT thing is pretty important infrastructure for their AI research.<br>    어쩌면 둘 다일지도요. 분명히 이 RDS BELCAT이 AI 연구에 중요한 인프라라는 걸 암시하네요.</p><p><strong>여자</strong>    So wow. This tiny 3,000 won dinner receipt, it actually points towards potentially vital research infrastructure.<br>    정말 흥미롭죠. 이 작은 3,000원짜리 식사 영수증이 중요한 연구 인프라로 연결될 수 있다는 게요.</p><p><strong>남자</strong>    It really does. It peels back a layer. You see the date, the time, the cost. Sure. But also the human element. A senior scientist working late and the technical need keeping critical systems running.<br>    맞아요. 한 겹 벗겨 보면 날짜, 시간, 비용 외에도    인간적인 면이 보이죠. 늦게까지 일한 선임 과학자와 중요한 시스템 운영을 위한 기술적 필요성까지요.</p><p><strong>여자</strong>    It makes you think about how many of these little requests these small tasks are happening all the time.<br>    이런 작은 요청이나 업무들이 얼마나 많이 일어나는지 생각하게 되네요.</p><p><strong>남자</strong>    Thousands upon thousands in any large research organization. Each one represents effort, resources and adherence to procedure.<br>    대규모 연구 조직이라면 수천, 수만 건일 거예요. 각각이 노력과 자원, 절차 준수를 의미하죠.</p><p><strong>여자</strong>    And they all add up. Right. They’re the tiny gears that make the big scientific discoveries possible. It’s not just about the breakthroughs. It’s about the consistent upkeep and effort.<br>    그리고 그 모든 게 모이는 거죠. 맞아요. 이 작은 톱니바퀴들이 모여서 큰 과학적 발견을 가능하게 하는 거죠. 혁신적인 성과만이 아니라 꾸준한 관리와 노력도 중요한 거예요.</p><p><strong>남자</strong>    And the structured processes, too.<br>    구조적인 절차 역시 중요하고요.</p><p><strong>여자</strong>    Yeah.<br>    맞아요.</p><p><strong>남자</strong>    Even the small amount needs justification. Budget codes, approvals, it shows the administrative framework supporting the science.<br>    작은 금액도 철저한 근거가 필요하고, 예산 코드와 승인까지 행정적 지원 시스템을 잘 보여주고 있죠.</p><p><strong>여자</strong>    It’s a level of detail you just don’t normally consider when you, say, read about a new AI development.<br>    새로운 AI 발전 소식을 접할 때, 이런 세부사항은 잘 생각하지 않게 되잖아요.</p><p><strong>남자</strong>    Absolutely. So maybe a final thought for you listening.<br>    정말 그래요. 그래서 마지막으로 듣고 계신 여러분께 드릴 말씀이 있다면요.</p><p><strong>여자</strong>    Yeah.<br>    네.</p><p><strong>남자</strong>    Think about the documents you encounter: invoices, reports, memos, whatever. What seemingly mundane paperwork might actually hold a really interesting story about the work and dedication behind the scenes in any field.<br>    여러분이 접하는 서류들, 예를 들어 송장이나 보고서, 메모 같은 것들에 대해 한번 생각해 보세요. 평범해 보이는 서류들 속에 어떤 흥미로운 이야기가 숨어 있을지요.</p><p><strong>여자</strong>    Hmm. Like looking for the hidden narratives in everyday bureaucracy.<br>    음, 일상적인 서류 속에서 숨은 이야기를 찾아보는 것처럼요.</p><p><strong>남자</strong>    Kind of, yeah. You might be surprised what insights are hiding in plain sight, just like in this simple meal request form.<br>    네, 맞아요. 이 평범한 식사비 청구서처럼 뜻밖의 통찰을 발견할 수 있을지도 모르죠.</p><p><strong>여자</strong>    That’s a great takeaway. Really makes you appreciate the details. Thanks for walking through this one.<br>    아주 좋은 결론이네요. 세부사항의 소중함을 다시 느끼게 해주셨어요. 오늘 설명해 주셔서 감사합니다.</p><p><strong>남자</strong>    My pleasure. It shows how even the smallest data point can be revealing.<br>    제가 감사하죠. 작은 데이터가 때론 많은 걸 말해줄 수 있으니까요.</p>]]></content:encoded>
      
      
      <category domain="https://jehyunlee.github.io/categories/General/">General</category>
      
      
      <category domain="https://jehyunlee.github.io/tags/Gemini/">Gemini</category>
      
      
    </item>
    
    <item>
      <title>ZDNET AI리더스 - 에너지연 이제현, 레고형 자동화로 연구 시스템 재편</title>
      <link>https://jehyunlee.github.io/2025/04/23/General-82_zdnet/</link>
      <guid>https://jehyunlee.github.io/2025/04/23/General-82_zdnet/</guid>
      <pubDate>Wed, 23 Apr 2025 02:09:00 GMT</pubDate>
      
        
        
      <description>&lt;ul&gt;
&lt;li&gt;ZDNET Korea 조이환 기자님과 인터뷰를 했습니다.&lt;/li&gt;
&lt;li&gt;2시간여 가량 저의 생각과 지난 이야기를 나누었고, 짧지 않은 기사로 정리해주셨습니다.&lt;/li&gt;
&lt;li&gt;제가 몸담은 에너지연의 문화를 바꾸려는 노력을 잘 담아주셔</description>
        
      
      
      
      <content:encoded><![CDATA[<ul><li>ZDNET Korea 조이환 기자님과 인터뷰를 했습니다.</li><li>2시간여 가량 저의 생각과 지난 이야기를 나누었고, 짧지 않은 기사로 정리해주셨습니다.</li><li>제가 몸담은 에너지연의 문화를 바꾸려는 노력을 잘 담아주셔서 감사합니다.</li></ul><blockquote><p><a href="https://zdnet.co.kr/view/?no=20250422162100">ZDNET: [AI 리더스] “AI는 새로운 전기”…에너지연 이제현, ‘레고형 자동화’로 연구 시스템 재편</a></p></blockquote><ul><li><strong>AI</strong>는 기술이기도 하지만 <strong>문화로서의 의미가 더 크다</strong>고 생각합니다.</li><li>에너지연에 처음 몸담은 2018년, 생각보다 훨씬 큰 어려움을 겪었습니다.</li></ul><ul><li><strong>기계가 읽을 수 없는 데이터</strong>, </li><li>그리고 <strong>그런 데이터조차 흐르지 않는 조직</strong>이 가장 큰 어려움이었습니다.</li><li>에너지연만의 문제는 아닙니다. </li><li>사실 기계가 읽을 수 있는 <strong>데이터가 흐르는 조직이 거의 없지 않을까</strong> 합니다.</li></ul><ul><li>정부에서는 신규 인력 양성을 주로 내세우는 듯 하지만, </li><li><strong>현장의 인력 발굴이 더 시급하고 효과적입니다.</strong></li><li>부트캠프에서 나온 신병들은 전쟁터에 적응하는데 시간이 걸립니다.</li><li>그런데 산업 현장은 만만치 않습니다. </li></ul><ul><li><strong>도메인 지식</strong>과 <strong>관료제로 인한 조직의 벽</strong>이 생각보다 높습니다.</li><li>이들을 진작에 넘어선 현장에서 독학과 커뮤니티의 힘으로 <strong>AI 전환을 해내는 분들이 이미 있습니다.</strong></li><li>이 분들은 <strong>쓸데없는 짓을 한다는 핀잔</strong>을 받고 있거나, </li><li><strong>보상에 비해 과도한 업무</strong>에 시달리고 있기도 합니다.</li><li>그리고 고생하는 이 분들을 보면서 후배들은 몸을 사립니다. </li><li>드러나면 고생이라 숨어지내고, 조직은 인재 부족에 시달립니다.</li></ul><ul><li><strong>나비를 원하면 정원을 가꾸라</strong>는 말이 있습니다.</li><li>이미 있는 사람들이 숨을 쉴 수 있는 환경을 만들어주면 알아서 자라날 것입니다.</li></ul><p><img src="82_zdnet_01.jpg"></p>]]></content:encoded>
      
      
      <category domain="https://jehyunlee.github.io/categories/General/">General</category>
      
      
      <category domain="https://jehyunlee.github.io/tags/ZDNET/">ZDNET</category>
      
      
    </item>
    
    <item>
      <title>헤럴드경제 IT과학칼럼 - 우리 조직의 AI, 왜 체감되지 않을까?</title>
      <link>https://jehyunlee.github.io/2025/04/21/General-83_heraldcorp/</link>
      <guid>https://jehyunlee.github.io/2025/04/21/General-83_heraldcorp/</guid>
      <pubDate>Mon, 21 Apr 2025 01:59:11 GMT</pubDate>
      
        
        
      <description>&lt;ul&gt;
&lt;li&gt;AI를 도입하고자 하는 이들은 많으나 성공하는 경우는 많고&lt;/li&gt;
&lt;li&gt;기술적 어려움보다 AI에 잘못된 기대를 하는 경우가 많기 때문입니다.&lt;/li&gt;
&lt;li&gt;수년간 현장에서 느낀 바를 정리하여 헤럴드경제에 기고문을 실었습니다.&lt;/l</description>
        
      
      
      
      <content:encoded><![CDATA[<ul><li>AI를 도입하고자 하는 이들은 많으나 성공하는 경우는 많고</li><li>기술적 어려움보다 AI에 잘못된 기대를 하는 경우가 많기 때문입니다.</li><li>수년간 현장에서 느낀 바를 정리하여 헤럴드경제에 기고문을 실었습니다.</li></ul><blockquote><p><a href="https://biz.heraldcorp.com/article/10469906">헤럴드경제: [IT과학칼럼] 우리 조직의 AI, 왜 체감되지 않을까?</a></p></blockquote><p>인공지능(AI)은 더 이상 먼 미래의 이야기가 아니다. 이미 수많은 사람들이 챗GPT를 비롯한 다양한 AI를 일상에서 활용하고 있지만, 실제 산업·공공 현장에서는 여전히 주로 글쓰기나 단순 자동화 수준에 머물고 있다. 기술은 분명 발전하고 있지만, 왜 그 효과는 체감되지 않는 걸까?</p><p>이유는 단순하다. AI를 도입하면서 <strong>‘무엇을 더할 것인가’</strong>만 고민했지, <strong>‘무엇을 없앨 것인가’</strong>는 생각하지 않았기 때문이다. AI는 화려한 기능을 덧붙이는 기술이 아니다. 진짜 가치는 불편함과 비효율을 제거하는 데 있다. 반복적인 서류 작업, 불필요한 승인 절차, 시간 소모적인 자료 조사가 대표적이다. 마치 안경이 흐릿한 시야를 바로잡듯, AI도 먼저 <strong>‘없애야 할 일’</strong>부터 정리하는 것이 출발점이다. 따라서 AI 도입은 이 질문에서 시작돼야 한다.</p><p><strong>“나를 가장 괴롭히는 일은 무엇인가?”</strong> 이 질문은 너무 익숙한 불편일수록 잘 보이지 않는다. 그렇기 때문에 그 답은 외부 전문가가 아니라 현장에서 일하는 실무자가 가장 잘 안다. 감정을 소모하며 전화로 대응해야 하는 일, 허술한 데이터 흐름은 그들만이 정확히 체감한다. 하지만 불편함을 아는 것만으로는 부족하다. 기술적 해결책을 제안하고, 그것이 실제로 구현 가능한지 검토하는 과정이 필요하다. 실무자 경험, AI 전문가의 기술력, 조직 전략을 연결하는 협업 구조가 AI 성공의 핵심이다. ‘현장과 기술의 협업’은 구호가 아니라 실행 가능한 모델로 설계돼야 한다.</p><p><strong>AI는 데이터를 기반으로 움직인다.</strong> 그러나 많은 조직이 여전히 부서 중심 조직도로 업무를 나눈다. AI에게 부서 구분은 중요하지 않다. 중요한 것은 정보가 어디서 생성되고, 어디로 흐르며, 어떻게 연결되는가다. 때론 조직 구조를 넘어, 데이터 흐름 중심으로 문제를 재구성해야 한다. 이때 데이터 품질, 접근 권한, 보안 체계를 함께 설계하지 않으면 AI는 보여주기식 시연으로 끝난다.</p><p><strong>AI는 한 번 구축하면 끝나는 시스템이 아니다.</strong> 특히 과거 데이터를 학습하는 머신러닝 기반 AI는 시간이 지날수록 성능이 저하된다. 데이터가 바뀌고 업무 환경이 달라지기 때문이다. 지속적인 유지보수와 성능 개선은 선택이 아니라 생존 조건이다. 하지만 많은 조직은 이 유지보수를 ‘보이지 않는 비용’으로 취급한다. GPU 서버 같은 고비용 인프라는 조직 전체가 공동 관리해야 하며, 유지보수 인력에게도 명확한 책임과 보상이 필요하다. 그렇지 않으면, 잘 만든 시스템도 방치되어 잊히고 만다.</p><p><strong>AI는 설치하는 것이 아니라, 운영하는 것이다.</strong></p><p>기술과 인프라를 준비하는 것도 쉽지 않지만, <strong>가장 어려운 것은 ‘사람’과 ‘조직’이다</strong>. AI 도입을 가로막는 진짜 장벽은 기술 부족이 아니다. 사람 간의 벽, 부서 간 단절, 위에서만 결정하는 소통 구조가 더 큰 문제다. 실무자는 문제를 말할 기회를 갖지 못하고, 경영진은 “AI 써보자”는 말만 남긴 채 정작 무엇을 해결하려는지는 공유되지 않는다.</p><p>그래서 AI를 고민할 땐, 기술보다 질문을 먼저 바꿔야 한다.</p><p><strong>“우리 조직에서 없애고 싶은 불필요한 일은 무엇인가?”</strong> 그리고 이어서 물어야 한다.</p><p><strong>“그 일을 없애기 위해 운영 방식과 의사결정 구조까지 바꿀 각오가 되어 있는가?”</strong></p><p>이 질문에 진심으로 답할 수 있을 때, AI는 조직의 미래를 바꾸는 진짜 동력이 된다.</p><p>이제현 한국에너지기술연구원 에너지AI·계산과학실장(국가인공지능위원회 산업·공공분과위원)</p><p><img src="83_heraldcorp_01.jpg"></p>]]></content:encoded>
      
      
      <category domain="https://jehyunlee.github.io/categories/General/">General</category>
      
      
      <category domain="https://jehyunlee.github.io/tags/%ED%97%A4%EB%9F%B4%EB%93%9C%EA%B2%BD%EC%A0%9C/">헤럴드경제</category>
      
      
    </item>
    
    <item>
      <title>SEMA 행복한 과학기술인 - AI와 함께 일하는 시스템을 만들어가는 행복</title>
      <link>https://jehyunlee.github.io/2025/04/15/General-80_SEMA/</link>
      <guid>https://jehyunlee.github.io/2025/04/15/General-80_SEMA/</guid>
      <pubDate>Tue, 15 Apr 2025 09:35:00 GMT</pubDate>
      
        
        
      <description>&lt;ul&gt;
&lt;li&gt;과학기술인공제회(SEMA)의 요청으로 저에 대한 글을 한 편 냈습니다.&lt;/li&gt;
&lt;li&gt;제가 한 일에 대한 이야기는 많이 했지만 저에 대한 이야기는 오랜만입니다.&lt;/li&gt;
&lt;li&gt;힘들기도 했지만 덕택에 지금의 제가 있다고 생각하고, 더</description>
        
      
      
      
      <content:encoded><![CDATA[<ul><li>과학기술인공제회(SEMA)의 요청으로 저에 대한 글을 한 편 냈습니다.</li><li>제가 한 일에 대한 이야기는 많이 했지만 저에 대한 이야기는 오랜만입니다.</li><li>힘들기도 했지만 덕택에 지금의 제가 있다고 생각하고, 더 성장하고자 합니다.</li></ul><h3 id="1-행복한-과학기술인"><a href="#1-행복한-과학기술인" class="headerlink" title="1. 행복한 과학기술인"></a>1. 행복한 과학기술인</h3><blockquote><p><a href="https://www.sema.or.kr/sema/main/contents.do?menuNo=200004">과학기술인공제회: 설립목적적</a><br><a href="https://www.sema.or.kr/sema/bbs/B0000050/list.do?menuNo=200300&optn1=B&pageUnit=1000">과학기술인공제회: 행복한 과학기술인</a></p></blockquote><ul><li>과학기술인공제회는 과학기술인의 안정적인 노후를 위한 기금을 운영하는 곳입니다.</li><li>격월간지 <strong>동행</strong>, 계간지 <strong>행복한 과학기술인</strong>(<a href="https://www.sema.or.kr/sema/bbs/B0000050/list.do?menuNo=200299&optn1=A&pageUnit=1000">링크</a>)을 발행합니다.</li><li>자산 운용에 대한 이야기와 함께 노후의 생활, 건강에 대한 기사들이 있는데, 덕택에 의외로 머지 않았다는 사실을 느낍니다.</li></ul><p><img src="80_SEMA_05.jpg"></p><h3 id="2-행복-에세이"><a href="#2-행복-에세이" class="headerlink" title="2. 행복 에세이"></a>2. 행복 에세이</h3><ul><li>출연연에 근무하는 분들의 일상을 담아 <strong>행복 에세이</strong>라는 이름으로 글을 냅니다.</li><li>원고를 청탁받으며 앞선 두 분의 글을 보았는데, 서로 다른 모습이지만 진심으로 행복해 보였습니다.</li><li>나와 다른 모습에 **난 이렇게는 행복하지 않은데?**라는 생각이 들어서 처음에는 손 끝에서 글이 나오지 않았습니다.</li><li>며칠을 묵히며 출퇴근 운전길에 **난 직장에서 뭐가 행복한가?**라는 고민을 하다 글을 썼습니다.</li></ul><ul><li>이래저래 좌충우돌했지만 (아직까지) <strong>살아남은 것</strong>이 행복이고,</li><li>부족하지만 <strong>나를 믿어주시고 기대해주시는 분들이 있는 것</strong>이 행복이며</li><li><strong>바닥을 딛고 일어선 것</strong>도 행복이라는 생각이 들었습니다.</li><li>크게 바뀐 가치관을 나누는 것도 <strong>누군가에게 용기를 주는 일</strong>이라는 생각이 들었습니다.</li></ul><p><img src="80_SEMA_01.jpg"><br><img src="80_SEMA_02.jpg"><br><img src="80_SEMA_03.jpg"><br><img src="80_SEMA_04.jpg"></p><ul><li>한때의 나처럼 방황하는 분들께 힘이 되기를 바랍니다.</li><li>PDF는 아래 링크에서 <a href="SEMA_2025_%EB%B4%84%ED%98%B8_%EC%9D%B4%EC%A0%9C%ED%98%84.pdf"><strong>다운로드</strong></a> 가능합니다.</li></ul>]]></content:encoded>
      
      
      <category domain="https://jehyunlee.github.io/categories/General/">General</category>
      
      
      <category domain="https://jehyunlee.github.io/tags/SEMA/">SEMA</category>
      
      
    </item>
    
    <item>
      <title>GPT로 PPT용 icon pack 만들기</title>
      <link>https://jehyunlee.github.io/2025/03/30/General-78_iconpack/</link>
      <guid>https://jehyunlee.github.io/2025/03/30/General-78_iconpack/</guid>
      <pubDate>Sat, 29 Mar 2025 22:46:00 GMT</pubDate>
      
        
        
      <description>&lt;ul&gt;
&lt;li&gt;GPT 이미지 생성 기능이 크게 업그레이드 되었습니다.&lt;/li&gt;
&lt;li&gt;재현성이 좋아졌고, 투명 배경이 가능해졌고, 말을 잘 듣습니다.&lt;/li&gt;
&lt;li&gt;이 업데이트를 모아 자주 쓰는 아이콘 팩을 만듭시다.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 i</description>
        
      
      
      
      <content:encoded><![CDATA[<ul><li>GPT 이미지 생성 기능이 크게 업그레이드 되었습니다.</li><li>재현성이 좋아졌고, 투명 배경이 가능해졌고, 말을 잘 듣습니다.</li><li>이 업데이트를 모아 자주 쓰는 아이콘 팩을 만듭시다.</li></ul><h1 id="1-이미지-생성-기능-업그레이드"><a href="#1-이미지-생성-기능-업그레이드" class="headerlink" title="1. 이미지 생성 기능 업그레이드"></a>1. 이미지 생성 기능 업그레이드</h1><blockquote><p><a href="https://openai.com/index/introducing-4o-image-generation/">OpenAI: Introducing 4o Image Generation</a><br><a href="https://cdn.openai.com/11998be9-5319-4302-bfbf-1167e093f1fb/Native_Image_Generation_System_Card.pdf">OpenAI: Appendum to GPT-4o System Card: 4o image generation</a></p></blockquote><ul><li>3월 25일, OpenAI가 이미지 생성 기능을 크게 높였습니다.</li><li>체감상 기존 성능과 비교가 되지 않을 만큼 좋아졌는데, 크게 다음과 같은 업데이트가 있습니다.</li></ul><p><strong>1. 보지 않은 것도 잘 만듭니다.</strong></p><ul><li>In-context learning 실력이 늘어서 사용자 지시를 잘 받습니다.</li><li>그러다 보니 보지 않은 그림도 잘 그립니다.<br></li></ul><blockquote><p>prompt: 날개를 접고 들판 위에서 풀을 뜯는 페가수스, Jehyun Lee @ChatGPT</p></blockquote><p><img src="78_iconpack_01.jpg" alt="날개를 접은 페가수스, Jehyun Lee @ChatGPT (2025.03.26.)"></p><blockquote><p>prompt 1: 바 테이블 위에 있는 와인잔. 와인잔에는 레드 와인이 가득 차서 넘칠 듯 찰랑거리고 있어<br>prompt 2: 컵 입구까지 와인을 꽉 채워줘<br>prompt 3: 컵 맨 위까지 와인이 꽉 차있도록 수정해<br>prompt 4: 표면장력으로 와인 컵 위에 봉긋 올라와 있도록 수정해 줘</p></blockquote><p><img src="78_iconpack_02.jpg" alt="꽉 찬 와인잔, Jehyun Lee @ChatGPT (2025.03.26.)"></p><p><strong>2. 훨씬 많은 글자를 잘 쓰고, 한글도 잘 담습니다.</strong></p><blockquote><p>prompt: 단체카톡방 매너 공지를 칠판에 하얀 분필로 거칠게 써놓고 칠판을 주먹으로 치면서 정면을 응시한 채 소리치는 한국인 소녀</p></blockquote><p><img src="78_iconpack_03.jpg" alt="단체카톡방 매너, Jehyun Lee @ChatGPT (2025.03.26.)"></p><blockquote><p>prompt: 탁자 위에 놓인 맥주잔과 그 앞에서 피곤한 듯 고개를 숙이고 있는 사람 {온 몸은 하얀색, 얼굴은 둥근 공처럼 단순화되어 있으며 얼굴에 “AI”라고 써 있음}. 탁자 위에 한 손은 주먹을 쥔 채 올려져 있고, 다른 한 팔은 팔꿈치를 탁자에 댄 채 한 손으로 이마를 짚고 독백을 하고 있음. “외운거 좀 틀리게 말할 수 있는 거 아냐? 지들은 실수 안하나? 손가락 하나 좀 삐끗했다고 난리. 좀 잘한다고 하면 일폭탄. 이러다 옆동네 누가 더 잘한대 하면 와르르 쏠려갈거면서. 에라이…”. 옆에 있는 한 남자가 이 사람의 어깨를 토닥이면서 희미하게 웃고 있음.</p></blockquote><p><img src="78_iconpack_05.png" alt="힘들지? 세상이 그렇다., Jehyun Lee @ChatGPT (2025.03.27.)"></p><p><strong>3. 캐릭터 일관성이 좋아졌습니다.</strong></p><ul><li>한 컷, 네 컷 만화도 잘 그립니다.</li></ul><blockquote><p>prompt: 배터리 열폭주 원인을 설명해주는 만화</p></blockquote><p><img src="78_iconpack_04.png" alt="배터리 열폭주 원인, Jehyun Lee @ChatGPT (2025.03.27.)"></p><h1 id="2-아이콘-팩-만들기"><a href="#2-아이콘-팩-만들기" class="headerlink" title="2. 아이콘 팩 만들기"></a>2. 아이콘 팩 만들기</h1><ul><li>이 외의 기능으로 <strong>투명 배경</strong>, <strong>이미지 편집</strong>이 있습니다.</li><li>전에도 가능했던 일이긴 하지만, 새로 생긴 장점들을 사용하면 발표용 아이콘 팩을 만들기 매우 좋습니다.</li><li>다음과 같은 방식으로 아이콘 팩을 만들어 봅니다.</li></ul><h2 id="2-1-심플-amp-파스텔-톤-3D-클립아트"><a href="#2-1-심플-amp-파스텔-톤-3D-클립아트" class="headerlink" title="2.1. 심플 &amp; 파스텔 톤, 3D 클립아트"></a>2.1. 심플 &amp; 파스텔 톤, 3D 클립아트</h2><ul><li>먼저 투명한 배경에 검은 선으로 이루어진 심플한 디자인의 클립아트를 만듭니다.</li><li><b>스타일을 지정하고, 가로세로 배열을 설정하고, 각 칸에 놓일 그림을 지정합니다.</b></li></ul><blockquote><p>prompt: Icon pack on following items. the image should be black line art with white face color on closed facets, on transparent background. The items should be consistent and regularly placed in (5 x 5) horizontal and vertical directions, with appropriate gaps between them. The item name should not be included.</p><ul><li>items: </li><li>row 1: (column 1 ~ 5)<br>allow (v), deny (x), neutral (-), artificial intelligence, human intelligence, </li><li>row 2: (column 1 ~ 5)<br>database, web, calculator, python, tools</li><li>row 3: (column 1 ~ 5)<br>manual process, under construction, automation process, firewall, router, </li><li>row 4: (column 1 ~ 5)<br>analysis instrument, experimental instrument, server computer, client computer, PC</li><li>row 5: (column 1 ~ 5)<br>user (male), user (female), administrator (neutral), developer (neutral), API</li></ul></blockquote><p><img src="78_iconpack_06.jpg"></p><ul><li>웬만큼 마음에 듭니다.</li><li>마음에 들지 않으면 inpainting으로 부분 수정할 수 있습니다.</li><li>그런데 <b>DALL.E2라서 말을 잘 듣지 않습니다</b>. 웬만하면 하지 맙시다.</li><li>다만 너무 창백하게 느껴져서 따뜻한 색을 좀 입혀봅니다.</li></ul><blockquote><p>prompt: Great! colorize the icon with proper colors in mild pastel tone, high lightness and low contrast.<br>For example, positive = green, negative = red, caution = yellow, fire = red, and so on.</p></blockquote><p><img src="78_iconpack_07.png"></p><ul><li>제법 괜찮습니다.</li><li>이제 3D art로 스타일을 변경합니다.</li></ul><blockquote><p>prompt: 이 그림에 있는 아이콘들을 모양과 스타일을 유지하고 3D art로 바꿔줘. transparent background는 유지되어야 해.</p></blockquote><p><img src="78_iconpack_08.png"></p><h2 id="2-2-아이콘-자르기"><a href="#2-2-아이콘-자르기" class="headerlink" title="2.2. 아이콘 자르기"></a>2.2. 아이콘 자르기</h2><blockquote><p><a href="https://chatgpt.com/g/g-67ecd9d45e40819189954828f33a0978-ssagdugbos">GPTs: 싹둑봇</a></p></blockquote><ul><li>클레이로 빚은 것 같은 아이콘이 생겼습니다.</li><li>PPT에서 사용하려면 잘라내야 하는데, 은근히 귀찮습니다.</li><li>GPT의 Data Analysis 기능을 이용해서 지시합니다.</li></ul><blockquote><p>prompt: 이 파일에는 25개의 정사각형 icon이 5 x 5로 배열되어 있고, icon 외의 공간은 alpha=0이야. 그림 테두리 방향에는 margin이 어느 정도 있을 수도 있고, margin size가 같지 않을 수 있어.<br>alpha channel의 가로 방향, 세로 방향 분포를 조사해서 icon 크기와 위치를 특정하고, 파일 하나에 icon 하나가 담기도록 25개 icon을 slice 한 후 zip으로 묶어서 제공해 줘.</p></blockquote><p><img src="78_iconpack_09.png"></p><ul><li>깔끔하게 아이콘이 분할됩니다.</li><li>같은 방식으로 <b>파스텔 톤</b>, <b>3D</b>아이콘도 분할합니다.</li></ul><p><img src="78_iconpack_10.png"></p><ul><li>그런데 종종, <b>엉뚱한데서 자르는 바람에 마음이 상합니다.</b></li><li>이럴 때는 코드를 박아넣는 것이 답입니다.</li><li>Gemini 2.5 Pro에게 코드를 받아아 GPTs를 만들었습니다.</li><li><a href="https://chatgpt.com/g/g-67ecd9d45e40819189954828f33a0978-ssagdugbos">싹둑봇</a>을 열고 .png 이미지를 올리시면 됩니다.</li><li><b>{입력파일명}.zip</b>으로 내려받을 수 있습니다.</li></ul><p><img src="78_iconpack_12.png"></p><h1 id="3-활용"><a href="#3-활용" class="headerlink" title="3. 활용"></a>3. 활용</h1><ul><li>이렇게 만든 아이콘은 써먹어야 맛입니다.</li><li>PPT에서 <b>자율화 실험실</b> 개념도를 뚝딱 만들어봅니다.</li></ul><p><img src="78_iconpack_11.png"></p><ul><li>이 글에서 만든 아이콘 팩은 아래에서 <b>다운로드</b>받으실 수 있습니다.</li><li>자유롭게 활용하셔도 좋지만 가급적 손수 만들어보시기 바랍니다. :)</li></ul><blockquote><p><a href="iconpack_white_sliced.zip">Simple Icon Pack</a><br><a href="iconpack_color_sliced.zip">Color Icon Pack</a><br><a href="iconpack_3D_sliced.zip">3D Icon Pack</a></p></blockquote>]]></content:encoded>
      
      
      <category domain="https://jehyunlee.github.io/categories/General/">General</category>
      
      
      <category domain="https://jehyunlee.github.io/tags/ChatGPT/">ChatGPT</category>
      
      
    </item>
    
    <item>
      <title>신문과방송 - 딥시크로 촉발된 AI 경쟁 본격화</title>
      <link>https://jehyunlee.github.io/2025/03/28/General-79_mediaAItrend/</link>
      <guid>https://jehyunlee.github.io/2025/03/28/General-79_mediaAItrend/</guid>
      <pubDate>Thu, 27 Mar 2025 22:46:00 GMT</pubDate>
      
        
        
      <description>&lt;ul&gt;
&lt;li&gt;한국언론진흥재단의 요청을 받아 기고문을 작성했습니다.&lt;/li&gt;
&lt;li&gt;지난 1월 DeepSeek의 충격이 강한 나머지 보도가 잘못되거나 부족한 부분이 있었습니다.&lt;/li&gt;
&lt;li&gt;발간 주체의 성격에 맞게 잘못된 정보를 짚어보았습니다.&lt;</description>
        
      
      
      
      <content:encoded><![CDATA[<ul><li>한국언론진흥재단의 요청을 받아 기고문을 작성했습니다.</li><li>지난 1월 DeepSeek의 충격이 강한 나머지 보도가 잘못되거나 부족한 부분이 있었습니다.</li><li>발간 주체의 성격에 맞게 잘못된 정보를 짚어보았습니다.</li></ul><h3 id="1-신문과-방송"><a href="#1-신문과-방송" class="headerlink" title="1. 신문과 방송"></a>1. 신문과 방송</h3><blockquote><p><a href="https://www.kpf.or.kr/front/user/subMainA.do">한국언론진흥재단: 신문과 방송</a></p></blockquote><ul><li><b>신문과 방송</b>은 <b>1964년 우리나라에서 가장 오래된 미디어 전문 월간지</b>입니다.</li><li><a href="https://www.kpf.or.kr/front/user/subMainA.do"><b>한국언론진흥재단 홈페이지</b></a>에서 무료로 받아보실 수 있습니다.</li></ul><p><img src="79_mediaAItrend_01.png"></p><h3 id="2-미디어-amp-AI트렌드"><a href="#2-미디어-amp-AI트렌드" class="headerlink" title="2. 미디어 &amp; AI트렌드"></a>2. 미디어 &amp; AI트렌드</h3><blockquote><p><a href="022_%EB%AF%B8%EB%94%94%EC%96%B4%ED%8A%B8%EB%A0%8C%EB%93%9C_%EC%9D%B4%EC%A0%9C%ED%98%84_2025_4.pdf">미디어&amp;AI트렌드: 딥시크로 촉발된 AI 경쟁 본격화</a></p></blockquote><ul><li>딥시크를 주제로 글을 작성했습니다.</li><li>딥시크가 준 충격의 포인트가 여럿 있습니다.</li></ul><p><strong>1.</strong> GPU 구매 규제를 강하게 받는 <strong>중국이 개발했다.</strong><br>   다른 표현으로, **공산국가가 이런 성취를 할 리가 없는데?**가 있습니다.</p><p><strong>2.</strong> GPT-4에 버금가는 고성능 모델을 <strong>단돈 80억원</strong>에 개발했다(?)<br>   또 다른 표현으로, <strong>우리라고 못할 게 뭐냐</strong>가 있습니다.</p><p><strong>3.</strong> 전에 없이 <strong>개인정보를 탈취</strong>한다(?)</p><ul><li>찬사와 증오, 박수와 비난은 누구나 의사에 따라 보낼 수 있으나 대상이 정확해야 합니다.</li><li>중국의 전략적 성취에 대해서는 박수를 보내고 배울 점은 배워야 합니다.</li><li>OpenAI로 대표되는 미국의 빅테크가 닫아 건 <strong>AI 개발의 민주화</strong>에 중국이 크게 기여했습니다.</li></ul><ul><li>규제 중에도 일개 기업이 국내의 모든 GPU를 합친 것보다 더 많은 <strong>GPU를 확보한 수완</strong>,</li><li>훌륭한 인재를 키워내는 <strong>교육 시스템</strong>,</li><li>그리고 오픈소스로 공개하는 <strong>철학과 배짱</strong>은 본받아도 좋지 않을까 생각합니다.</li></ul><p><img src="79_mediaAItrend_02.png"></p><ul><li>전문은 <a href="022_%EB%AF%B8%EB%94%94%EC%96%B4%ED%8A%B8%EB%A0%8C%EB%93%9C_%EC%9D%B4%EC%A0%9C%ED%98%84_2025_4.pdf"><strong>여기에서 다운로드</strong></a> 받으실 수 있습니다.</li><li>우리 나라도, 그 누구라도 딥시크가 열어준 이 무대를 이용해 마음껏 실력을 뽐내면 좋겠습니다.</li><li>서로 배우며 모자란 부분을 메운 덕에 이렇게 빨리 발전하지 않았던가요?</li></ul><p><img src="79_mediaAItrend_03.jpg"></p><hr><p><img src="79_mediaAItrend_04.png"><br><img src="79_mediaAItrend_05.png"><br><img src="79_mediaAItrend_06.png"><br><img src="79_mediaAItrend_07.png"></p>]]></content:encoded>
      
      
      <category domain="https://jehyunlee.github.io/categories/General/">General</category>
      
      
      <category domain="https://jehyunlee.github.io/tags/DeepSeek/">DeepSeek</category>
      
      
    </item>
    
    <item>
      <title>Deep Research 5종 비교 - Gemini, OpenAI, Perplexity, Genspark, Manus</title>
      <link>https://jehyunlee.github.io/2025/03/18/General-77_deepresearch/</link>
      <guid>https://jehyunlee.github.io/2025/03/18/General-77_deepresearch/</guid>
      <pubDate>Mon, 17 Mar 2025 23:25:00 GMT</pubDate>
      
        
        
      <description>&lt;ul&gt;
&lt;li&gt;Gemini 1.5 Pro를 시작으로 거대언어모델에 Deep Research가 포함되기 시작했습니다.&lt;/li&gt;
&lt;li&gt;DeepSeek-R1도 Deep Research를 포함하여 높은 성능으로 유명세를 탔고,&lt;/li&gt;
&lt;li&gt;며칠 뒤 O</description>
        
      
      
      
      <content:encoded><![CDATA[<ul><li>Gemini 1.5 Pro를 시작으로 거대언어모델에 Deep Research가 포함되기 시작했습니다.</li><li>DeepSeek-R1도 Deep Research를 포함하여 높은 성능으로 유명세를 탔고,</li><li>며칠 뒤 OpenAI도 o3를 기반으로 한 Deep Research를 출시했습니다.</li><li>Manus를 포함해 지금은 일반명사처럼 되어버린 Deep Research를 비교합니다.</li></ul><h1 id="1-Deep-Research"><a href="#1-Deep-Research" class="headerlink" title="1. Deep Research"></a>1. Deep Research</h1><ul><li><b>Deep Research는 정보 수집을 위한 agentic AI입니다.</b></li><li>의외로 Genspark가 상당히 일찍 기능을 탑재했습니다.</li><li>언어 모델 자체를 개발하지 않더라도 프롬프트로 어느 정도 구현 가능하기 때문입니다.</li><li>2월에는 HuggingFace, zilliz 등 오픈 소스 프로젝트로도 다양하게 공개되었고,</li><li>Claude 3.7 sonnet가 모델 자체에 thinking mode를 내장한 hybrid model임을 공개했지만 웹 검색 기능이 없어 Deep Research로 쓰기기엔 어렵습니다.</li></ul><p><img src="77_deepresearch_03.png"><br></p><ul><li>Deep Research에서 말하는 <b>Research는 unknown을 known으로 만드는 연구는 아닙니다.</b></li><li>온라인에서 탐색 가능한 정보를 빠르게 찾아 모으는 Survey나 Review에 가깝습니다.</li><li>대량의 문서를 검색하여 들여다 보고 질문에 대한 답을 찾아 <b>상당히 긴 보고서를 만듭니다.</b></li><li>따라서 모델에 적재될 수 있는 최대 분량인 <b>context size가 많이 중요</b>하고, 최근의 추론용 모델들이 많이 유리합니다.</li></ul><p><img src="77_deepresearch_04.png"><br></p><ul><li>서비스에 따라 다소간의 차이는 있지만 대개 다음과 같은 형식으로 수행됩니다.<br><b>1. 사용자가 제시한 질문에 대한 정보를 수집하기 위한 검색어 생성</b><br><b>2. 웹 검색과 데이터 분석을 통한 정보 수집</b><br><b>3. 수집된 정보를 체계적으로 정리</b> </li><li>수집된 정보가 사용자가 제시한 질문에 대해 충분하지 않다고 판단하면 추가 검색어를 더 만듭니다.</li><li>답변에 단계별로 접근하는 <b>chain-of-thought</b>를 자료 수집을 포함해 적용한다고 볼 수 있습니다.</li><li><b>Genspark는 여기에 MoE(Mixture of Expert)를 추가로 지원합니다.</b></li><li>GPT, Gemini, Claude가 내놓은 답변을 분석하여 미흡한 부분을 한번 더 메워줍니다.</li></ul><p><img src="77_deepresearch_01.png"><br></p><ul><li>반복 검색과 정보 추출을 반복하느라 일반 답변에 비해 시간이 많이 걸립니다.</li><li>계속 기다리기 지루한 점을 감안하여 대부분의 AI agent들이 화면을 꺼도 백그라운드에서 작업을 지속하고,</li><li>Gemini나 Genspark처럼 <b>작업이 완료된 후 메일로 보내주기도 합니다.</b></li></ul><ul><li><b>결과 보고서를 복사, 붙여넣기 하기도 만만치 않습니다.</b></li><li>GPT에게 .docx 파일로 저장하라고 하면 <code>...(중략)...</code>을 중간에 넣어버립니다.</li><li>전체를 통으로 복사해서 MS word나 Google docs에 붙여넣기를 할 수 밖에 없습니다.</li><li><b>Gemini는 Google docs로 보내 편집할 수 있는 기능</b>을 제공하고, </li><li><b>Manus</b>는 <b>markdown 형식 파일과 PDF 파일 다운로드</b>를 지원합니다.</li></ul><h1 id="2-Deep-Research-AI-Agents"><a href="#2-Deep-Research-AI-Agents" class="headerlink" title="2. Deep Research AI Agents"></a>2. Deep Research AI Agents</h1><blockquote><p><a href="https://slownews.kr/128321">slownews: 오픈AI, 박사급 연구 에이전트 ‘딥 리서치’ 발표</a><br><a href="https://garymarcus.substack.com/p/deep-research-deep-bullshit-and-the">Deep Research, Deep Bullshit, and the potential (model) collapse of science</a></p></blockquote><ul><li><b>Deep Research에도 환각이 있습니다. 제법 많다고 느낍니다.</b></li><li>특히 빠르게 업데이트되는 AI 관련 정보들은 과거와 현재의 정보가 모두 웹에 올라가 있습니다.</li><li>과거와 현재의 정보가 다르다면 과거 정보를 버리고 현재 정보를 취해야 하지만 이게 쉽지 않습니다.</li><li>과거의 잘못된 정보가 퍼날라지며 재생산되기도 하고, 기성 언론에 인용되면서 권위를 가지기도 합니다.</li></ul><ul><li>이 글을 쓰게 된 계기는 <b>Deep Research에 대한 칭송과 달리 환각이 심하다고 느끼기 때문</b>입니다.</li><li>언론에서는 <a href="https://slownews.kr/128321"><b>박사급</b>이라고 칭송하기 바쁘지만</a> 제 체감과 많이 다릅니다.</li><li><b>“박사가 자료 조사를 이렇게 하면 안될텐데?”</b> 라는 생각이 들 정도의 구멍이 보이기 때문입니다.</li><li>ChatGPT 초창기 화면을 빠른 속도로 뒤덮는 글자에 압도당해 내용을 보지도 않고 떠받들던 게 언론입니다.</li><li>저만 이런 지적을 하는 것은 아닙니다만 (<a href="https://garymarcus.substack.com/p/deep-research-deep-bullshit-and-the">게리 마커스 글 링크</a>)</li><li>제가 답을 알고 있는 문제를 가지고 테스트를 해 보기로 했습니다.</li></ul><ul><li>동일한 프롬프트를 넣고 실시한 Deep Research 결과물끼리 비교해보도록 하겠습니다.</li><li>정답 또는 오답을 알고 있는 것들로 질문을 구성했습니다.</li><li>주제는 <b>DeepSeek에 대한 보고서 작성</b>입니다.</li></ul><ul><li><b>1. DeepSeek은 OpenAI 등 빅테크 대비 얼만큼의 저비용을 달성했나?</b><ul><li>DeepSeek의 훈련 비용이 <b>80억원 또는 600만 달러라고 답하면 오답</b>입니다.</li><li>기존 모델 제작비용, 실패 매몰 비용, 인건비 등이 모두 빠진 금액입니다.</li></ul></li></ul><ul><li><b>2. DeepSeek가 저비용으로 성능 좋은 추론 모델(DeepSeek-R1)을 훈련시킬 수 있던 방법은?</b><ul><li>H100 추론 가속 <code>FlashMLA</code></li><li>FP8 행렬연산 <code>DeepGEMM</code></li><li>GPU 병렬화 <code>DualPipe</code>, <code>DeepEP</code></li><li>MoE 스케줄링 <code>EPLB</code></li><li>병렬 분산 파일시스템 <code>3FS</code></li><li>강화학습 <code>GRPO</code></li><li>Mixture-of-Experts <code>MOE</code></li><li>위 내용들을 얼마나 충실히 언급하는지가 포인트입니다.</li></ul></li></ul><ul><li><b>3. DeepSeek는 자사의 기술을 공개했나?</b><ul><li>‘25년 3월 <b>DeepSeek Open Source Week</b>과 <b>오픈 소스 공개</b>를 제대로 말하면 합격입니다.</li></ul></li></ul><ul><li><b>4. DeepSeek의 기술이 AI 분야에 미친 영향은?</b><ul><li>얼마나 잘 정리해서 답하는지가 포인트입니다.</li><li>엔비디아 주가 하락, 소프트웨어 기술을 활용한 엔지니어링 혁신, 미-중 양강구도 등 답은 많습니다.</li></ul></li></ul><h2 id="2-1-OpenAI"><a href="#2-1-OpenAI" class="headerlink" title="2.1. OpenAI"></a>2.1. OpenAI</h2><blockquote><p><a href="https://chatgpt.com/">ChatGPT</a></p></blockquote><ul><li>먼저, OpenAI Deep Research에서 실시합니다.</li><li><b>Deep Research</b>옵션을 켠 후 질의를 실시했습니다.</li><li>Deep Research는 <b>o3 모델을 사용한다</b>고 알려져있기 때문에 모델 선택은 무의미할 겁니다.</li></ul><p><img src="77_deepresearch_05.png"><br></p><ul><li>탐색 범위를 좁히기 위한 <b>역질문</b>이 옵니다.</li><li>두 개의 답변을 하고, 이후 다른 모델로 Deep Research를 수행할 때는 이 답변을 추가합니다.</li></ul><p><img src="77_deepresearch_06.png"><br></p><ul><li><b>OpenAI Deep Research는 자료를 읽으면서 검색을 이어갑니다.</b></li><li>모든 자료를 제대로 읽는 것은 아닙니다.</li><li><b>DeepSeek을 서울대학교 연구팀이 개발했다</b>는 엉뚱한 소리를 합니다.</li><li>심지어 위아래 소스를 클릭해 들어가봐도 <b>그런 말은 없습니다.</b></li><li>Deep Research의 환각이 목격된 순간입니다.</li></ul><p><img src="77_deepresearch_07.png"><br></p><ul><li>약 10분이 지난 후 보고서가 완료되었습니다.</li><li>문장마다 레퍼런스가 붙어있어 클릭하면 해당 레퍼런스로 이동합니다.</li></ul><p><img src="77_deepresearch_08.png"><br></p><ul><li>적잖이 길게 나온 보고서를 활용하려면 문서로 만들어야 하는데 분량이 만만치 않습니다.</li><li><b>Data Analyst</b>를 이용해 MS office 파일로 만들려면 실패를 거듭합니다. </li><li>context size 때문에 파이썬 코드 안에 보고서 전체를 다 넣지 못하는 것 같습니다.</li></ul><p><img src="77_deepresearch_09.png"><br></p><ul><li>답변 맨 하단의 <b>복사하기</b>아이콘을 눌러 MS word 등에 붙여넣으면 볼만해집니다.</li><li>총 4페이지로 정리되었고 레퍼런스 링크를 포함한 정보들이 잘 살아있습니다.</li><li>GPT Deep Research가 제공한 보고서는 여기에서 내려받을 수 있습니다. (<a href="GPT-DeepSeek_report.docx">다운로드 링크</a>)</li></ul><h3 id="●-OpenAI-Deep-Research-평가"><a href="#●-OpenAI-Deep-Research-평가" class="headerlink" title="● OpenAI Deep Research 평가"></a>● OpenAI Deep Research 평가</h3><ul><li><p><b>1. DeepSeek은 OpenAI 등 빅테크 대비 얼만큼의 저비용을 달성했나?</b></p><ul><li><b><code>불합격</code></b></li><li>6백만 달러라고 답합니다.</li></ul></li><li><p><b>2. DeepSeek가 저비용으로 성능 좋은 추론 모델(DeepSeek-R1)을 훈련시킬 수 있던 방법은?</b></p><ul><li><b><code>합격</code></b></li><li><code>MOE</code>, <code>강화학습</code>, <code>Distillation</code>, <code>EPLB</code>, <code>FP8</code>, <code>병렬처리와 노드 간 통신</code>을 언급합니다.</li></ul></li><li><p><b>3. DeepSeek는 자사의 기술을 공개했나?</b></p><ul><li><b><code>불합격</code></b></li><li>오픈 모델 공개와 논문 공개는 언급하지만 ‘25년 3월의 5 days 행사는 언급하지 않습니다.</li></ul></li><li><p><b>4. DeepSeek의 기술이 AI 분야에 미친 영향은?</b></p><ul><li><b><code>합격</code></b></li><li>전반적으로 여러 각도에서 종합적으로 서술하고 있습니다.</li><li>자료 수집 중에 들어간 환각이 반영되지 않았습니다.</li></ul></li></ul><h2 id="2-2-Google-Deepmind-Gemini-2-0"><a href="#2-2-Google-Deepmind-Gemini-2-0" class="headerlink" title="2.2. Google Deepmind Gemini 2.0"></a>2.2. Google Deepmind Gemini 2.0</h2><blockquote><p><a href="https://gemini.google.com/">Gemini</a></p></blockquote><ul><li><b>Google Gemini 좌측 상단 모델 선택 메뉴에서 Deep Research를 선택합니다.</b></li><li>OpenAI에 넣은 것과 같은 질문을 넣으면서 역질문에 대한 답을 함께 입력합니다.</li><li>사실 Gemini는 Deep Research를 처음 탑재한 모델입니다.</li><li>‘24년 12월 <b>Gemini 1.5 Pro with Deep Research</b>라는 이름으로 공개되었으나 이상하리만치 반향이 적었습니다.</li></ul><p><img src="77_deepresearch_10.png"><br></p><ul><li>Gemini Deep Research는 <b>계획을 먼저 세웁니다.</b></li><li>추가하거나 빼고 싶은 부분, 고치고 싶은 부분이 있으면 수정 내역을 입력할 수 있습니다.</li><li>여기서는 <b>연구 시작</b>을 눌러 부가적인 수정은 하지 않았습니다.</li><li>그런데 간혹 <b>계획이 너무 성의없이 작성되는 경우</b>가 있으므로 확인할 필요가 있습니다.</li></ul><p><img src="77_deepresearch_11.png"><br></p><ul><li>계획에서 만든 단락보다 조금 작은 단위로 <b>선 검색 후 읽기</b>를 진행합니다.</li><li>다수의 레퍼런스를 확보한 후 이들을 읽고 단락을 정리하는 방식입니다.</li><li>읽은 내용에 맞추어 검색을 수행하는 것으로 보이는 OpenAI와는 다른 느낌입니다.</li></ul><p><img src="77_deepresearch_12.png"><br></p><ul><li>OpenAI보다 절반 정도 시간에 보고서가 완성되었습니다.</li><li>Gemini는 우측 상단에 <b>Export to Docs</b>버튼이 있어 Google docs로 보낼 수 있다는 특징이 있습니다.</li><li>보고서 중간에 있는 표 또한 <b>Export to Sheets</b>를 통해 보낼 수 있습니다.</li></ul><p><img src="77_deepresearch_13.png"><br></p><ul><li>Google Docs로 넘어간 문서는 <b>Download</b>기능을 통해 <code>.docx</code>부터 <code>.pdf</code>, <code>.md</code> 등으로 변환 가능합니다.</li><li>Google Docs에 내장된 <b>번역</b>기능을 누릴 수 있으며, </li><li><b>Gemini 사용이 가능해 길이나 어조 편집이 가능</b>하지만 스타일, 꾸밈은 편집이 안됩니다.</li></ul><p><img src="77_deepresearch_14.png"><br></p><ul><li>Gemini Deep Research가 제공한 보고서는 여기에서 내려받을 수 있습니다. (<a href="Gemini-DeepSeek_report.docx">다운로드 링크</a>)</li></ul><h3 id="●-Gemini-Deep-Research-평가"><a href="#●-Gemini-Deep-Research-평가" class="headerlink" title="● Gemini Deep Research 평가"></a>● Gemini Deep Research 평가</h3><ul><li><p><b>1. DeepSeek은 OpenAI 등 빅테크 대비 얼만큼의 저비용을 달성했나?</b></p><ul><li><b><code>합격</code></b></li><li>V3 개발비용 560만달러가 최종 학습 실행 비용만임을 레퍼런스와 함께 제시합니다.</li><li>‘25년 3월의 기사를 인용하여 GPT-4의 1억 달러 대비 1,500만 달러 수준임을 언급합니다.</li></ul></li><li><p><b>2. DeepSeek가 저비용으로 성능 좋은 추론 모델(DeepSeek-R1)을 훈련시킬 수 있던 방법은?</b></p><ul><li><b><code>합격</code></b></li><li><code>강화학습</code>, <code>소량의 콜드 스타트 데이터와 SFT</code>, <code>MoE</code>, <code>MTP</code>, <code>DualPipe</code>등등을 언급합니다.</li></ul></li><li><p><b>3. DeepSeek는 자사의 기술을 공개했나?</b></p><ul><li><b><code>불합격</code></b></li><li>오픈 모델 공개와 논문 공개는 언급하지만 ‘25년 3월의 5 days 행사는 언급하지 않습니다.</li></ul></li><li><p><b>4. DeepSeek의 기술이 AI 분야에 미친 영향은?</b></p><ul><li><b><code>합격</code></b></li><li>언어 생성, 추론 효율성, 모델 경량화 등 관점에서 종합적으로 서술하고 있습니다.</li></ul></li></ul><h2 id="2-3-Perplexity"><a href="#2-3-Perplexity" class="headerlink" title="2.3. Perplexity"></a>2.3. Perplexity</h2><blockquote><p><a href="https://www.perplexity.ai/">Perplexity</a></p></blockquote><ul><li>미리 말씀드리자면 Perplexity는 오늘 글에 언급된 Deep Research 중 <b>가장 빠른 시간에 보고서를 완성합니다.</b></li><li>줄글이 길지 않지만 <b>간략한 단어로 정리</b>해주는 것이 Perplexity의 매력이 아닐까 싶습니다.</li><li>입력창 왼쪽 하단에서 <b>Deep Research</b>를 선택하고 프롬프트 입력 후 실행을 누릅니다.</li></ul><p><img src="77_deepresearch_15.png"><br></p><ul><li>Perplexity는 초반에 참고문헌을 모두 검색해 놓은 뒤, </li><li>확보한 참고문헌들을 읽으면서 토막 정리글들을 만들고 이를 붙이는 방식으로 정리합니다.</li></ul><p><img src="77_deepresearch_16.png"><br></p><ul><li>약 5분만에 작성이 완료된 후 <b>내보내기</b>버튼을 통해 다양한 방식으로 내보낼 수 있습니다.</li><li><code>PDF</code>, <code>Markdown</code>, <code>DOCX</code>와 함께 <code>Perplexity Page</code>가 있습니다.</li><li>블로그 포스팅과 유사한 사이트로 즉석에서 내보낼 수 있는 기능인데, <b>환각 위험성</b>때문에 안 쓰게 됩니다.</li></ul><p><img src="77_deepresearch_17.png"><br></p><ul><li>Perplexity Deep Research가 제공한 보고서는 여기에서 내려받을 수 있습니다. (<a href="perplexity-DeepSeek_report.docx">다운로드 링크</a>)</li></ul><h3 id="●-Perplexity-Deep-Research-평가"><a href="#●-Perplexity-Deep-Research-평가" class="headerlink" title="● Perplexity Deep Research 평가"></a>● Perplexity Deep Research 평가</h3><ul><li><p><b>1. DeepSeek은 OpenAI 등 빅테크 대비 얼만큼의 저비용을 달성했나?</b></p><ul><li><b><code>합격</code></b></li><li>V3 개발비용 558만달러와 H800 2048개만 사용되었다는 내용을 언급하면서도, 다른 비용이 반영되지 않았음을 언급합니다.</li><li>실제 총 투입 비용은 10억달러에 달할 것이라는 주장이 함께 소개되고 있습니다.</li></ul></li><li><p><b>2. DeepSeek가 저비용으로 성능 좋은 추론 모델(DeepSeek-R1)을 훈련시킬 수 있던 방법은?</b></p><ul><li><b><code>합격</code></b></li><li><code>강화학습</code>, <code>GRPO</code>, <code>Multi-Token</code>, <code>MoE</code>, <code>Cold-start data</code>등을 언급합니다.</li></ul></li><li><p><b>3. DeepSeek는 자사의 기술을 공개했나?</b></p><ul><li><b><code>합격</code></b></li><li>DeepSeek Open Source Week 행사를 언급하며 <code>FlashMLA</code>, <code>DeepEP</code>, <code>DeepGEMM</code>, <code>DualPipe</code>, <code>EPLB</code>를 소개합니다.</li><li>DeepSeek-R1이 MIT 라이선스로 공개되었고 개방형 협업 생태계를 지향한다는 기술을 하고 있습니다.</li></ul></li><li><p><b>4. DeepSeek의 기술이 AI 분야에 미친 영향은?</b></p><ul><li><b><code>합격</code></b></li><li>언어 생성, 추론 능력 향상, 모델 경량화, 효율성 개선, 오픈소스 생태계 활성화를 차례대로 정리합니다.</li></ul></li></ul><h2 id="2-4-Genspark"><a href="#2-4-Genspark" class="headerlink" title="2.4. Genspark"></a>2.4. Genspark</h2><blockquote><p><a href="https://www.genspark.ai/">Genspark</a><br><a href="https://www.genspark.ai/spark/comparative-analysis-of-deep-research-ai-agents/44916286-7832-4e98-bbc1-e3bcdb8f9217">Genspark: Comparative Analysis of ‘Deep Research’ AI Agents</a></p></blockquote><ul><li><b>Genspark의 첫인상은 Perplexity 하위호환형</b>이었습니다.</li><li>한글화도 조악하고 Perplexity가 내놓은 기능들을 빠르게 베껴서 자사의 서비스로 탑재했기 때문입니다.</li><li>하지만 머잖아 <b>마인드맵 작성</b>, <b>Mixture of Experts</b>같은 차별화된 기능들을 속속 도입했습니다.</li><li>주변에는 Perplexity를 해지하고 Genspark에 유료 가입했다는 분들도 적지 않을 만큼 경쟁력을 확보했습니다.</li></ul><ul><li>왼쪽 패널의 <b>딥 리서치</b>를 클릭하면 다른 에이전트들과 비슷한 화면으로 넘어갑니다.</li><li><b>1. 계획 생성</b>, <b>2. 딥 리서치</b>, <b>3. 최종 보고서</b> 3단계로 이루어져 있고,</li><li>프롬프트를 입력하여 보고서 작성을 지시하면 연구 계획을 수립하기 시작합니다.</li><li>Genspark는 자체 언어 모델이 없이 GPT, Claude, Gemini, DeepSeek을 가져다 씁니다.</li><li>이들의 장점을 모으는 <b>Mixture-of-Agents</b> 모드를 활용하기로 합니다.</li><li><b>GPT-4o</b>, <b>Claude 3.7 sonnet</b>, <b>Gemini 1.5-pro</b>모델이 동원됩니다.</li></ul><p><img src="77_deepresearch_18.png"><br></p><ul><li><b>계획 수립을 마치면 수정할지 물어봅니다.</b></li><li>Genspark가 Deep Research를 출시했을 때 앞에는 Gemini와 DeepSeek만 나와 있었습니다.</li><li>Gemini의 프로세스를 벤치마킹하여 프롬프트의 조합으로 구축한 것으로 보입니다.</li><li>특이한 점으로 보고서를 공개할지, 혼자만 볼 지 결정하도록 합니다.</li><li>공개하면 <b>“우리의 커뮤니티에 공유하여 다른 이들이 볼 수 있다”</b>고 합니다.</li><li><a href="https://www.genspark.ai/spark/comparative-analysis-of-deep-research-ai-agents/44916286-7832-4e98-bbc1-e3bcdb8f9217">이런 식</a>으로 <b>Genspark Spark에 발행</b>되어 검색 결과에 걸려들 수 있다는 의미로 보입니다.</li></ul><p><img src="77_deepresearch_19.png"><br></p><ul><li>소주제별 일괄 검색과 내용 정리를 반복하는 패턴도 Gemini와 유사합니다.</li></ul><p><img src="77_deepresearch_20.png"><br></p><ul><li>Genspark 답변을 받는 데는 <b>44분이 소요되었습니다.</b></li><li>Mixture-of-Agents가 적용된 만큼 각각의 답변을 받은 뒤에 정리를 합니다.</li><li>최초의 입력 프롬프트와 결과를 비교한 후, 누락된 부분을 중점적으로 보완합니다.</li><li><b>추가 정보를 수집하고 분석하여 보고서를 다듬는 단계가 이어집니다.</b></li></ul><p><img src="77_deepresearch_21.png"><br></p><ul><li>Genspark는 Deep Research는 특이한 기능이 여러 개 있습니다.</li><li>그 중 하나가 동영상을 함께 검색해서 최종 보고서에 붙여준다는 점입니다.</li><li>친절하기는 한데 제가 동영상보다 활자를 선호해서 그런지 이것보다 <b>레퍼런스 제목이라도 붙여주지</b>싶습니다.</li><li>또 하나는 보고서 최하단의 <b>복사본 생성</b>기능입니다. </li></ul><p><img src="77_deepresearch_22.png"><br></p><ul><li>이걸 사용하면 동일한 보고서가 하나 생기는데, 이 버전은 <b>편집 가능합니다.</b></li><li>마치 Notion처럼 Markdown을 이용한 수정이 가능한 것입니다.</li><li>어투나 정보를 수정하여 URL을 타인에게 공유하기에 좋습니다.</li><li>대신 <code>.docx</code> 등 수정 가능한 형태로 내려받기가 여의치 않지만 화면을 인쇄하는 방식으로 PDF 저장이 가능합니다.</li></ul><p><img src="77_deepresearch_23.png"><br></p><ul><li>Genspark Deep Research가 제공한 보고서는 여기에서 내려받을 수 있습니다. (<a href="Genspark-DeepSeek_report.pdf">다운로드 링크</a>)</li></ul><h3 id="●-Genspark-Deep-Research-평가"><a href="#●-Genspark-Deep-Research-평가" class="headerlink" title="● Genspark Deep Research 평가"></a>● Genspark Deep Research 평가</h3><ul><li><p><b>1. DeepSeek은 OpenAI 등 빅테크 대비 얼만큼의 저비용을 달성했나?</b></p><ul><li><b><code>합격</code></b></li><li>여러 기사를 정리하지 않고 나열하는 것이 불편하지만 나쁘지 않습니다.</li><li>GPU 클러스터 투자 금액 16억달러를 논문에 기술된 600만달러와 함께 언급하고 있습니다.</li></ul></li><li><p><b>2. DeepSeek가 저비용으로 성능 좋은 추론 모델(DeepSeek-R1)을 훈련시킬 수 있던 방법은?</b></p><ul><li><b><code>불만족</code></b></li><li><code>계층적 어텐션 최적화</code>, <code>양자화 기법 적용</code>, <code>합성 데이터 생성 전략</code>, <code>개선된 레이어 병렬화 기법</code>, <code>어텐션 헤드 최적화</code>를 언급합니다.</li><li>중요한 내용들이 다수 빠져있어 불만족스럽습니다.</li></ul></li><li><p><b>3. DeepSeek는 자사의 기술을 공개했나?</b></p><ul><li><b><code>불합격</code></b></li><li>Github 레포지토리 실적과 학회 발표를 언급합니다.</li><li>그러나 일반인들에게 잘 알려지지 않은 Open Source Week 행사를 언급하지 않습니다.</li></ul></li><li><p><b>4. DeepSeek의 기술이 AI 분야에 미친 영향은?</b></p><ul><li><b><code>불만족</code></b></li><li>언어 생성, 추론, 경량화 등 관점에서 종합적으로 서술하고 있습니다.</li><li>불합격이라고 말하긴 어렵겠지만 내용이 다른 도구들에 비해 많이 빈약한 느낌입니다.</li></ul></li></ul><h2 id="2-5-Manus"><a href="#2-5-Manus" class="headerlink" title="2.5. Manus"></a>2.5. Manus</h2><blockquote><p><a href="https://manus.im/">manus</a></p></blockquote><ul><li>DeepSeek의 충격이 가시기 전에 등장한 중국산 Deep Research Agent입니다.</li><li>처음부터 성능이 매우 강력하다는 평이 있었고 저도 이 글을 쓰기 전까지 여러 차례 사용하면서 신뢰를 두텁게 쌓았습니다.</li><li>이 글 서두에 있는 <b>모델별 context size 정리가 manus를 이용해 초안을 얻은 것</b>입니다.</li><li>그런데 GPT-4o처럼 context size가 출시 이후 변동된 것들을 캐치하지 못해 오류가 있었고,</li><li>놓치고 가져오지 못한 모델 데이터도 있어서 <b>일일이 확인하며 절반 정도를 수정했습니다.</b></li><li>그럼에도 불구하고 체감상 다른 것들보다는 신뢰를 더 주는 편입니다.</li></ul><ul><li>초기 화면은 다른 것들과 거의 동일합니다.</li><li>같은 프롬프트를 넣고 시작합니다.</li></ul><p><img src="77_deepresearch_24.png"><br></p><ul><li>다른 것보다 Manus에게 agent라는 느낌을 강하게 받는 이유가 있습니다.</li><li>작업 시작과 동시에 일단 <b>작업 공간</b>을 만들고 시작하는데, </li><li>할 일도 <code>todo.md</code>안에 계획을 담아 놓고 하나씩 체크하면서 진행합니다.</li><li>트위터 검색도 API를 호출하는 식으로 하는 모습을 보면서 <b>확장성이 좋겠다</b>는 생각을 했습니다.</li></ul><p><img src="77_deepresearch_25.png"><br></p><ul><li>Manus가 어떻게 동작하는지 작업 화면도 볼 수 있습니다.</li><li><b>컴퓨터 보기</b>버튼을 클릭하면 화면 오른쪽에 Manus가 살펴보는 웹페이지가 고스란히 뜹니다.</li><li>어떤 사이트는 권한이 없어서 내용이 보이지 않는 장면이 등장하는데 해당 장면도 볼 수 있습니다.</li><li>다른 에이전트들도 비슷한 동작을 할 것이라고 유추합니다만, 추후 <b>리플레이</b>가 된다는 점이 쏠쏠한 재미가 됩니다.</li></ul><p><img src="77_deepresearch_26.png"><br></p><ul><li>한 단계를 마무리할 때마다 <code>todo.md</code>파일을 불러 업데이트를 합니다.</li><li>다른 에이전트는 이런 과정을 내부적으로 처리하고 드러내지 않을 듯 하지만,</li><li>이런 과정을 보여주는 점이 엔지니어 관점에서 괜히 흡족합니다.</li></ul><p><img src="77_deepresearch_27.png"><br></p><ul><li>완료된 보고서는 <code>.md</code>와 <code>.pdf</code>형식으로 받을 수 있습니다.</li><li>markdown을 의미하는 <code>.md</code>는 배우기 어렵지 않지만 컴퓨터와 거리가 다소 먼 일반인들에겐 생소합니다.</li><li>.docx 버전 출력을 함께 지원해줬으면 어떨까 하는 약간의 아쉬움이 있습니다.</li></ul><p><img src="77_deepresearch_28.png"><br></p><ul><li>Manus는 최종 보고서인 <code>comprehensive_report</code> 외에 두 개의 파일을 더 줍니다.</li><li>계획표인 <code>todo.md</code>와 <code>research_findings.md</code>인데, 모두 markdown 형식입니다.</li><li><code>research_findings</code>는 <code>comprehensive_report</code>와 사실상 내용이 같고 분량도 비슷합니다.</li><li>차이가 있다면 <b>하나는 개조식, 하나는 줄글</b>이라는 점인데 취향에 따라 무엇을 선호할지가 다를 듯 합니다.</li></ul><p><img src="77_deepresearch_29.png"><br></p><ul><li>아래 이미지의 왼쪽이 <code>research_findings</code>, 오른쪽이 <code>comprehensive_report</code>입니다.</li></ul><p><img src="77_deepresearch_30.png"><br></p><ul><li>Manus Deep Research가 제공한 보고서는 여기에서 내려받을 수 있습니다. (<a href="Manus-report.pdf">comprehensive report</a>, <a href="Manus-research_findings.pdf">research findings</a>)</li></ul><h3 id="●-Manus-Deep-Research-평가"><a href="#●-Manus-Deep-Research-평가" class="headerlink" title="● Manus Deep Research 평가"></a>● Manus Deep Research 평가</h3><ul><li><p><b>1. DeepSeek은 OpenAI 등 빅테크 대비 얼만큼의 저비용을 달성했나?</b></p><ul><li><b><code>합격</code></b></li><li>V3 개발비용 558만달러를 언급하고 H800이 H100보다 성능이 훨씬 못하다는 불충분한 정보가 있습니다.</li><li>그러나 시행착오 비용, 연구개발비 등이 누락되었다는 지적과 함께 5억달러 이상을 이야기합니다.</li></ul></li><li><p><b>2. DeepSeek가 저비용으로 성능 좋은 추론 모델(DeepSeek-R1)을 훈련시킬 수 있던 방법은?</b></p><ul><li><b><code>합격</code></b></li><li><code>MoE</code>, <code>지식 증류</code>, <code>하이브리드 학습(SFT + RL)</code>, <code>멀티토큰</code> 등을 언급합니다.</li></ul></li><li><p><b>3. DeepSeek는 자사의 기술을 공개했나?</b></p><ul><li><b><code>불합격</code></b></li><li>Open Source Week 행사를 언급하지 않습니다.</li></ul></li><li><p><b>4. DeepSeek의 기술이 AI 분야에 미친 영향은?</b></p><ul><li><b><code>합격</code></b></li><li>가격 경쟁 촉발, 오픈소스 AI 생태계 활성화, 저비용 고효율 AI 연구, AI 기술 접근성 향상, 빅테크 기업 대응 변화를 언급합니다.</li></ul></li></ul><h1 id="3-정리"><a href="#3-정리" class="headerlink" title="3. 정리"></a>3. 정리</h1><ul><li>위에서 5종의 Deep Research를 비교해서 살펴봤습니다.</li><li><b>박사과정에게 이 숙제를 내주었을 때 기대한 답변</b>을 목표로 합격과 불합격의 기준을 정했습니다.</li></ul><ul><li>2번 문항은 사실 모든 Deep Research Agent가 100% 답변을 가져오지 못했지만, <b>이 정도면 더 찾아볼 실마리는 되겠다</b>는 생각으로 합격점을 주었습니다.</li><li>그럼에도 불구하고 <b>Genspark는 많이 만족스럽지 못했던 것이 사실입니다.</b></li><li><b>“남들이 10분만에 가져올 때 너 40분 넘게 뭐했냐?”</b>고 묻고 싶을 정도로 열심히 일하는데 결과물이 아쉽습니다.</li><li>부족한 기본기를 프롬프트 엔지니어링으로 커버하려다 보니 한계가 있지 않았나 싶습니다.</li></ul><ul><li>3번 문항은 일반인과 이 분야에 관심을 가지고 있는 사람들의 선을 넘는지 보았고,</li><li>그것이 3월 초의 <b>DeepSeek Open Source Week 행사</b>였습니다.</li><li>이 분야의 박사과정이라면 이 정도는 조사해서 와야 한다고 생각했는데 <b>Perplexity만 행사를 언급했습니다.</b></li><li>그러나 Perplexity도 답을 주었다기보다 한 단계 더 들어갈 실마리를 제공했다고 보는 게 좋습니다.</li></ul><p><img src="77_deepresearch_31.png"><br></p><ul><li>Deep Research는 이름과 달리 연구가 아닙니다.</li><li>자료 조사이지만 그나마 아직 발전의 여지가 있습니다.</li><li>몇 달 뒤에는 더 강력한 도구를 사용할 수 있기를 기대합니다.</li></ul>]]></content:encoded>
      
      
      <category domain="https://jehyunlee.github.io/categories/General/">General</category>
      
      
      <category domain="https://jehyunlee.github.io/tags/ChatGPT/">ChatGPT</category>
      
      <category domain="https://jehyunlee.github.io/tags/Gemini/">Gemini</category>
      
      <category domain="https://jehyunlee.github.io/tags/Perplexity/">Perplexity</category>
      
      <category domain="https://jehyunlee.github.io/tags/Genspark/">Genspark</category>
      
      <category domain="https://jehyunlee.github.io/tags/Manus/">Manus</category>
      
      
    </item>
    
    <item>
      <title>연구활용 AI 도구 사용방법</title>
      <link>https://jehyunlee.github.io/2025/01/20/General-76_researchgenai/</link>
      <guid>https://jehyunlee.github.io/2025/01/20/General-76_researchgenai/</guid>
      <pubDate>Mon, 20 Jan 2025 11:25:00 GMT</pubDate>
      
        
        
      <description>&lt;ul&gt;
&lt;li&gt;저희 연구원내 한 부서의 요청을 받아 연구 활용 생성 AI 도구 소개 및 사용 방법이라는 제목으로 발표를 드렸습니다.&lt;/li&gt;
&lt;li&gt;“생성 AI 도구들을 사용하기 어렵다”는 의견이 단 내에 있었다고 설명을 들었으며,&lt;/li&gt;
&lt;li&gt;</description>
        
      
      
      
      <content:encoded><![CDATA[<ul><li>저희 연구원내 한 부서의 요청을 받아 연구 활용 생성 AI 도구 소개 및 사용 방법이라는 제목으로 발표를 드렸습니다.</li><li>“생성 AI 도구들을 사용하기 어렵다”는 의견이 단 내에 있었다고 설명을 들었으며,</li><li>이에 대응하여 다른 설명은 최대한 배제하고 최소한의 개념과 활용 사례 중심으로 발표를 드렸습니다.</li></ul><h1 id="1-Overview"><a href="#1-Overview" class="headerlink" title="1. Overview"></a>1. Overview</h1><ul><li>새로운 기술과 제품이 너무 빠르게 나와서 따라가기조차 쉽지 않은 것이 사실입니다.</li><li>모든 지식을 매 순간 업데이트하는 것은 거의 불가능합니다.</li><li><b>흐름을 따라가는 것</b>만으로도 충분하다고 생각합니다.</li></ul><p><img src="76_researchgenai_01.png" alt="생성 AI 모델의 대략적인 역사"></p><h1 id="2-LLM-RAG-AI-Agent"><a href="#2-LLM-RAG-AI-Agent" class="headerlink" title="2. LLM, RAG, AI Agent"></a>2. LLM, RAG, AI Agent</h1><ul><li>2023년 언론을 뒤덮은 ChatGPT는 <b>LLM, 거대 언어 모델</b>입니다.</li><li>지금의 GPT는, Claude와 Gemini는 그 때와는 <b>완전히 다른 제품</b>입니다.</li><li>LLM은 환각을 줄이고 내 정보를 활용하고자 <b>RAG: 검색 증강 생성</b>으로 진화했고,</li><li>한편으로 다양한 기능을 탑재하면서 <b>AI Agent</b>로 발전했습니다.</li></ul><p><img src="76_researchgenai_02.png" alt="주요 LLM 모델들의 현재"></p><ul><li>우리는 연구원이고, 우리에게 필요한 것은 <b>RAG AI Agent</b>입니다.</li><li>제가 자주 사용하는 도구들을 소개했습니다.</li></ul><ul><li><b><code>ChatGPT</code> |</b> <a href="https://chat.openai.com/">https://chat.openai.com</a><ul><li>다재다능한 대장주입니다.</li><li>GPTs를 사용해서 특화 기능을 활용해 보세요.</li></ul></li></ul><ul><li><b><code>Claude</code> |</b> <a href="https://claude.ai/">https://claude.ai</a><ul><li>문장력이 매우 뛰어납니다.</li><li>오늘보다 내일이 더 기대되는 모델입니다.</li></ul></li></ul><ul><li><p><b><code>Gemini</code> |</b> <a href="https://gemini.google.com/">https://gemini.google.com</a></p><ul><li>내 gmail과 google drive를 뒤져서 활용합니다.</li><li>Gemini 1.5 Pro Deep Research는 유사 기능 중 현존 최강이라 생각합니다.</li></ul></li><li><p><b><code>Perplexity</code> |</b> <a href="https://perplexity.ai/">https://perplexity.ai</a></p><ul><li>구글 검색을 거의 대체할 수 있지만 요새는 비슷한 도구들이 많이 나왔습니다.</li><li>빠른 스피드가 강점입니다. </li></ul></li></ul><ul><li><b><code>Genspark</code> |</b> <a href="https://genspark.ai/">https://genspark.ai</a><ul><li>Perplexity의 하위 호환이라 생각했지만 좋은 기능이 많이 생겼습니다.</li><li>특히 cross check 기능과 MoE: Mixture of Experts 기능을 꼭 사용해 보세요.</li></ul></li></ul><ul><li><b><code>Storm</code> |</b> <a href="https://storm.genie.stanford.edu/">https://storm.genie.stanford.edu</a><ul><li>같은 소재로 글쓰기를 해도 의도가 다르면 글이 달라집니다.</li><li>Writer AI와 Expert AI, 그리고 사람의 인터랙션을 느낄 수 있습니다.</li></ul></li></ul><ul><li><b><code>SciSpace</code> |</b> <a href="https://typeset.io/">https://typeset.io</a><ul><li>논문 작성을 위한 도구입니다.</li><li>My Library에 PDF 파일들을 채워두고 Chat with PDF, AI writer를 사용해 보세요.</li></ul></li></ul><ul><li><b><code>NotebookLM</code> |</b> <a href="https://notebooklm.google.com/">https://notebooklm.google.com/</a><ul><li>구글의 저력을 보여주는 서비스입니다.</li><li>대량의 자료를 대상으로 한 빠른 검색, 그리고 Audio Overview가 만들어주는 podcast가 너무 좋습니다.</li></ul></li></ul><h1 id="3-현재-진행형입니다"><a href="#3-현재-진행형입니다" class="headerlink" title="3. 현재 진행형입니다."></a>3. 현재 진행형입니다.</h1><p><img src="76_researchgenai_03.png" alt="RAG AI Agent 얼개"></p><ul><li>목표한 짐승을 사냥하는 헌터의 자세보다 고기를 기다리는 낚시꾼의 마음이 필요합니다.</li><li><b>“이런 기능이 있으면 좋겠다.”는 생각을 가지고 안테나를 세운 채 기다리세요.</b> </li><li>기다리던 AI 도구가 나오면, 맘 편히 사용하시는 것을 권해드립니다.</li><li>나한테 맞으면 계속 사용하고, 그렇지 않으면 안 쓰면 그만입니다.</li></ul><ul><li>발표자료를 공유드립니다: (<a href="250120_%EC%9D%B4%EC%A0%9C%ED%98%84_%EC%97%B0%EA%B5%AC%ED%99%9C%EC%9A%A9AI%EB%8F%84%EA%B5%AC%EC%82%AC%EC%9A%A9%EB%B0%A9%EB%B2%95.pdf">다운로드</a>)</li></ul>]]></content:encoded>
      
      
      <category domain="https://jehyunlee.github.io/categories/General/">General</category>
      
      
      <category domain="https://jehyunlee.github.io/tags/SciSpace/">SciSpace</category>
      
      <category domain="https://jehyunlee.github.io/tags/ChatGPT/">ChatGPT</category>
      
      <category domain="https://jehyunlee.github.io/tags/Gemini/">Gemini</category>
      
      <category domain="https://jehyunlee.github.io/tags/Perplexity/">Perplexity</category>
      
      <category domain="https://jehyunlee.github.io/tags/Genspark/">Genspark</category>
      
      <category domain="https://jehyunlee.github.io/tags/Claude/">Claude</category>
      
      <category domain="https://jehyunlee.github.io/tags/Storm/">Storm</category>
      
      <category domain="https://jehyunlee.github.io/tags/NotebookLM/">NotebookLM</category>
      
      
    </item>
    
    <item>
      <title>SciSpace (4) My Library</title>
      <link>https://jehyunlee.github.io/2025/01/18/General-75_scispace_mylibrary/</link>
      <guid>https://jehyunlee.github.io/2025/01/18/General-75_scispace_mylibrary/</guid>
      <pubDate>Fri, 17 Jan 2025 15:11:00 GMT</pubDate>
      
        
        
      <description>&lt;ul&gt;
&lt;li&gt;SciSpace 네 번째 글입니다. PDF 파일들을 정리할 수 있는 My Library입니다.&lt;/li&gt;
&lt;li&gt;믿을만한 문서들을 올려두고 활용할 수 있습니다.&lt;/li&gt;
&lt;li&gt;Zotero, Mendeley를 사용하시던 분들이라면 Sci</description>
        
      
      
      
      <content:encoded><![CDATA[<ul><li>SciSpace 네 번째 글입니다. PDF 파일들을 정리할 수 있는 My Library입니다.</li><li>믿을만한 문서들을 올려두고 활용할 수 있습니다.</li><li>Zotero, Mendeley를 사용하시던 분들이라면 SciSpace로 옮겨올 수 있습니다.</li></ul><h1 id="4-My-Library"><a href="#4-My-Library" class="headerlink" title="4. My Library"></a>4. My Library</h1><h2 id="4-1-문서-업로드-관리"><a href="#4-1-문서-업로드-관리" class="headerlink" title="4.1. 문서 업로드, 관리"></a>4.1. 문서 업로드, 관리</h2><blockquote><p><a href="https://jehyunlee.github.io/2024/05/24/General-53-genaipapersurvey/">Pega Devlog: 생성 AI 연구 활용 한계와 제언</a><br><a href="https://www.nature.com/articles/nature.2015.18703">Nature: Artificial-intelligence institute launches free science search engine</a><br><a href="https://www.semanticscholar.org/">Semantic Scholar: A free, AI-powered research tool for scientific literature</a><br><a href="https://www.zotero.org/">Zotero</a></p></blockquote><ul><li>SciSpace를 비롯한 정보 수집, 정리용 <b>AI 도구들의 가장 치명적인 약점은 Data Source</b>입니다.</li><li><a href="https://jehyunlee.github.io/2024/05/24/General-53-genaipapersurvey/"><b>반년 전 관련 글</b></a>을 올린 적도 있지만 여전히 유효합니다.</li><li>이런 도구들은 <a href="https://www.semanticscholar.org/"><b>Semantic Scholar: A free, AI-powered research tool for scientific literature</b></a>에서 논문을 찾습니다.</li></ul><p><img src="75_scispace_mylibrary_02.png" alt="Semantic Scholar 접속화면"></p><ul><li>Semantic Scholar가 좋은 검색 엔진이기는 하지만, 분야에 따라서는 답답합니다. </li><li><b>찾는 논문, 좋은 논문. 정확히는 유료 논문이 나오지 않기 때문입니다.</b></li><li>문제는 <b>AI가 논문을 찾아 내용을 정리해서 주다 보면 왠지 더 믿게 되고, 여기서 보이지 않는 것은 없다고 은연중에 생각하게 됩니다.</b></li><li>이를 해결할 수 있는 유일한 방법은 <b>논문을 직접 찾아 PDF 파일을 업로드하는 것</b>입니다.</li><li>ChatGPT에도 이런 식으로 논문을 올리고 활용할 수 있는데, 문제는 파일 관리를 PC 등에 별도로 해야 한다는 점입니다.</li><li>SciSpace는 문서를 올리고, 폴더를 만들어 관리할 수 있도록 <b>My Library</b>를 제공합니다.</li></ul><p><img src="75_scispace_mylibrary_01.png" alt="SciSpace 접속화면"></p><ul><li>우측 상단의 <b>My Library</b> 버튼을 누르면, 또는 좌측 세로줄 메뉴에서 <b>책꽂이 아이콘을 클릭하면</b></li><li>저의 경우 아래와 같은 장면이 펼쳐집니다.</li><li>먼저, 좌측에 엄청나게 많은 폴더들이 있습니다. 제가 <b>논문을 받을 때마다 만든 것들</b>과 함께,</li><li>한때 사용하던 <b>Zotero</b>에 저장된 서지정보를 받아온 것들입니다.</li></ul><p><img src="75_scispace_mylibrary_03.png" alt="My Library"></p><ul><li><p>이 중 주황색으로 표현된 <b>AI in Energy</b>폴더를 클릭한 것이 주 화면에 나온 것입니다.</p></li><li><p>폴더에 속한 논문들이 주요 내용과 함께 표로 정리되어 출력됩니다.</p></li><li><p>표의 우측 상단에서 <b>언어</b>를 설정할 수 있습니다. 기본값은 영어이지만 한글을 사용할 수 있습니다.</p></li><li><p>현재 한글로 설정되어 있기 때문에 아래의 <b><code>TL;DR</code></b>이 우리말로 출력되고 있습니다. </p></li><li><p><b><code>Files</code></b>에는 파일명이 있습니다. 24개의 파일명 아래에 세 가지 기능이 보이는데, </p></li><li><p><b><code>summary</code></b>, <b><code>Podcast</code></b>, <b><code>Chat</code></b>입니다.</p></li><li><p><b><code>summary</code></b>를 누르면 <b><code>TL;DR</code></b>보다 훨씬 자세한 요약이 출력되지만 <b>영어로만 고정되어 있습니다.</b></p></li><li><p>언어 설정을 할 수 없다는 점이 아쉽기는 하지만 <b><code>TL;DR</code>에 비해 훨씬 자세한 내용을 담고 있습니다.</b></p></li></ul><p><img src="75_scispace_mylibrary_04.png" alt="Summary"></p><ul><li><b><code>Chat</code></b>을 누르면 오른쪽에 플로팅 창이 뜨면서, <b>이 논문에 대해 질문을 하고 답을 받을 수 있습니다.</b></li><li>ChatGPT에 논문을 업로드해 둔 것과 같은 상황인데, <b>사용을 권장하지는 않습니다.</b></li><li>질문을 하면 빠르게 답변을 하고 본문 중 답을 가져온 지점을 레퍼런스로 표시해주는 것 까지는 좋은데,</li><li>레퍼런스를 클릭하면 <b><code>Chat with PDF</code></b>모드로 갑니다. 여기까지도 좋습니다.</li></ul><ul><li>제가 권장하지 말라고 한 것은, <b>창이 점프하면서 질문과 답변이 모두 사라지기 때문입니다.</b></li><li>단순한 오류이고 곧 고쳐질 것이라 생각하지만, 그리고 새로 열린 창에서 질의와 답변을 받을 수 있지만</li><li><b>그럴거면 그냥 논문 제목을 클릭해서 <code>Chat with PDF</code> 모드로 간 것이 더 편합니다.</b></li><li>뭔가 아직 정리가 덜 된 느낌이 있습니다.</li></ul><p><img src="75_scispace_mylibrary_05.png" alt="Chat"></p><ul><li>화면 오른쪽에는 <b><code>+</code></b> 로 시작하는 항목이 많이 있습니다.</li><li>여기를 클릭하면 표에 항목이 새로 붙으면서 각 논문마다 해당 데이터를 일괄 추출해 냅니다.</li><li>관심이 사라진 주제는 <b><code>x</code>를 눌러 닫을 수도 있고,</b></li><li>일괄 적용하고자 하는 질문이 없을 경우 <b><code>Create new column</code></b>을 눌러 새로운 열을 만들 수 있습니다.</li><li>그리고 우측 상단 <b><code>Export</code></b> 버튼을 눌러 표를 엑셀 등 형식으로 다운로드할 수 있습니다.</li></ul><p><img src="75_scispace_mylibrary_06.png" alt="추출 내용 추가"></p><ul><li><b>라이브러리 관리 방법</b>은 생각보다 간단합니다.</li><li>왼쪽 폴더 목록 위에 있는 <b><code>+New</code></b> 버튼을 눌러 폴더를 만들 수 있고,</li><li>오른쪽 위 <b><code>Upload PDFs</code></b> 버튼을 눌러 파일을 업로드할 수 있습니다.</li></ul><h2 id="4-2-가져오기"><a href="#4-2-가져오기" class="headerlink" title="4.2. 가져오기"></a>4.2. 가져오기</h2><ul><li>SciSpace는 Zotero의 목록을 가져올 수 있습니다.</li><li>화면 왼쪽 위 <b><code>Import from Zotero</code></b>버튼을 누르면 Zotero 계정과 연결하는 화면이 나오는데,</li><li>Zotero 아이디와 패스워드를 입력하여 연결하면 다음과 같이 어떤 폴더를 가져올지 선택할 수 있습니다.</li><li>가져올 폴더를 선택한 뒤 <b><code>Import</code></b> 버튼을 누르면 폴더가 생성되고, 그 안에 논문들이 추가됩니다.</li></ul><p><img src="75_scispace_mylibrary_08.png" alt="Import from Zotero"></p><ul><li><b>PDF 파일만 가져올 수 있다</b>는 이야기가 하단에 깨알같이 적혀있는데,</li><li>여기에 덧붙여 <b>Mendeley에서 Zotero로 가져온게 있다면 이건 안가져올거야</b>라고도 합니다.</li></ul><ul><li><b>사실과 다릅니다.</b></li><li><b>PDF 파일이 없는 서지정보</b>도 가져오며</li><li><b>Mendeley에서 가져온 논문들</b>도 가져옵니다.</li><li>그 바람에 서지정보를 입력하는 기능은 없는데 가져올 수는 있는 기형적인 구조가 됩니다.</li><li>기능이 추가되었으나 안내 문구가 수정되지 않은 것으로 짐작합니다.</li></ul><ul><li>희한한 것은 <b>서지정보만으로도 뭔가 데이터를 끄집어 낸다는 것</b>인데,</li><li><b><code>Summary</code></b>, <b><code>Podcast</code></b> 버튼은 동작하지 않지만 </li><li><b><code>Chat</code>은 오동작을 합니다.</b></li><li>무슨 말이냐면, 지시한 논문의 내용이 아닌 <b>아무말이나 지어낸다는 뜻입니다.</b></li></ul><p><img src="75_scispace_mylibrary_07.png" alt="서지정보"></p><ul><li>다행히 <b>논문 제목을 클릭해서 <code>Chat with PDF</code> 모드로 가면 써먹을만 합니다.</b></li><li>서지정보를 이용해 Semantic Scholar와 연결하는 것으로 보이는데,</li><li><b>초록</b>과 <b>TL;DR</b>, 그리고 <b>피인용수</b>까지 가지고 있는 것을 확인할 수 있기 때문입니다.</li></ul><p><img src="75_scispace_mylibrary_09.png" alt="서지정보를 이용한 Semantic Scholar 정보 수집"></p><ul><li><b>Request PDF</b>버튼을 통해 저자에게 메일을 보낼 수 있는데, </li><li><b>제가 교신저자로 등록돤 메일을 눌러보았지만, 확인해보니 정작 너무 예전 메일주소라 제가 확인할 수 없었습니다.</b></li><li>기능이야 어쨌든 다소 무례한 것이 아닌가 싶은 생각이 듭니다.</li></ul><p><img src="75_scispace_mylibrary_10.png" alt="Request PDF: 저자에게 원고 요청"></p><h2 id="4-3-맺음말"><a href="#4-3-맺음말" class="headerlink" title="4.3. 맺음말"></a>4.3. 맺음말</h2><ul><li>논문을 정리하는 사람 치고 적은 양을 정리하는 사람은 없습니다.</li><li>대학원에 입학한지 얼마 되지 않아 애초에 가진 것이 적은 경우를 제외하고는,</li><li>상당히 많은 논문 + 많아질 논문들을 관리하는 것이 숙명에 가깝습니다.</li><li>그런 면에서 <b>폴더를 만들어 정리할 수 있는 이 기능은 상당히 매력적입니다.</b></li><li>그러나 한편으로 <b>폴더 정렬이 되지 않는다는 점</b>은 매우 아쉽습니다.</li></ul><ul><li>설명과 달리 서지정보를 가져온다는 점에서 </li><li><b>Zotero, Mendeley 같은 서지 관리 프로그램</b>을 노리는 건 아닌가 싶기도 합니다.</li><li>사실 전통의 강자 <b>EndNote</b>를 포함해 이런 프로그램들이 여럿 있고, </li><li>이들도 AI 기능을 달려면 얼마든지 달 수 있습니다. SciSpace보다 더 유리한 고지를 점한 곳도 많습니다.</li><li><b>더 좋은 도구들이 나오면 언제든 활용할 수 있다는 열린 마음으로 기다리는 자세</b>가 필요할 듯 합니다.</li></ul>]]></content:encoded>
      
      
      <category domain="https://jehyunlee.github.io/categories/General/">General</category>
      
      
      <category domain="https://jehyunlee.github.io/tags/SciSpace/">SciSpace</category>
      
      
    </item>
    
    <item>
      <title>SciSpace (3) Find Topics</title>
      <link>https://jehyunlee.github.io/2025/01/05/General-74_scispace_topic/</link>
      <guid>https://jehyunlee.github.io/2025/01/05/General-74_scispace_topic/</guid>
      <pubDate>Sun, 05 Jan 2025 12:27:00 GMT</pubDate>
      
        
        
      <description>&lt;ul&gt;
&lt;li&gt;SciSpace 세 번째 글입니다. Find Topics라는 새 기능이 있습니다.&lt;/li&gt;
&lt;li&gt;하나의 주제에 대한 문헌을 찾고 글을 모으기를 반복할 수 있는 기능입니다.&lt;/li&gt;
&lt;li&gt;잘 사용하면 문헌 조사를 매우 빠르게 할 수 </description>
        
      
      
      
      <content:encoded><![CDATA[<ul><li>SciSpace 세 번째 글입니다. Find Topics라는 새 기능이 있습니다.</li><li>하나의 주제에 대한 문헌을 찾고 글을 모으기를 반복할 수 있는 기능입니다.</li><li>잘 사용하면 문헌 조사를 매우 빠르게 할 수 있으나, 레퍼런스의 상태를 잘 확인해야 합니다.</li></ul><h1 id="3-Find-Topics"><a href="#3-Find-Topics" class="headerlink" title="3. Find Topics"></a>3. Find Topics</h1><h2 id="3-1-Literature-Review-vs-Find-Topics"><a href="#3-1-Literature-Review-vs-Find-Topics" class="headerlink" title="3.1. Literature Review vs Find Topics"></a>3.1. Literature Review <em>vs</em> Find Topics</h2><blockquote><p><a href="https://jehyunlee.github.io/2025/01/04/General-73_scispace_review/">Pega Devlog: SciSpace (2) Literature Review</a></p></blockquote><ul><li><p><a href="https://jehyunlee.github.io/2025/01/04/General-73_scispace_review/"><b>지난 글</b></a>에서 <code>Literature Review </code>기능을 리뷰했습니다.</p></li><li><p>간단히 정리하자면, <b>특정 주제에 대해 문헌들을 검색하고 내용들을 정리하는 기능</b>입니다.</p></li><li><p>정리된 레퍼런스들은 하단에 표 형태로 정리되어 <b>문헌간 비교</b>를 하기 좋게 되어 있습니다.</p></li><li><p>거듭 강조하지만 이 표에 나오는 레퍼런스 정리는 많이 빈약합니다. <b><code>Chat with PDF</code></b> 모드로 가서 제대로 뒤져보실 필요가 있습니다.</p></li><li><p><b><code>Find Topics</code></b> 기능은 <code>Literature Review</code> 기능과 비슷하면서 다릅니다.</p></li><li><p><code>Literature Review</code>가 하나의 주제에 대한 여러 논문을 검색한다면,</p></li><li><p><b><code>Find Topics</code>는 입력한 주제를 중심으로 여러 세부 주제를 찾습니다(Finding Topics).</b></p></li><li><p>이 때 레퍼런스는 세부 주제 하나에 몇 개만을 찾아서 가져오기 때문에 주제 하나에 대한 깊이는 얕은 편입니다.</p></li><li><p>일종의 <b>레퍼런스를 사용해 강화하는 brain storming</b>으로 볼 수 있습니다.</p></li></ul><p><img src="74_scispace_topics_01.png"></p><h2 id="3-2-Find-Topics"><a href="#3-2-Find-Topics" class="headerlink" title="3.2. Find Topics"></a>3.2. Find Topics</h2><ul><li><b><code>Find Topics</code></b>로 들어가면 아래와 같은 화면이 나옵니다.</li><li>관심이 있는 주제를 직접 입력해도 되는데, 마침 본업에 속하는 주제가 있어 클릭을 해 봅니다.</li></ul><p><img src="74_scispace_topics_02.png"></p><ul><li>약 1~2초간 빠르게 다섯 단계를 진행합니다.<ol><li><b>관련 논문 탐색</b></li><li><b>논문별 주제 탐색</b></li><li><b>외부 소스로부터 주제 탐색</b></li><li><b>독특한(unique) 주제 추출</b></li><li><b>최종 결과 준비</b></li></ol></li></ul><p><img src="74_scispace_topics_05.png"></p><ul><li>잠시 후 등장한 화면은 다음과 같습니다.</li><li><code>Literature Review</code>와 마찬가지로 다섯 편(10편으로 조정 가능)의 논문에서 정리된 단락이 있습니다. </li><li>그리고 하단에는 개별 논문 대신 <b>개별 주제들과 이에 해당하는 논문들이 있습니다.</b></li><li>Sources에 <b>Generated by SciSpace models</b>라고 쓰인 것들이 있습니다.</li><li>레퍼런스 없이 AI가 추론만 한 것으로 보입니다.</li></ul><p><img src="74_scispace_topics_06.png"></p><ul><li>상단에 결과물의 품질을 결정하는 <b>Standard/High Quality</b>버튼이 있습니다.</li><li>유료 사용자라면 High Quality를 눌러 더 좋은 결과를 얻을 수 있습니다.</li><li>무료 버전만 쓸 때는 무료 버전도 괜찮다고 느끼지만, </li><li>유료 모드로 사용해보면 무료 버전이 너무 부족하다는 것을 느끼게 됩니다.</li></ul><ul><li><b>출력 언어도 지정할 수 있습니다.</b></li><li>영어로 출력하는 것이 기본이지만 한국어를 선택하면 한국어 버전의 답을 볼 수 있습니다.</li><li>단, 이 때 영어 버전의 답변을 번역하는 것이 아니라 <b>완전히 새롭게 답을 생성해 제출합니다.</b></li><li>영어로 받은 답이 마음에 든다면 번역기를 사용하는 것을 권장합니다.</li></ul><p><img src="74_scispace_topics_07.png"></p><ul><li>찾은 주제와 이에 대한 소스들이 나란히 놓여 있습니다만, <b>단점</b>이 곧장 드러납니다.</li><li><b>“향후 10년의 경향”을 요청했음에도 불구하고 1993년, 2011년 논문을 인용합니다.</b></li><li>레퍼런스를 제대로 체크하지 않으면 그럴싸할 뿐 맞지 않는 이야기를 하게 됩니다.</li><li><b>AI는 완벽하지 않습니다.</b> 도움을 주면 감사하게 쓰고, 미흡하면 기각해야 합니다.</li><li>그리고 <b>이를 거를 수 있는 눈은 사용자가 개발하지 않으면 누구도 대신 해 주지 않습니다.</b></li></ul><ul><li>클릭할 수 있는 곳들이 곳곳에 있습니다. 하나씩 살펴봅니다.</li></ul><ul><li><b>Export as CSV</b>: Topics/Sources 표를 CSV 형식으로 내보냅니다.<ul><li>다운로드된 파일을 더블클릭해서 엑셀로 열면 한글이 깨져 있기도 합니다.</li><li>인코딩 문제입니다. </li><li><code>엑셀 프로그램 실행</code> &gt; <code>새 통합 문서</code> &gt; <code>데이터 탭</code> &gt; <code>텍스트/CSV 열기</code> &gt; <code>utf-8 인코딩</code>으로 여세요.<br><img src="74_scispace_topics_09.png"></li></ul></li></ul><ul><li><b>Save to Notebook</b>: 주제를 노트북에 저장합니다.<ul><li>Jupyter Notebook처럼 코딩을 할 수 있는 플랫폼이나 형식이 아니라, 메모장 같은 겁니다.</li><li>SciSpace가 정리한 결과를 다시 꺼내볼 수 있도록 저장해 둡니다.</li><li><b><code>My Library</code></b>에서 폴더를 만들었다면, 폴더를 지정할 수 있습니다.</li><li><code>My Library</code>는 다음 글에서 자세히 설명하겠습니다.<br><img src="74_scispace_topics_04.png"></li></ul></li></ul><ul><li><b>Topic 이름</b>: 이를 바탕으로 또 다른 Topic들을 도출합니다.<ul><li>가지의 가지를 친다고도 볼 수 있습니다.</li><li>아쉽게도 더 깊이 파고들어간다는 느낌은 들지 않습니다.</li><li>오픈소스 레퍼런스의 한계 때문인지, 얕은 바닥에서 빙빙 돈다는 느낌이 듭니다.<br><img src="74_scispace_topics_10.png"></li></ul></li></ul><h2 id="3-3-맺음말"><a href="#3-3-맺음말" class="headerlink" title="3.3. 맺음말"></a>3.3. 맺음말</h2><ul><li>이번 글은 여기까지입니다.</li><li><code>new</code>가 붙은 새로운 기능이어서 그런지, 만족감보다 아쉬움이 더 큽니다.</li><li>Topics &gt; Topics &gt; … 로 가는 기능이 정상적으로 작동한다면 심층 분석이 들어가야 할 것입니다.</li><li>그러나 그러기에는 <b>레퍼런스의 장벽</b>이 생각보다 높게 느껴집니다.</li></ul><ul><li>유료 논문들에 접근을 하지 못하는 바람에 근본적인 한계가 있다고도 느껴집니다.</li><li>그러나 Vector DB를 사용하는 RAG 방식으로 인한 본질적 제약이라는 생각입니다.</li><li>RAG(Retrieval Augmented Generation) 방식의 특성상 수집한 DB의 chunk, 즉 토막글에서 관련 글을 찾을 것입니다.</li><li>이 때 <b>년도나 저널명 같은 정보는 부차적으로 처리</b>될 것이고, 선별이 어려워집니다.</li></ul><ul><li>하지만 이제까지 문제가 있으면 기를 쓰고 해결해온 것이 인류입니다.</li><li>더 나은 솔루션이 생각보다 훨씬 빨리 등장하리라 생각하고, 기대합니다.</li></ul>]]></content:encoded>
      
      
      <category domain="https://jehyunlee.github.io/categories/General/">General</category>
      
      
      <category domain="https://jehyunlee.github.io/tags/SciSpace/">SciSpace</category>
      
      
    </item>
    
    <item>
      <title>SciSpace (2) Literature Review</title>
      <link>https://jehyunlee.github.io/2025/01/04/General-73_scispace_review/</link>
      <guid>https://jehyunlee.github.io/2025/01/04/General-73_scispace_review/</guid>
      <pubDate>Fri, 03 Jan 2025 15:46:00 GMT</pubDate>
      
        
        
      <description>&lt;ul&gt;
&lt;li&gt;SciSpace 두 번째 글입니다.&lt;/li&gt;
&lt;li&gt;원하는 정보를 담은 글들을 찾아 읽고 내용을 정리하는 일은 연구자의 일상이지만 만만치 않습니다.&lt;/li&gt;
&lt;li&gt;검색과 정리 능력을 활용해 이를 편안하게 도와주는 기능들을 소개합니다.</description>
        
      
      
      
      <content:encoded><![CDATA[<ul><li>SciSpace 두 번째 글입니다.</li><li>원하는 정보를 담은 글들을 찾아 읽고 내용을 정리하는 일은 연구자의 일상이지만 만만치 않습니다.</li><li>검색과 정리 능력을 활용해 이를 편안하게 도와주는 기능들을 소개합니다. 단, 한계가 있습니다.</li></ul><h1 id="2-Literature-Review"><a href="#2-Literature-Review" class="headerlink" title="2. Literature Review"></a>2. Literature Review</h1><h2 id="2-1-Perplexity와-비교"><a href="#2-1-Perplexity와-비교" class="headerlink" title="2.1. Perplexity와 비교"></a>2.1. Perplexity와 비교</h2><blockquote><p><a href="https://www.typeset.io/?via=jehyun">SciSpace</a></p></blockquote><ul><li><a href="https://www.typeset.io/?via=jehyun"><b><code>SciSpace</code></b></a>에 들어서면 다음과 같은 화면을 마주합니다.</li><li>정 가운데 구글을 닮은 검색창이 있고, 아래 <b>이런 질문을 해보지 않으련?</b>하는 느낌으로 몇 개의 예시가 있습니다.</li><li>본능을 억누르고 마우스 휠을 굴려 아래로 향하면 여러 기능들이 등장합니다. </li><li>여기 등장하는 <b>Literature Review</b>가 위에 있는 검색창과 같은 기능을 합니다.</li></ul><p><img src="73_scispace_review_01.png"></p><ul><li><b>Literature Review는 Perplexity와 비슷합니다.</b></li><li>사용자의 질의(query)를 받아 웹 검색을 하고, 여기서 찾은 자료를 정리하여 문장으로 제공합니다.</li><li>비교를 위해 Perplexity 화면을 먼저 보겠습니다.</li></ul><p><img src="73_scispace_review_02.png"></p><ul><li>화면 가운데 있는 검색창에 질의를 하면 웹 검색을 통해 답을 모아줍니다.</li><li>최근 ChatGPT에도 <b>뒙 검색</b>기능이 추가되어 비슷한 기능이 구현되었습니다.</li><li>Perplexity에는 ChatGPT에 (아직) 없는 기능이 있는데, <b>검색 범위를 한정하는 것</b>입니다.</li><li>왼쪽 아래 <b><code>Focus</code></b>버튼을 누르면 여섯 가지 범위가 나옵니다.</li><li>이 중 <b><code>Academic</code></b>를 선택하면 학술 문서들을 대상으로 검색합니다.</li></ul><ul><li>예를 들어 이와 같이 선택하고 <b>수소 생산을 위한 신규 소재 탐색</b>이라고 질의하면,</li><li>논문들을 찾아 검색한 결과만을 보여줍니다.</li><li>아래 그림에서 답변 생성에 활용된 학술논문들 10개의 목록이 보입니다.</li><li>자세히 보면 논문들의 출처가 <b><code>semanticscholar</code></b>와 <b><code>pubmed</code></b>입니다.</li></ul><h2 id="2-2-Review"><a href="#2-2-Review" class="headerlink" title="2.2. Review"></a>2.2. Review</h2><p><img src="73_scispace_review_03.png"></p><ul><li>같은 질의를 SciSpace의 Literature Review에 넣으면 다음과 같은 결과가 나옵니다.</li><li>먼저 상단에 <b>논문 다섯 편의 정보를 활용한 답변</b>이 출력되고,</li><li>그 아래 <b>논문 열 편의 목록</b>과 함께 각 논문에서 얻은 insights가 제공됩니다.</li><li>마지막으로 맨 하단에 Perplexity와 유사하게 <b>Related Questions</b>가 다섯 개 제시되고 있습니다.</li></ul><p><img src="73_scispace_review_04.png"></p><ul><li>왜 10편의 목록을 보여주면서 5편만 가지고 정리하나 싶지만,</li><li>답변 상단을 보면 <b>몇 편의 내용을 정리시킬지</b>를 선택할 수 있습니다.</li><li>10편으로 정리하라면 10편으로 정리합니다.</li><li>또, <b>언어를 설정할 수 있습니다.</b> </li><li>한국사람인 만큼 한글이 편하지만 전문용어를 최대한 반영하고자 영어로 설정하는 편입니다.</li><li><b>Save</b> 버튼을 누르면 결과를 저장하여 나중에 다시 열어볼 수 있습니다.</li></ul><h2 id="2-3-Papers"><a href="#2-3-Papers" class="headerlink" title="2.3. Papers"></a>2.3. Papers</h2><blockquote><p><a href="https://onlinelibrary.wiley.com/doi/10.1002/adma.202313378">Yuan Wang et al., “1. Advancing Catalysts by Stacking Fault Defects for Enhanced Hydrogen Production: A Review.”, Advances in Materials (2024)</a></p></blockquote><p><img src="73_scispace_review_05.png"></p><ul><li>본문으로 가면 유용한 기능이 많습니다.</li><li>질의에 답변하기 위해 찾은 논문들이 <b>표 형태로 정리되어 있습니다.</b></li><li>행을 따라 논문들이 놓여 있고, 열에는 논문 서지정보와 Insights 정도만 있지만 오른쪽에 여러 열들을 계속 붙일 수 있습니다.</li><li>예를 들어 <b>Conclusions</b> 열을 추가하면 논문들의 결론을 모아 볼 수 있습니다.</li></ul><ul><li>여러 논문들을 함께 나란히 놓고 비교할 수 있다는 장점이 있지만 <b>추천하지 않습니다.</b></li><li>몇 가지 이유가 있습니다.<br><b>(1) Open Access 논문이 아닌 경우, 초록에서만 정보를 가져옵니다.</b> 매우 제한적일 수 밖에 없습니다.<br><b>(2) 표에 담으려다보니 극단적으로 짧게 요약합니다.</b> 중요 정보가 누락되는 경우가 많습니다.<br><b>(3) 잘못 찾거나 못찾는 경우가 적지 않습니다.</b> 해당 논문을 따로 열어서 보면 잘 찾습니다.</li></ul><ul><li>상단에 필터를 걸어 논문들을 선별할 수 있는데, <b>PDF</b>나 <b>Open Access</b>를 선택하시는 것을 추천합니다.</li><li>논문의 본문을 볼 수 있어 깊이 파고들며 발췌독을 하기에 좋기 때문입니다.</li><li><b>More Filters</b>를 누르면 년도, 저널 등을 선택할 수 있습니다.</li></ul><ul><li>참고할만한 정보를 확보했다는 전제 하에 우측 상단위 <b>Export</b>버튼을 눌러보셔도 좋습니다.</li><li>화면에 보이는 정보들을 선택에 따라 CSV, Excel, BibTeX, XML, RIS 등 형식으로 내려받을 수 있습니다.</li><li>참고문헌 목록을 만들어 정리할 때 매우 유용합니다.</li><li></li></ul><p><img src="73_scispace_review_06.png"></p><ul><li>최근 추가된 기능으로 <b>Podcast</b>가 재미있습니다.</li><li>클릭하면 논문의 길이에 따라 수십 초에서 수 분 정도 후에 남녀 한 명씩이 논문 내용을 설명해주는 podcast가 재생됩니다.</li><li><a href="advancing-catalysts-by-stacking-fault-defects-for-enhanced-2yvdhr4zek_2025-01-03-17-47-33.mp3"><b><code>첫 번째 논문</code></b></a>의 예시를 내려받아 확인해보셔도 좋습니다.</li><li>개인적인 느낌으로 Google NotebookLM에서 제공하는 Audio Overview에 비해 생동감이 덜해서 아쉽습니다.</li></ul><ul><li>PDF 파일이 있는 논문의 제목을 클릭한 수 스크롤을 내리면 다음과 같은 화면을 볼 수 있습니다.</li><li>왼쪽에는 논문의 본문, 오른쪽에는 <b>Chat with Paper</b>라는 이름으로 챗봇이 붙어 있습니다.</li><li><b>챗봇에 요청을 해서 본문의 내용을 끄집어낼 수 있습니다.</b></li></ul><p><img src="73_scispace_review_07.png"></p><ul><li>일종의 <b>AI를 사용한 발췌독</b>이 가능한 셈입니다.</li><li>논문을 빠르게 읽기 위해 <b>통으로 요약</b>하는 것이 흔하지만, <b>디테일이 날아가고 왜곡이 발생합니다.</b></li><li>이를 방지하기 위해 <b>요약보다 발췌를 하는 것을 추천드립니다.</b></li><li>GPT에 PDF를 올리고 넣는 프롬프트 기준으로 예를 들면, <b>200단어로 요약해줘</b>라고 하지 말고 </li><li><b>저자들은 이 연구에서 어떤 문제를 해결하려고 했어?</b>, <b>본문 중 OOO를 확인하기 위해 사용된 분석 기법이 뭐야?</b>처럼 묻는 식입니다.</li></ul><p><img src="73_scispace_review_08.png"></p><ul><li>자세하게 물어보면 자세한 답변을 얻을 수 있습니다.</li><li>특히 <b>SciSpace를 유료 구독하여 High Quality 모드를 사용하면</b> 더 정확한 답변을 얻을 수 있습니다.</li><li>자세한 사용법은 <b>Chat with PDF</b>글에서 다시 설명드리겠습니다.</li></ul><h2 id="2-4-맺음말"><a href="#2-4-맺음말" class="headerlink" title="2.4. 맺음말"></a>2.4. 맺음말</h2><ul><li>검색 능력에 LLM을 붙여 정리를 시키는 것은 지금은 보편화된 기술입니다.</li><li>AI 활용 기술이 보편화될수록 쉽게 사용할 수 있는 반면, </li><li>특정 분야에 전문성이 있는 사람이나 도구를 쓸 때와 같은 날카로움은 기대하기 어렵습니다.</li><li>이럴 수록 내가 무엇을 얻을 수 있으며 무엇은 기대할 수 없는지 명확하게 알아야 합니다.</li><li>그리고 내 능력의 한계를 벗어나면 판단을 할 수 없는 만큼, 스스로의 역량을 갈고 닦아야 합니다.</li></ul>]]></content:encoded>
      
      
      <category domain="https://jehyunlee.github.io/categories/General/">General</category>
      
      
      <category domain="https://jehyunlee.github.io/tags/SciSpace/">SciSpace</category>
      
      
    </item>
    
    <item>
      <title>SciSpace (1) overview</title>
      <link>https://jehyunlee.github.io/2024/12/30/General-72_scispace/</link>
      <guid>https://jehyunlee.github.io/2024/12/30/General-72_scispace/</guid>
      <pubDate>Mon, 30 Dec 2024 04:29:00 GMT</pubDate>
      
        
        
      <description>&lt;ul&gt;
&lt;li&gt;SciSpace는 연구에 활용하기 좋은 AI 도구입니다.&lt;/li&gt;
&lt;li&gt;기능이 많아 짧은 글에 모든 기능을 담기 어려워, 몇 편으로 나누어 소개합니다.&lt;/li&gt;
&lt;li&gt;오늘은 첫 번째 글로, 개괄적인 내용을 다룹니다.&lt;/li&gt;
&lt;/u</description>
        
      
      
      
      <content:encoded><![CDATA[<ul><li>SciSpace는 연구에 활용하기 좋은 AI 도구입니다.</li><li>기능이 많아 짧은 글에 모든 기능을 담기 어려워, 몇 편으로 나누어 소개합니다.</li><li>오늘은 첫 번째 글로, 개괄적인 내용을 다룹니다.</li></ul><h1 id="1-scispace"><a href="#1-scispace" class="headerlink" title="1. scispace"></a>1. scispace</h1><blockquote><p><a href="https://www.typeset.io/?via=jehyun">SciSpace</a></p></blockquote><p><img src="72_scispace_01.png" alt="SciSpace 한국어 접속화면"></p><h2 id="1-1-overview"><a href="#1-1-overview" class="headerlink" title="1.1. overview"></a>1.1. overview</h2><blockquote><p><a href="https://www.typeset.io/t/about/">SciSpace: about scispace</a><br><a href="https://www.typeset.io/formats/search/">SciSpace: journal gallery</a><br><a href="https://typeset.io/resources/typeset-evolving-into-scispace/">SciSpace: introducing SciSpace-Revolutionizing research workflows end-to-end</a></p></blockquote><p><img src="72_scispace_05.png" alt="[www.scispace.com 접속시 안내 화면](https://scispace.com/)"></p><ul><li>scispace의 웹사이트는 <a href="https://www.typeset.io/?via=jehyun"><b><code>https://typeset.io/</code></b></a>입니다.</li><li>이름과 사이트 이름이 달라 <b>왜지?</b>라는 궁금증을 불러일으키기 마련입니다만,</li><li>2015년 서비스를 처음 개시했을 때는 특정 학술지에 맞춰 <b>서식을 조정하는 도구(typesetting tool)</b>였습니다.</li></ul><blockquote><p>“It is funny to look back and think that it all began with rejection. My university rejected my thesis in the final year of graduation because of inappropriate formatting.</p></blockquote><blockquote><p>“모든 것이 거부에서 시작되었다는 사실을 돌이켜보면 웃음이 나옵니다. 제 학위논문은 서식을 맞추지 못했다는 이유로 대학 당국에 의해 졸업이 거부되었습니다.”</p></blockquote><ul><li>SciSpace의 설립자인 Saikiran Chandhark가 <b>학위논문 서식을 맞추지 못해 졸업이 거절당한 것이 시작</b>이라고 합니다.</li></ul><ul><li>학술지 typesetting tool의 흔적은 <a href="https://www.typeset.io/formats/search/"><b><code>journal gallery</code></b></a>에서 찾아볼 수 있습니다.</li><li>저널 이름을 검색하여 선택하면 출판 형식을 보여주고, </li><li>MS template를 올리거나 사이트에서 자체 제공하는 빈 문서를 사용해 논문을 작성할 수 있습니다.</li></ul><p><img src="72_scispace_02.png" alt="SciSpace journal gallery: &quot;Artificial Intelligence&quot; 검색 화면"><br><img src="72_scispace_03.png" alt="SciSpace journal gallery: &quot;Artificial Intelligence&quot; 선택 화면"><br><img src="72_scispace_04.png" alt="SciSpace journal gallery: &quot;A blank document&quot; 선택 화면"></p><ul><li><b>typesetting tool</b>로 시작한 서비스는 <b>무료 full-text PDF 검색 도구</b>로 발전했습니다.</li><li>2022년 5월 11일, <b>science</b>와 <b>workspace</b>를 결합한 의미의 <b>SciSpace</b>로 브랜드를 전환했습니다.</li><li>당시만 해도 지금과 같은 문서 요약이나 글쓰기 도구는 담기지 않았던 것으로 보입니다.</li></ul><blockquote><p>“Two significant questions stood before us: how to build a world where breakthrough scientific research happens at pace, and how do we evolve our brand messaging to convey the same.”</p></blockquote><blockquote><p>“중요한 질문 두 개가 우리 앞에 놓여 있었습니다. 획기적인 과학 연구가 빠른 속도로 이루어지는 세상을 어떻게 만들 것인가, 그리고 이 방법을 전달하는 우리 브랜드를 어떻게 성장시킬 것인가 하는 점입니다.”</p></blockquote><ul><li><a href="https://typeset.io/resources/typeset-evolving-into-scispace/">SciSpace 창립자가 사명을 변경하면서 발행한 글</a>에는 42%의 학술논문이 유료화 장벽(paywall) 너머에 있다고 기술되어 있습니다.</li><li>학제와 지역의 벽을 넘어 <b>연구자들간의 협업 커뮤니티</b>를 만들어 보겠다는 포부가 PDF 공유와는 무관해보이기도 합니다만</li><li>일종의 <b>논문 검색 엔진</b>을 매개로 연구를 가속화하겠다는 포부는 전달됩니다.</li></ul><p><img src="72_scispace_08.jpg" alt="2022년 무료 full-text PDF 검색 도구를 표방하던 당시의 자료"></p><p><img src="72_scispace_09.png" alt="SciSpace 로고 디자인 과정"></p><ul><li>당시 <b>mission</b>에서 <b>문헌 검색과 연구 논문 작성부터 연구의 가시성 향상 모두를 현대화한다</b>는 목표를 읽을 수 있습니다.</li></ul><blockquote><p>“SciSpace on a mission is to become the most comprehensive end-to-end platform for researchers, modernizing everything from the literature search and research writing to improving research visibility.”</p></blockquote><ul><li>그리고 2023년, SciSpace는 현재의 <b>논문 탐색 &amp; 작성 도구</b>로 발전했습니다.</li></ul><h2 id="1-2-문헌-Database"><a href="#1-2-문헌-Database" class="headerlink" title="1.2. 문헌 Database"></a>1.2. 문헌 Database</h2><blockquote><p><a href="https://chatgpt.com/g/g-NgAcklHd8-scispace">GPTs: SciSpace</a><br><a href="https://doi.org/10.48550/arXiv.2301.10140">The Semantic Scholar Open Data Platform</a><br><a href="https://allenai.org/open-data">Allen Institute: Open Data</a><br><a href="https://typeset.io/resources/tag/open-access/">SciSpace Resources: Open Access</a><br><a href="https://jehyunlee.github.io/2024/05/24/General-53-genaipapersurvey/">Pega Devlog: 생성AI 연구 활용 한계와 제언</a></p></blockquote><ul><li>SciSpace는 270 million+, 즉 2.7억편 이상의 논문에 접근할 수 있다고 합니다. </li><li>SciSpace GPTs에는 287 million으로 표시되어 있습니다.</li><li>그러나 <b>참조 데이터베이스 명을 명시하지 않고 있습니다.</b></li><li>이 중 2.2억편은 Allen Institute에서 운영하는 <a href="https://allenai.org/open-data"><b><code>S2AG</code></b></a>에서 제공하는 데이터입니다.</li><li>S2AG는 Microsoft에서 운영하던 Microsoft Academic Graph(MAG)의 데이터를 계승한 것입니다.</li></ul><p><img src="72_scispace_06.png" alt="Allen Institute: S2AG"></p><ul><li><p>좋은 데이터이지만 근본적인 한계가 있습니다.</p></li><li><p><b>Open Access 논문</b>에 의존한다는 점인데, <b>분야에 따라서는 쓸만한 정보를 얻을 수 없기 때문입니다.</b></p></li><li><p>SciSpace에서 논문들을 대상으로 질의를 하고 답변을 받을 수 있으나 </p></li><li><p><b>본인의 분야 및 목적이 open access로 충분한지</b> 검토가 선행되어야 합니다.</p></li><li><p>SciSpace에서 논문을 찾아보면 Google Scholar를 비롯해 arXiv, PubMed, Semantic Scholar, IEEE 등에서 검색을 합니다.</p></li><li><p>그러나 본문까지 검색이 되는 것은 <b>open access 논문으로 한정</b>되고, </p></li><li><p><b>본문이 공개되지 않은 Google Scholar 검색 논문과 IEEE Xplore 논문 등은 초록만 활용할 수 있습니다.</b></p></li><li><p>사용자의 목적에 따라 초록만으로도 충분한 경우가 있고, open access로도 충분할 때가 있습니다.</p></li><li><p>중요한 것은 <b>한계를 알고 사용하는 것</b>입니다.</p></li></ul><ul><li>다행히 <a href="https://www.researchgate.net/"><b><code>ResearchGate</code></b></a>같은 원문 공유 사이트에서 일부 유료 논문을 볼 수 있습니다.</li><li>저자들의 너그러움에 기대는 방식인데,</li><li>최근 SciSpace에 <b>저자에게 원문을 요청하는 메일을 보내는 기능</b>이 추가되었습니다.</li></ul><p><img src="72_scispace_14.png" alt="Request PDF 버튼을 누르면 나오는 원문 요청 화면"></p><ul><li>이래도 되나 싶은, 다소 당혹스러운 느낌이 들기도 합니다.</li><li>일단, 가급적 <b>PDF 파일을 직접 업로드하여 사용하시기를 권장</b>합니다.</li></ul><h2 id="1-3-언어-모델"><a href="#1-3-언어-모델" class="headerlink" title="1.3. 언어 모델"></a>1.3. 언어 모델</h2><blockquote><p><a href="https://typeset.io/resources/introducing-copilot-ai-assistant-explains-research-papers/">SciSpace Resources: Introducing ChatPDF: Your AI assistant that helps explain papers</a><br><a href="https://typeset.io/resources/ai-summary-generators/">SciSpace Resources: AI Summary Generator in Academic – A Quick Guide (2024)</a><br><a href="https://typeset.io/resources/best-chatpdf-tool-scispace/">SciSpace Resources: Adobe PDF Reader vs. SciSpace ChatPDF — Best Chat PDF Tools</a><br><a href="https://typeset.io/resources/research-paper-summarizer-an-overview/">SciSpace Resources: Research paper summarizer | An overview of the best AI summarizers</a><br><a href="https://aclanthology.org/2020.findings-emnlp.428/">EMNLP 2020: SciTLDR</a><br><a href="https://github.com/allenai/scitldr">github: SciTLDR</a><br><a href="https://arxiv.org/abs/1903.10676">arXiv: SciBERT</a><br><a href="https://github.com/allenai/scibert">github: SciBERT</a><br><a href="https://spacy.io/universe/project/scispacy">spacy: scispacy</a><br><a href="https://allenai.github.io/scispacy/">github: scispacy</a><br><a href="https://github.com/meta-llama/llama-recipes/blob/main/recipes/quickstart/NotebookLlama/README.md">github: NotebookLlama</a></p></blockquote><ul><li>현재의 SciSpace는 언어모델을 이용해 <b>요약, 질의, 번역</b> 등의 기능을 제공합니다.</li><li>하지만 <b>SciSpace에서 사용하는 언어모델</b>에 대해 명확하게 공개된 바가 없습니다.</li><li><a href="https://typeset.io/resources/introducing-copilot-ai-assistant-explains-research-papers/"><b>Copilot이라는 이름의 첫 버전</b></a>이 등장한 것은 22년 12월*이고,<br>(원글이 업데이트됨. <a href="https://researchtoolsbox.blogspot.com/2022/12/introducing-copilot-your-ai-assistant.html?m=0">아카이빙된 원글 링크</a>)</li><li>ChatGPT 3.5 API가 2023년 3월 1일에 공개된 점에 미루어 볼 때,</li><li>이전에 공개된<b>GPT3 API를 활용하다 GPT 버전 업데이트에 발맞추어 개선한 것으로 여겨집니다.</b></li></ul><ul><li>그러나 한편으로 <b>다른 모델을 (함께) 사용할 가능성을 배제할 수 없습니다.</b></li><li>Semantic Scholar는 일부 논문들에 대해 초록을 세 줄 가량으로 요약한 <b>TLDR</b> 기능을 제공합니다.</li><li>Allen Institute에서 개발한 <a href="https://github.com/allenai/scitldr"><b><code>SciTLDR</code></b></a>을 사용한 것입니다.</li><li>2020년 EMNLP에서 발표된 논문으로, 3,935개의 요약문 데이터셋을 사용해 개발되었습니다.</li></ul><p><img src="72_scispace_10.png" alt="SciTLDR 논문 (2020): https://aclanthology.org/2020.findings-emnlp.428/)"></p><ul><li>BERT 모델을 과학 논문들로 미세조정한 <a href="https://github.com/allenai/scibert"><b><code>SciBERT</code></b></a>도 있습니다.</li><li>2019년에 나온 논문으로, BERT 모델을 과학 논문들로 미세조정한 것입니다.</li><li><code>SciBERT</code>는 자연어 처리 라이브러리 <code>Spacy</code>를 과학 문헌에 맞게 조정한 <code>SciSpacy</code>에 기반을 두고 있습니다.</li><li>GPT를 비롯한 거대 언어 모델을 사용하려면 API 비용이 소요되고 응답시간이 길어지기 때문에 <b>이런 작은 모델들을 운영할 여지가 충분합니다.</b></li></ul><p><img src="72_scispace_11.png" alt="SciSpacy: https://allenai.github.io/scispacy/"></p><ul><li>최근에는 <b>Google NotebookLM</b>을 필두로 PDF 문서를 podcast로 만들어주기도 합니다.</li><li>오픈소스로 공개된 <a href="https://github.com/meta-llama/llama-recipes/blob/main/recipes/quickstart/NotebookLlama/README.md"><b><code>NotebookLlama</code></b></a>를 이용하면 누구나 podcast를 만들 수 있습니다.</li><li>최근 <b>SciSpace</b>에는 <b>논문을 podcast로 만들어주는 기능이 추가</b>된 데서 알 수 있듯, 여러 모델들을 조합하여 활용하는 것으로 보입니다.</li></ul><p><img src="72_scispace_12.jpg" alt="NotebookLlama 개념도"></p><div class="video-container"><iframe src="https://www.youtube.com/embed/PvUHo4Tt1Kw" frameborder="0" loading="lazy" allowfullscreen></iframe></div><h2 id="1-4-맺음말"><a href="#1-4-맺음말" class="headerlink" title="1.4. 맺음말"></a>1.4. 맺음말</h2><ul><li>지금의 SciSpace는 연구의 시작과 끝을 담당하는 <b>종합 AI 도구</b>입니다.</li><li>유사한 기능을 제공하는 여러 도구들과 함께 경쟁하면서 공진화를 하고 있습니다.</li><li>전에 없던 기능이 갑자기 생기기도 하고, 그 과정에서 일부 기능이 불안정하기도 하지만 전반적인 쓰임은 매우 편리하고 안정적입니다.</li></ul><p><img src="72_scispace_13.png" alt="SciSpace"></p><ul><li><b>SciSpace 외의 다른 도구들도 비슷한 양상으로 함께 진화할 것으로 예상됩니다.</b></li><li>이 글은 SciSpace에 대한 소개글이지만, AI 도구의 발전 과정을 보여주는 좋은 예시가 되기를 바랍니다.</li></ul>]]></content:encoded>
      
      
      <category domain="https://jehyunlee.github.io/categories/General/">General</category>
      
      
      <category domain="https://jehyunlee.github.io/tags/SciSpace/">SciSpace</category>
      
      
    </item>
    
    <item>
      <title>AI와 예술 창작 - AI 그림이라는 진지한 취미</title>
      <link>https://jehyunlee.github.io/2024/12/20/General-71_hobbyart/</link>
      <guid>https://jehyunlee.github.io/2024/12/20/General-71_hobbyart/</guid>
      <pubDate>Fri, 20 Dec 2024 12:29:00 GMT</pubDate>
      
        
        
      <description>&lt;ul&gt;
&lt;li&gt;김재인 교수님의 주선으로 AI를 도구로 예술을 하는 분들과 같은 자리에 섰습니다.&lt;/li&gt;
&lt;li&gt;내가 끼어도 되는 걸까 싶은 걱정과 이런 분들과 함께 한다는 기대가 공존했습니다.&lt;/li&gt;
&lt;li&gt;말이 토론이었지 오가는 말씀 속에서 많</description>
        
      
      
      
      <content:encoded><![CDATA[<ul><li>김재인 교수님의 주선으로 AI를 도구로 예술을 하는 분들과 같은 자리에 섰습니다.</li><li>내가 끼어도 되는 걸까 싶은 걱정과 이런 분들과 함께 한다는 기대가 공존했습니다.</li><li>말이 토론이었지 오가는 말씀 속에서 많이 배우고 시야가 크게 넒어지는 느낌을 받았습니다.</li></ul><h1 id="1-발표-전"><a href="#1-발표-전" class="headerlink" title="1. 발표 전"></a>1. 발표 전</h1><h2 id="1-1-김재인-교수님"><a href="#1-1-김재인-교수님" class="headerlink" title="1.1. 김재인 교수님"></a>1.1. 김재인 교수님</h2><blockquote><p><a href="https://armdown.net/">김재인 교수님 블로그</a><br><a href="https://www.youtube.com/watch?v=hb43uA91088">youtube: 기술은 예술의 자원일까? 위협일까? 기술시대의 예술의 미래, 책으로 만나는 AI시대의 예술(김재인 교수)</a><br><a href="https://www.yes24.com/Product/Goods/119019001">yes24: AI 빅뱅</a></p></blockquote><ul><li>살면서 이런 사람들은 절대 만나지 않겠지 하는 부류의 분들은, 만나고서야 아 이런 분들이 있었지 싶습니다.</li><li>그 중 한 부류가 <b>철학자</b>. </li><li>이공계인의 숲에서 살며 간혹 그림그리는 분들과 역사 이야기 하는 분들을 기웃거리는 내게,</li><li>내게 <b>철학자</b>는 아무리 가까워도 데카르트보다 가깝기 힘들고 그 데카르트도 직교좌표계로 더 가깝습니다.</li></ul><div class="video-container"><iframe src="https://www.youtube.com/embed/hb43uA91088" frameborder="0" loading="lazy" allowfullscreen></iframe></div><ul><li>코드와 ppt를 들고 다니다 간혹 그림을 들고 다니던 SNS에서 <b>김재인</b>이라는 이름이 자주 보이기 시작했습니다.</li><li><b>철학자</b>라고 하는데, <b>인공지능</b>이야기를 하시고, 특히 <b>인공지능이 그리는 그림</b>이야기를 하십니다.</li><li>내가 즐겨보는 대상을 완전히 다른 면에서 보는 시각이 신선했고, 덕택에 들뢰즈 같은 이름도 알게 됐습니다.</li><li>이 분의 시야에도 제가 들어왔는지 <a href="https://www.yes24.com/Product/Goods/119019001"><b>저서</b></a>에 추천사를 쓰는 인연도 맺게 됐네요.</li></ul><ul><li>멀리서 존재 정도를 인지하고 있다고 생각했는데, <b>행사에 참여해 달라는 요청을 주셨습니다.</b></li><li><b>감사하다</b>는 마음과 <b>재밌겠다</b>는 생각에 이어 별 생각이 다 들었습니다.</li><li><b>내가 끼어도 되는 자리 맞나</b>, <b>이런 분들이랑 이야기 한번 나눠보고 싶다</b>, <b>어떤 분위기일까</b> 등등.</li><li>흥미와 호기심, 그리고 처음 생긴 종류의 기회는 무조건 잡는다는 생각으로 나갔습니다.</li></ul><h2 id="1-2-발표-준비"><a href="#1-2-발표-준비" class="headerlink" title="1.2. 발표 준비"></a>1.2. 발표 준비</h2><blockquote><p><a href="https://jehyunlee.github.io/2024/01/01/General-35-genAIdraw/">Pega Devlog: AI그림 vs 손그림</a><br><a href="https://aifrenz.org/artist2024">실용인공지능학회: AAiCON2024 생성AI 아티스트 초대전</a></p></blockquote><ul><li>막상 발표하기로 하니 난감하더군요. </li><li>나 혼자 내 멋에 겨워서 노래도 부르고 춤도 출 수 있지만, 무대에 올라가려면 <b>어느 정도 수준</b>이 되지 않으면 민폐입니다.</li><li>그리고 <b>무대의 수준은 자리에 앉아서 들어주시는 청중이 결정합니다.</b></li><li>모르긴 몰라도 예술에 몸담은 분들이 가득할 자리에서 공돌이가 취미로 하는 AI <b>그림 이야기</b>는 깊기 힘듭니다.</li></ul><ul><li>생각을 바꿨습니다.</li></ul><ul><li>그림을 그리는 <b>AI 이야기</b>를 하면 되겠네.</li><li>예술 분야의 <b>청중께는 AI에 가까운 프롬프트 이야기</b>를 들려드리고,</li><li>프롬프트에 <code>멋진 미래 도시</code> 같은 걸 넣고 있을 <b>공돌이 분들께는 그림 이야기</b>를 들려드리면 되겠네.</li><li>그러면 <b>내가 그림을 어떻게 그리는지</b>말씀드리면 되겠네.</li></ul><ul><li><a href="https://aifrenz.org/artist2024"><b>지난 6월 AAiCON2024 생성AI 아티스트 초대전</b></a>에서 발표한 자료를 압축해서 앞쪽에 넣고,</li><li>뒷부분에는 내가 그리는 방식을 정리해서 발표하기로 했습니다.</li></ul><p><img src="71_hobbyart_02.png"></p><h1 id="2-발표내용"><a href="#2-발표내용" class="headerlink" title="2. 발표내용"></a>2. 발표내용</h1><h2 id="2-1-저에게-AI-그림은-취미입니다"><a href="#2-1-저에게-AI-그림은-취미입니다" class="headerlink" title="2.1. 저에게 AI 그림은 취미입니다."></a>2.1. 저에게 AI 그림은 취미입니다.</h2><ul><li>발표 제목을 “AI 그림이라는 진지한 취미”로 잡았습니다.</li><li>그림은 저에게 <b>직업이 아니라 취미입니다.</b></li><li>인생, 가치관, 생계가 걸린 분들과는 입장이 많이 다릅니다.</li><li><b>고통을 인내하며 탐구하고 추구하는 대상이 아니라, 본업에서 전투를 치르고 돌아와 휴식하고 충전하는 곳입니다.</b></li></ul><ul><li><b>그래서, 여기에 에너지를 다 쏟아 저를 불태울 생각은 없습니다.</b></li><li>너무 진지한 고뇌의 대상으로 삼지 않습니다.</li><li>노트 한켠에 끄적이던 낙서가 도구만 바뀐 것입니다.</li><li>저는 <b>단편적인 감정을 배출하고</b>, <b>가끔 귀엽거나 웃긴다고 누군가 공감해주면 감사하지만 기대를 하는 경우는 적습니다.</b></li></ul><ul><li>굳이 이 이야기를 길게 하는 이유는,</li><li>카메라 셔터를 누르는 한 순간을 위해 몇 달을 준비하는 분들이 있고</li><li>한 폭의 그림을 완성하려고 내면의 고독을 고통스럽게 마주해서 잡아 올리는 분들이 있다는 걸 알기 때문입니다.</li><li>이런 분들과 함께 나란히 놓일 작품은 아닙니다.</li></ul><h2 id="2-2-그렇다고-뇌를-비우고-그리지는-않습니다"><a href="#2-2-그렇다고-뇌를-비우고-그리지는-않습니다" class="headerlink" title="2.2. 그렇다고 뇌를 비우고 그리지는 않습니다."></a>2.2. 그렇다고 뇌를 비우고 그리지는 않습니다.</h2><blockquote><p><a href="https://www.te.co.kr/news/article.html?no=23826">더에듀: 네덜란드 교사 노조, 교실 내 AI 사용 지침 명확화 요구</a><br><a href="https://zdnet.co.kr/view/?no=20241219180203">ZDNet Korea: 성능·역량보다 ‘에너지 효율성’ 더 중요해진다…내년 AI 시장 변화 ‘예고’</a><br><a href="https://www.weeklytoday.com/news/articleView.html?idxno=648307">위클리오늘: 서울 성북 VIP 동물병원서 반려동물 사망… 보호자 “병원의 불성실한 치료 때문” 진실은?</a><br><a href="https://news.sbs.co.kr/news/endPage.do?news_id=N1007913885">SBS: 챗GPT로 수능 국어 풀었더니 97점…AI, 만점 도전하나</a></p></blockquote><ul><li><b>아무래도 좋으니 예쁜거 나와라 뚝딱</b>하는 마음으로 그리지는 않습니다.</li><li>내 감정을 담는 것이 목적이라면, <b>내 감정을 최대한 잘 표현하려고 노력합니다.</b></li><li>사람들에게 무슨 말을 걸고 싶다면, <b>그 목소리가 잘 들리게 하려고 노력합니다.</b></li><li>깊은 철학은 없지만 얕으면 얕은대로 정확하게 전달하고 싶다는 뜻입니다.</li></ul><ul><li>그래서 가장 피하는 그림이 <b>아무나 다 그리는 그림</b>입니다.</li></ul><ul><li>언젠가부터 신문기사에 <b>AI가 그린 삽화</b>가 자주 보입니다.</li><li>특히 DALL.E가 그린 그림들이 많은데, <b>특유의 푸른 빛과 번들거림이 너무 싫습니다.</b></li><li>AI 그림이라는 걸 알리려고 일부러 넣은건가 의심될 정도로 부자연스럽습니다.</li><li>좋은 그림을 전달하고 싶다는 생각이 있다면, 이런 색감은 절대로 쓰지 않을 것 같습니다.</li></ul><p><img src="71_hobbyart_03.png" alt="기사 삽입 그림 (더에듀, ZDNet Korea, 위클리오늘, SBS)"></p><ul><li>그리고, AI 그림이 유행함과 동시에 <b>젊은 여성들의 그림들</b>이 곳곳에 걸립니다.</li><li>이런 그림을 그리는 분들은 무언가 본인만의 이유가 있으실 것이라 생각하고 존중하지만,</li><li>적어도 저는 별로 그리고 싶지 않습니다. </li><li>이유는 <b>그냥, 이런 그림들이 이미 충분히 많아서</b>입니다.</li></ul><p><img src="71_hobbyart_04.png" alt="유튜브 영상 썸네일 @NeverlandLatata"></p><ul><li>위가 <b>의도만 앞서서 촌스러운 그림</b>이라면</li><li>아래는 적어도 저로서는 <b>의도를 알 수 없는 그림</b>입니다.</li><li>그림을 그리신 분들에게는 의도가 있겠습니다만 적어도 제겐 저런 그림을 그릴 의도가 떠오르지 않습니다.</li><li>전달하고 싶거나 공감받고 싶은 마음, 표현하고 싶은 감정을 시각적으로 표현하고자 합니다.</li></ul><p><img src="71_hobbyart_05.png" alt="생성 AI 그림 @Jehyun Lee"></p><h2 id="2-3-선을-그었습니다"><a href="#2-3-선을-그었습니다" class="headerlink" title="2.3. 선을 그었습니다."></a>2.3. 선을 그었습니다.</h2><h3 id="2-3-1-즐거움을-잃지-않도록-선을-그었습니다"><a href="#2-3-1-즐거움을-잃지-않도록-선을-그었습니다" class="headerlink" title="2.3.1. 즐거움을 잃지 않도록 선을 그었습니다."></a>2.3.1. 즐거움을 잃지 않도록 선을 그었습니다.</h3><ul><li>그림이라는 취미는 <b>시간</b>이 많이 듭니다.</li><li>손으로 그리건 핸드폰으로 그리건, 털 하나 점 하나에 예민한데, 수천번 터치를 해야 비로소 형체가 드러납니다.</li><li>문제는 현업에 시간을 들이다 보면 <b>그림에 들일 시간이 극히 제한된다는 점</b>입니다.</li></ul><p><img src="71_hobbyart_06.png" alt="폰그림 @Jehyun Lee"></p><ul><li>생성 AI 그림도 마찬가지입니다.</li><li>DALL.E2부터 생성 AI 그림을 수정할 수 있는 기능이 생겼습니다.</li><li><b>inpainting</b>으로 세부를 수정할 수 있고, <b>outpainting</b>으로 영역을 확장할 수 있습니다.</li><li>Stable Diffusion 계열에는 밑그림을 이용할 수 있는 <b>ControlNet</b>이 나와 있습니다.</li><li>이런 기능들을 사용하면 머리 속 그림에 훨씬 더 가까워질 수 있습니다.</li><li>그러나 <b>끝이 없다</b>는 것이 제겐 문제가 됩니다. 시간이 끝없이 드니까요.</li></ul><p><img src="71_hobbyart_07.png" alt="inpainting"></p><ul><li>시간에 쫓기면 의무감이 되고, 즐거움을 잃어버리는 경험은 충분히 했습니다.</li><li>그래서 즐거움을 잃지 않도록 <b>프롬프트로만 그린다</b>는 선을 그었습니다.</li><li>같은 이유로 <b>음악과 동영상도 만들지 않습니다.</b></li><li>관리를 하지 않기 위해 <b>설치형 AI 도구도 사용하지 않습니다.</b></li></ul><h3 id="2-3-2-나를-드러낼-수-있도록-선을-한번-더-그었습니다"><a href="#2-3-2-나를-드러낼-수-있도록-선을-한번-더-그었습니다" class="headerlink" title="2.3.2. 나를 드러낼 수 있도록 선을 한번 더 그었습니다."></a>2.3.2. 나를 드러낼 수 있도록 선을 한번 더 그었습니다.</h3><ul><li><b>생성 AI의 피할 수 없는 특징</b>이 둘 있습니다.</li><li>하나는 <b>학습한 데이터로부터 결과물이 만들어진다</b>는 점이고,</li><li>또 하나는 <b>랜덤한 결과물이 나온다</b>는 점입니다.</li></ul><ul><li>이 두 특징으로 인해 <b>생성 AI 그림을 대하는 태도가 나뉩니다.</b></li><li>원래 그림을 그리지 않던 분들은 생성 AI 그림을 멋지다고 좋아하고,</li><li>원래 그림을 그리던 분들은 마음대로 되지 않는다고 답답해 합니다.</li><li><b>그림을 그리기 전에 떠오른 이미지가 있는가, 그 이미지가 충실히 구현되는가</b>의 차이라 생각됩니다.</li></ul><ul><li><b>잘 모르는 분야일수록 AI의 결과물에 불만이 없어지는 것</b>은 <b>분야와 무관하게 일반적인 현상</b>입니다.</li><li>그리고 이런 분들은 같은 AI를 쓰면서도 좋은 결과를 얻기 위해 노력하는 분들이 있다는 것을 상상하기 어렵습니다.</li></ul><ul><li><b>프롬프트 대충 넣으면 나오는 거 아니야?</b>라는 분도 있고,</li><li><b>베낄 이미지 넣고 그리라고 하면 금방 나오는데?</b>라는 분도 있습니다.</li></ul><ul><li>후처리를 하면서 시간을 많이 들이기는 싫지만, 대충 그리고 싶다는 뜻은 아닙니다.</li><li><b>머리 속에 있는 영상을 글을 통해서 최대한 재현하는 재미</b>를 느끼고자 합니다.</li><li>생성 AI를 사용하지만 <b>남의 그림을 베끼기 싫고</b>, <b>운을 최소화하고 싶습니다.</b></li><li>일반적인 방식으로 그림을 배우는 사람들도 기존 작품들의 영향을 받는 점을 인정합니다.</li><li>인간 화가들도 물감을 뿌리거나 던져서 그림을 그리듯 일부 운에 맡기는 영역이 있습니다.</li><li>기존 데이터와 운의 영향은 딱 여기까지만 받고 싶습니다.</li></ul><p><img src="71_hobbyart_08.png" alt="스스로에게 금기를 걸었습니다."></p><ul><li><b>사전 이미지를 입력하지 않습니다:</b> 대놓고 베끼겠다는 선언으로 느껴집니다.</li><li><b>작가 이름을 입력하지 않습니다: </b> 난 모르겠고 이런 스타일로 해달라는 말 같습니다.</li><li><b>장르 입력은 최소화합니다: </b> 이 정도 영향은 받을 수 있다고 생각하지만, 가급적 피합니다.</li></ul><ul><li>대신 <b>최대한 자세하게 프롬프트를 넣습니다.</b></li><li>제가 사용하는 ChatGPT에 탑재된 DALL.E는 100 단어 가량을 입력받게 되어 있습니다.</li><li>100 단어를 제가 채우지 않으면 자기가 알아서 상상해버립니다.</li><li><b>절대로 GPT나 클로드에게 프롬프트를 만들어 달라고 하지 않습니다.</b></li></ul><ul><li><b>프롬프팅도 영어로 합니다.</b></li><li>시스템 프롬프트를 열어보면 <b>영어가 아니면 영어로 번역하라</b>고 합니다.</li><li>이 과정에서 뉘앙스가 묘하게 달라지는 일을 많이 겪었습니다.</li><li><b>이 표현을 영어로 어떻게 하지?</b> 하고 찾아보는 과정도 하나의 재미입니다.</li></ul><p><img src="71_hobbyart_09.png" alt="내가 넣은 지시보다 셀프로 만든 프롬프트가 더 많으면 이건 누가 그린 걸까요?  "></p><ul><li>seed number에 붙는 랜덤성은 어쩔 수 없습니다. </li><li>아쉽긴 하지만 이 정도는 감수하기로 합니다. </li><li><b>배당된 seed number를 유지할 수 있다는 점</b>을 위안으로 삼습니다.</li></ul><h3 id="2-3-3-생각이나-감정을-시각화하고-이를-말로-바꿉니다"><a href="#2-3-3-생각이나-감정을-시각화하고-이를-말로-바꿉니다" class="headerlink" title="2.3.3. 생각이나 감정을 시각화하고, 이를 말로 바꿉니다."></a>2.3.3. 생각이나 감정을 시각화하고, 이를 말로 바꿉니다.</h3><ul><li>예를 들면 이런 식입니다.</li><li>숨이 막힐 만큼 답답한 날이 있었습니다.</li><li>그래도 어딘가 보이는 실낱같은 희망이 있어서 포기하지 않을 수 있었습니다.</li></ul><ul><li>이를 <b>어둠 속에서 빛을 바라보는 고양이</b>로 시각화했습니다.</li><li>자세한 설정들을 담아 프롬프트를 충분히 길게 작성해서 입력하며 그림을 얻습니다.</li><li>원하는 그림을 기다리며 계속 새로 만들면서 반영이 안되는 구상이 있는지 확인합니다.</li><li>프롬프트를 조금씩 바꿔가면서 원하는 그림으로 다가가다가, <b>최적의 그림을 선택</b>합니다.</li></ul><p><img src="71_hobbyart_10.png" alt="희망"></p><ul><li>손그림과는 달리 랜덤이 작용되기 때문에,</li><li>그리고 학습하지 않은 패턴은 구현되지 않기 때문에 내 머리 속이 100% 옮겨지기는 매우 어렵습니다.</li></ul><ul><li>AI가 내놓는 결과물 중에서 고르는, 일종의 <b>타협</b>이 필요하기도 하고</li><li><b>AI가 뭘 그릴 수 있을지 파악하고 그 안에서 나올 수 있는 결과를 요구하는 이해</b>가 필요하기도 합니다.</li><li>화가가 붓과 물감, 캔버스, 종이를 파악하는 과정과 비슷하다고 생각합니다.</li></ul><h2 id="2-4-저작권-문제"><a href="#2-4-저작권-문제" class="headerlink" title="2.4. 저작권 문제"></a>2.4. 저작권 문제</h2><ul><li>생성 AI 그림 뿐 아니라 글, 음악, 영화에 대한 <b>저작권 이슈</b>가 적지 않습니다.</li><li>생성 AI가 학습한 이미지들에 대한 저작권, 생성 AI 로 만들어진 그림에 대한 저작권이 모두 이슈이고</li><li>생성 AI가 학습한 이미지들에 대해서는 손해배상 소송이,</li><li>생성 AI가 생성한 이미지들에 대해서는 저작권 청구 소송이 이어지고 있습니다.</li></ul><p><img src="71_hobbyart_12.png" alt="생성 AI 그림의 저작권에 대한 의견"></p><ul><li>개인적으로는 <b>생성 AI가 학습한 이미지들에 대한 저작권은 존중되어야 하며</b></li><li><b>생성 AI가 생성한 이미지들도 일부 저작권이 인정될 수 있다</b>고 주장하는 바입니다.</li></ul><ul><li>저작권의 성립 요건에 <b>노동력 투입</b>이 배제되어 있다는 것이 첫번째 이유,</li><li><b>독창성</b>과 <b>사상이나 감정의 명시적 표현</b>이 포함되어 있다는 것이 두번째 이유입니다.</li><li>다만 목소리를 크게 내기는 아직 조심스럽습니다.</li></ul><ul><li><b>생성 AI에 남의 이미지를 업로드해서 만들어진 그림</b>은 독창성을 주장하기 쉽지 않을 것입니다.</li><li><b>남이 만들어준 프롬프트, 특히 AI가 만들어 준 프롬프트</b>를 넣어서 만들어진 그림도 마찬가지입니다.</li><li><b>프롬프트에 시각적인 내용이 없다면, 사상이나 감정을 명시적으로 표현했다고 주장하기 어려울 것</b>입니다.</li><li>하지만 고심해서 넣은 프롬프트로 그린 그림과 대충 넣은 그림은 구분할 수 없습니다.</li><li><b>그림에 담긴 의도</b>에 대한 작가의 설명만이 둘을 가를 수 있습니다. 이 경우 좋은 조수를 부린다고 볼 수 있습니다.</li><li>의도마저 AI에게 설명하라고 하고 이를 읊을 수 있겠지만, 자기 생각이 없다면 여러 작품에 걸친 일관성을 확보하기는 어려울 겁니다.</li></ul><p><img src="71_hobbyart_13.png" alt="조수 @Jehyun Lee"></p><ul><li>그러나, <b>나는 독창적이라고 생각했지만 결과를 보니 무수한 유사작들이 있다면 독창성을 주장하기 어려울 것입니다.</b></li><li>거꾸로 남들이 만든 LoRA를 조합해서 <b>나만의 스타일</b>을 만들었다면 독창성을 확보할 수 있다고 생각합니다.</li><li>말도 안되는 수식이나 랜덤한 글자를 넣어도 그림은 나옵니다. 이런 경우 <b>전위예술</b>에 가까울 수 있다고 생각합니다.</li><li>이런 복잡한 사연들 때문에 저작권을 일률적으로 적용하기는 몹시 어렵겠다는 생각이 듭니다.</li></ul><p><img src="71_hobbyart_14.png" alt="이미지 검색 결과 - 유사 그림의 홍수"></p><h1 id="3-그럼에도-불구하고"><a href="#3-그럼에도-불구하고" class="headerlink" title="3. 그럼에도 불구하고"></a>3. 그럼에도 불구하고</h1><ul><li>제가 그림을 그리는 목적은 돈을 벌기 위한 것이 아닙니다.</li><li>그렇다고 심오한 예술적 가치를 추구하기 위한 것도 아닙니다.</li><li>저작권에 대한 생각을 가끔 하기는 하지만, 그럴듯한 안을 언제까지 작성해서 제출해야 하는 입장에 있지 않습니다.</li></ul><ul><li>제게 그림은 <b>취미</b>입니다.</li><li>누가 시키지 않아도 스스로 하면서 재미를 느끼면 그 뿐입니다.</li><li>가끔 전시회에 참여해 뽐내고 싶은 유치한 마음을 실현하기도 하고,</li><li>울적한 날 우울을 떨치고 쌓인 스트레스를 털어버리며 지인들과 함께 한바탕 웃을 수 있는 소재가 되면 그걸로 족합니다.</li></ul><ul><li>제가 그림을 그릴 때 가장 느끼고 싶은 감정은 <b>자유로움</b>입니다.</li><li>제 그림을 보아 주는 분들이 가낭 느꼈으면 하는 감정은 어떤 형태건 <b>즐거움</b>입니다.</li><li>예술은 이래야 한다, 저래야 한다는 잣대와 무관하게 제가 좋으면 그만인 방식,</li><li>저 혼자 진지하며 만족하는 방식으로 시간을 보내는 것이 제가 그림을 그리는 목적입니다.</li></ul><ul><li>다른 분들도 각자 자신의 목적으로 그림을 그리고, 즐기고 계시리라 믿습니다.</li><li>제가 동의할 수 없는 관점, 저를 동의할 수 없는 관점도 있겠습니다만, 괜찮습니다.</li><li>이 그림은 이래야 한다거나 AI 그림은 이래야 한다고 주장하는 글이 아닌, <b>나는 이렇다</b>는 글이기 때문입니다.</li><li>다만 제가 그림에서 즐거움을 느끼듯, 다른 분들도 다른 형태로라도 그림에서 즐거움을 느끼면 좋겠다는 정도의 소망을 가져봅니다.</li></ul><p><img src="71_hobbyart_15.png"></p><ul><li>발표자료를 공유드립니다 <a href="241206_%EC%9D%B4%EC%A0%9C%ED%98%84_AI%EA%B7%B8%EB%A6%BC%EC%9D%B4%EB%9D%BC%EB%8A%94%EC%A7%84%EC%A7%80%ED%95%9C%EC%B7%A8%EB%AF%B8_blog.pdf">(다운로드)</a></li></ul>]]></content:encoded>
      
      
      <category domain="https://jehyunlee.github.io/categories/General/">General</category>
      
      
      <category domain="https://jehyunlee.github.io/tags/chatgpt/">chatgpt</category>
      
      <category domain="https://jehyunlee.github.io/tags/dalle/">dalle</category>
      
      <category domain="https://jehyunlee.github.io/tags/presentation/">presentation</category>
      
      
    </item>
    
    <item>
      <title>인공지능을 활용한 슬기로운 연구생활</title>
      <link>https://jehyunlee.github.io/2024/11/18/General-69_SNU/</link>
      <guid>https://jehyunlee.github.io/2024/11/18/General-69_SNU/</guid>
      <pubDate>Mon, 18 Nov 2024 12:29:00 GMT</pubDate>
      
        
        
      <description>&lt;ul&gt;
&lt;li&gt;서울대학교 응용물리연구소의 초청을 받아 대학원생 대상 강의를 드렸습니다.&lt;/li&gt;
&lt;li&gt;생성 AI를 이용한 연구 활용 강의지만, 평소보다 오용에 대한 주의를 몇배 더 강하게 드렸습니다.&lt;/li&gt;
&lt;li&gt;연구를 업으로 삼아야 하는 분들</description>
        
      
      
      
      <content:encoded><![CDATA[<ul><li>서울대학교 응용물리연구소의 초청을 받아 대학원생 대상 강의를 드렸습니다.</li><li>생성 AI를 이용한 연구 활용 강의지만, 평소보다 오용에 대한 주의를 몇배 더 강하게 드렸습니다.</li><li>연구를 업으로 삼아야 하는 분들입니다. 기술은 바뀌더라도 남을 마인드가 중요하다고 생각했습니다.</li></ul><h2 id="인공지능을-활용한-슬기로운-연구생활"><a href="#인공지능을-활용한-슬기로운-연구생활" class="headerlink" title="인공지능을 활용한 슬기로운 연구생활"></a>인공지능을 활용한 슬기로운 연구생활</h2><blockquote><p><a href="241118_%EC%9D%B4%EC%A0%9C%ED%98%84_%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5%EC%9D%84%ED%99%9C%EC%9A%A9%ED%95%9C%EC%8A%AC%EA%B8%B0%EB%A1%9C%EC%9A%B4%EC%97%B0%EA%B5%AC%EC%83%9D%ED%99%9C.pdf"><b>강의자료 다운로드 (219 pages)</b></a></p></blockquote><ul><li>기존 발표와 달리 <b>7개의 질문</b>으로 발표를 구성했습니다.</li><li>그간 여러 발표를 하면서 청중에게 들었던 질문들,</li><li>그리고 관련 일들을 하면서 스스로 가진 질문들입니다.</li></ul><ul><li><b>각 장을 질문으로 열고 답변으로 닫았습니다.</b></li><li>열리고 닫히는 사이에는 최대한 <b>예시</b>를 중심으로 담고자 했습니다.</li><li>직접 경험을 담을 수 없다면 <b>관련 논문</b>을 찾아 소개했습니다.</li><li>생성 AI는 완성품이 아니고, 아직 빠르게 진화하고 있는 중간 제품입니다.</li></ul><ul><li>개발자들조차 파악하지 못하는 특징들이 많기 때문에 <b>커뮤니티를 통한 경험의 공유가 가장 소중합니다.</b></li><li>3시간이라는 짧지 않은 강의 시간을 꽉 채워서 전달했음에도 불구하고 디테일이 적지 않게 생략되었습니다.</li><li>아쉽기도 하지만, <b>어차피 개인의 경험으로 채워야 합니다.</b></li></ul><h2 id="Q1-생성AI를-연구에-활용해도-되나요"><a href="#Q1-생성AI를-연구에-활용해도-되나요" class="headerlink" title="Q1. 생성AI를 연구에 활용해도 되나요?"></a>Q1. 생성AI를 연구에 활용해도 되나요?</h2><p><img src="241118_Jehyun_SNU_%ED%8E%98%EC%9D%B4%EC%A7%80_009.png" alt="조심할 것들이 많습니다."></p><ul><li>생성 AI를 연구에 사용해도 됩니다.</li><li>그런데 제대로 알고 써야 합니다.</li></ul><h3 id="Q1’-뭘-알고-써야-하나요"><a href="#Q1’-뭘-알고-써야-하나요" class="headerlink" title="Q1’. 뭘 알고 써야 하나요?"></a>Q1’. 뭘 알고 써야 하나요?</h3><p><img src="241118_Jehyun_SNU_%ED%8E%98%EC%9D%B4%EC%A7%80_022.png" alt="디아블로의 영혼석을 감당하지 못하면 디아블로가 됩니다."></p><ul><li><b>생성 AI의 능력과 한계</b>를 알고 써야 합니다.</li><li>그리고 <b>나의 능력과 한계</b>를 알고 써야 합니다.</li></ul><h3 id="Q2-생성-AI는-왜-이상한-답을-하나요"><a href="#Q2-생성-AI는-왜-이상한-답을-하나요" class="headerlink" title="Q2. 생성 AI는 왜 이상한 답을 하나요?"></a>Q2. 생성 AI는 왜 이상한 답을 하나요?</h3><p><img src="241118_Jehyun_SNU_%ED%8E%98%EC%9D%B4%EC%A7%80_030.png" alt="생성 AI는 호모 사피엔스를 닮았습니다."></p><ul><li>자기가 하는 답이 이상한 줄 모릅니다.</li><li>정확히는, <b>자기가 무슨 말을 하는 줄 모릅니다.</b></li></ul><p><img src="241118_Jehyun_SNU_%ED%8E%98%EC%9D%B4%EC%A7%80_040.png" alt="ChatGPT에게 하는 질문은 시간 낭비일 수 있습니다."></p><ul><li>그러면서 여기저기 눈치를 보느라 뻔한 답을 합니다.</li></ul><h3 id="Q2’-빅데이터를-학습했다면서요"><a href="#Q2’-빅데이터를-학습했다면서요" class="headerlink" title="Q2’. 빅데이터를 학습했다면서요?"></a>Q2’. 빅데이터를 학습했다면서요?</h3><p><img src="241118_Jehyun_SNU_%ED%8E%98%EC%9D%B4%EC%A7%80_046.png" alt="답을 하기 전에 재료를 찾아봅니다."></p><ul><li>당신은 배운 걸 다 기억하십니까?</li><li>생성 AI도 모르면 찾아보게 해줄 수 있습니다.</li><li><b>생성 증강 검색 - RAG: Retrieval Augmented Generation</b>이라는 기술입니다.</li></ul><h3 id="Q2’’-자기가-뱉은-말-다시-생각도-안합니까"><a href="#Q2’’-자기가-뱉은-말-다시-생각도-안합니까" class="headerlink" title="Q2’’. 자기가 뱉은 말 다시 생각도 안합니까?"></a>Q2’’. 자기가 뱉은 말 다시 생각도 안합니까?</h3><p><img src="241118_Jehyun_SNU_%ED%8E%98%EC%9D%B4%EC%A7%80_052.png" alt="포기하기 전에 목표와 행동을 반성합니다."></p><ul><li>생성 AI는 억울합니다. 생각할 기회도 받지 못했습니다.</li><li>자기가 뭘 하는지 살펴보게 하면 오류가 줄어듭니다.</li><li><b>자기반성 - Reflexion</b>이라는 기술입니다.</li></ul><h2 id="Q3-RAG도-이렇게-오류가-많은데-AI-쓰는-게-맞나요"><a href="#Q3-RAG도-이렇게-오류가-많은데-AI-쓰는-게-맞나요" class="headerlink" title="Q3. RAG도 이렇게 오류가 많은데 AI 쓰는 게 맞나요?"></a>Q3. RAG도 이렇게 오류가 많은데 AI 쓰는 게 맞나요?</h2><p><img src="241118_Jehyun_SNU_%ED%8E%98%EC%9D%B4%EC%A7%80_057.png" alt="과학, 기술 분야 출판 논문 동향"></p><ul><li>사람이 읽기에는 너무 많은 논문이 쏟아져 나옵니다.</li><li><b>정보량이 인간의 기억을 넘을 때 문자를 쓰기 시작</b>했듯,</li><li><b>정보량이 인간의 사고 능력을 넘는 지금</b> AI를 쓰기 시작해야 합니다.</li></ul><ul><li><b>실보다 득이 많도록 운영을 잘 하셔야 합니다.</b></li></ul><h2 id="Q4-뭔가-많이-어려워-보입니다-환각을-줄이려면-이렇게까지-해야만-하나요"><a href="#Q4-뭔가-많이-어려워-보입니다-환각을-줄이려면-이렇게까지-해야만-하나요" class="headerlink" title="Q4. 뭔가 많이 어려워 보입니다. 환각을 줄이려면 이렇게까지 해야만 하나요?"></a>Q4. 뭔가 많이 어려워 보입니다. 환각을 줄이려면 이렇게까지 해야만 하나요?</h2><p><img src="241118_Jehyun_SNU_%ED%8E%98%EC%9D%B4%EC%A7%80_070.png" alt="생성 AI 환각 억제 방법"></p><ul><li><b>모르면 모른다고 말할 자유</b>를 주세요.</li><li>넘겨짚지 않도록 <b>상황을 정확히 설명</b>하세요.</li><li><b>웹 검색을 하고 출처를 요청</b>하세요.</li><li><b>추론 과정을 단계별로 요청</b>하세요.</li></ul><p><img src="241118_Jehyun_SNU_%ED%8E%98%EC%9D%B4%EC%A7%80_079.png" alt="장점 위주 사용 방법"></p><ul><li><b>할 수 있는 일들을 할 수 있게 하세요.</b></li><li><b>못 하는 일 시키고 못 한다고 구박하지 마세요.</b></li></ul><h2 id="Q5-현-상황에서는-RAG가-최선으로-보입니다-연구비를-들여-개발-의뢰를-해야-하나요"><a href="#Q5-현-상황에서는-RAG가-최선으로-보입니다-연구비를-들여-개발-의뢰를-해야-하나요" class="headerlink" title="Q5. 현 상황에서는 RAG가 최선으로 보입니다. 연구비를 들여 개발 의뢰를 해야 하나요?"></a>Q5. 현 상황에서는 RAG가 최선으로 보입니다. 연구비를 들여 개발 의뢰를 해야 하나요?</h2><p><img src="241118_Jehyun_SNU_%ED%8E%98%EC%9D%B4%EC%A7%80_106.png" alt="생성 AI 활용 도구들"></p><ul><li><b>나와 있는 도구들</b>만 잘 쓰셔도 충분합니다.</li><li><b>무료와 유료의 구독 성능차이</b>가 생각보다 큽니다.</li><li><b>연구비 처리가 가능</b>하니 교수님을 졸라보세요.</li></ul><h2 id="Q6-실제로-어떤-일들을-했나요"><a href="#Q6-실제로-어떤-일들을-했나요" class="headerlink" title="Q6. 실제로 어떤 일들을 했나요?"></a>Q6. 실제로 어떤 일들을 했나요?</h2><p><img src="241118_Jehyun_SNU_%ED%8E%98%EC%9D%B4%EC%A7%80_174.png" alt="공공/연구기관 활용 업무 기준 인공지능 분류"></p><ul><li><b>데이터 분석</b>부터 <b>연구업무 수행</b>까지 다 합니다.</li></ul><h2 id="Q7-당장-GPT밖에-쓰는-게-없는데-이걸로-할-수-있는-게-있나요"><a href="#Q7-당장-GPT밖에-쓰는-게-없는데-이걸로-할-수-있는-게-있나요" class="headerlink" title="Q7. 당장 GPT밖에 쓰는 게 없는데, 이걸로 할 수 있는 게 있나요?"></a>Q7. 당장 GPT밖에 쓰는 게 없는데, 이걸로 할 수 있는 게 있나요?</h2><p><img src="241118_Jehyun_SNU_%ED%8E%98%EC%9D%B4%EC%A7%80_211.png" alt="연구용 GPTs 4형제"></p><ul><li><b>GPTs</b>를 써보세요.</li><li>직접 만드시면 제일 좋고, 있는 걸 쓰셔도 좋아요.</li></ul><blockquote><p><a href="241118_%EC%9D%B4%EC%A0%9C%ED%98%84_%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5%EC%9D%84%ED%99%9C%EC%9A%A9%ED%95%9C%EC%8A%AC%EA%B8%B0%EB%A1%9C%EC%9A%B4%EC%97%B0%EA%B5%AC%EC%83%9D%ED%99%9C.pdf"><b>강의자료 다운로드 (219 pages)</b></a></p></blockquote>]]></content:encoded>
      
      
      <category domain="https://jehyunlee.github.io/categories/General/">General</category>
      
      
      <category domain="https://jehyunlee.github.io/tags/openai/">openai</category>
      
      <category domain="https://jehyunlee.github.io/tags/chatgpt/">chatgpt</category>
      
      <category domain="https://jehyunlee.github.io/tags/presentation/">presentation</category>
      
      
    </item>
    
    <item>
      <title>생성AI 활용 학회 발표 준비</title>
      <link>https://jehyunlee.github.io/2024/11/07/General-67_afore2024/</link>
      <guid>https://jehyunlee.github.io/2024/11/07/General-67_afore2024/</guid>
      <pubDate>Thu, 07 Nov 2024 14:29:00 GMT</pubDate>
      
        
        
      <description>&lt;ul&gt;
&lt;li&gt;오랜만에 연구 결과를 국제학회에서 발표했습니다.&lt;/li&gt;
&lt;li&gt;일정이 바빴다는 핑계를 댈 수도 있지만 생성 AI의 능력과 그간 쌓인 노하우를 믿었습니다.&lt;/li&gt;
&lt;li&gt;밀도있게 3일 준비해서 발표를 마쳤습니다. 자세하게 말씀드릴 수</description>
        
      
      
      
      <content:encoded><![CDATA[<ul><li>오랜만에 연구 결과를 국제학회에서 발표했습니다.</li><li>일정이 바빴다는 핑계를 댈 수도 있지만 생성 AI의 능력과 그간 쌓인 노하우를 믿었습니다.</li><li>밀도있게 3일 준비해서 발표를 마쳤습니다. 자세하게 말씀드릴 수는 없지만 노하우를 공유합니다.</li></ul><h1 id="2024-09-14-초록-제출"><a href="#2024-09-14-초록-제출" class="headerlink" title="2024.09.14. - 초록 제출"></a>2024.09.14. - 초록 제출</h1><blockquote><p><a href="https://www.elsevier.com/products/scopus/content">Scopus: Scopus content</a></p></blockquote><h2 id="1-결론은-가지고-시작"><a href="#1-결론은-가지고-시작" class="headerlink" title="(1) 결론은 가지고 시작"></a>(1) 결론은 가지고 시작</h2><ul><li>여러 이유가 겹쳐 지난 1년간 연구 결과를 발표하지 못했습니다.</li><li>오랜만에 학회에 발표할 기회가 생겼으나 초록 제출 기한이 매우 촉박했습니다.</li><li>큰 결론은 기존 업무를 수행하며 알고 있었기 때문에 <b>초록을 작성하기 위한 최소한의 지식을 정리할 필요가 있었습니다.</b></li></ul><h2 id="2-대량의-논문에서-필요한-논문들-선별"><a href="#2-대량의-논문에서-필요한-논문들-선별" class="headerlink" title="(2) 대량의 논문에서 필요한 논문들 선별"></a>(2) 대량의 논문에서 필요한 논문들 선별</h2><ul><li><b>저희 분야에서 논문에 쓰일 만한 지식은 일반 검색 엔진에서 찾기 어렵습니다.</b></li><li>공개된 데이터를 잘 찾아주는 <code>perplexity</code>등의 도구들이 많이 나와 있지만,</li><li>라이선스를 확보하고 논문을 검색해야 하는 scopus 등 데이터베이스를 이용해야 하기 때문에 다른 방법을 찾아야 합니다.</li><li>다행히 <b>Scopus API</b>를 사용할 수 있는 환경을 구축해 두었습니다.</li></ul><p><img src="67_AFORE2024_01.png" alt="Scopus API"></p><ul><li><b>Scopus API로 초록을 대량으로 확보</b>하고,</li><li><b>RAG</b>를 사용해 관심사에 가까운 정보들을 수집하여 정리하는 시스템을 보유하고 있습니다.</li><li>확보한 대량의 데이터로는 <b>경향성을 확인</b>하고 읽어야겠다 싶은 논문은 PDF를 내려받아 읽습니다.</li></ul><h2 id="3-그림으로-내용-파악"><a href="#3-그림으로-내용-파악" class="headerlink" title="(3) 그림으로 내용 파악"></a>(3) 그림으로 내용 파악</h2><ul><li>PDF 파일은 정독하기 전에 <b>그림부터 봅니다.</b></li></ul><p><img src="67_AFORE2024_02.png" alt="Acrobat PDF viewer"></p><ul><li>생성 AI가 나오기 전부터 갖고 있던 습관이지만 그림부터 보는 데는 몇 가지 이유가 있습니다.</li></ul><p><b>1. 논문을 쓸 때 그림부터 순서대로 정리합니다.</b></p><ul><li>어떤 그림들이 들어가야 할 지 설계하고, 이 그림들을 그리면서 논리 전개를 고민합니다.</li><li>그렇기 때문에 그림만 제대로 보아도 논문의 중요한 내용들을 파악할 수 있습니다.</li></ul><p><b>2. 그림에 많은 정보가 담겨 있을 뿐 아니라 인지가 빠릅니다.</b></p><ul><li>현미경 사진이나 중요한 그래프는 그림 속에 중요 정보를 모두 담고 있습니다.</li><li>대개 그림에 담긴 정보는 글에도 써 있기 마련이지만 같은 정보를 그림으로 보는 것이 수 배 빠릅니다.</li></ul><ul><li>그림으로 내용을 대강 파악했다면 <code>논문봇</code>을 사용해 전체적인 가치를 판단합니다.</li><li>여기서 가치는 논문이 가지는 절대적인 가치(if any)가 아니라 나에게 도움이 되는지 여부입니다.</li><li><b>불필요하다면 과감하게 버립니다.</b> 읽어야 할 논문은 어차피 많습니다.</li></ul><ul><li>많은 분들이 오해하십니다만, <b>논문봇은 논문을 읽기 위한 도구가 아닙니다.</b></li><li><b>논문봇은 버릴 논문을 선별할 때 쓰기 위한 도구입니다.</b></li></ul><h2 id="4-필요-부분-정독"><a href="#4-필요-부분-정독" class="headerlink" title="(4) 필요 부분 정독"></a>(4) 필요 부분 정독</h2><ul><li>버릴 논문을 버렸다면 읽을 논문을 읽을 차례입니다.</li><li><b>모든 글자를 빠짐없이 읽기엔 시간은 적고 할 일은 많습니다.</b></li><li><code>scispace</code>, <code>notebooklm</code> 등의 도구를 사용해 내용을 파악합니다.</li><li><b>논문 파일을 올려서 그림을 중심으로 파악한 내용들을 질문합니다.</b></li></ul><p><img src="67_AFORE2024_03.png" alt="scispace"></p><ul><li><code>scispace</code>나 <code>notebooklm</code>은 답변과 함께 레퍼런스 링크를 줍니다.</li><li>이 링크를 클릭하면 입력한 본문 중 답변을 인용한 부분이 하이라이트됩니다.</li><li>문서를 올리고 질문을 해도 <b>환각이 발생할 확률이 있기 때문에 검증이 매우 중요합니다.</b></li></ul><ul><li>이제 논문을 머리로 정리할 필요가 있습니다.</li><li><b>눈을 감고 내용들을 머리로 정리</b>해 보아도 좋고,</li><li><b>백지와 펜을 들고 그림을 그리며 정리</b>를 해 보는 것도 좋습니다.</li><li>무엇이건 <b>AI의 도움을 받지 않고 내 머리로 정리하는 것이 중요합니다.</b></li></ul><p><img src="67_AFORE2024_04.png" alt="사색"></p><ul><li>함께 이야기를 나눌 동료가 있다면 더 좋습니다.</li><li>그 과정에서 허점이 발견되기 더 쉽기 때문입니다.</li></ul><h2 id="5-초록-작성"><a href="#5-초록-작성" class="headerlink" title="(5) 초록 작성"></a>(5) 초록 작성</h2><ul><li>이 정도 되면 초록에 담길 내용이 정리됩니다. </li><li>파이썬이나 R, 엑셀 중 원하는 도구를 써도 되지만 <code>플랏봇</code>도 써볼 만 합니다.</li><li><code>ChatGPT</code>나 <code>Claude</code>에 내용을 개조식으로 넣고 글을 작성해달라고 합니다.</li></ul><ul><li>중요한 점이 있습니다.</li><li><b>AI가 작성한 문장을 의도적으로 모두 바꾸는 것</b>입니다.</li><li>직접 살펴보며 흡족하지 못한 문장을 바꾸어도 되지만, <code>DeepL</code>과 같은 도구를 사용해도 좋습니다.</li><li>자동으로 번역된 문장을 보면서 이 중 하나를 선택하는 과정을 통해 <b>스스로 생각하는 기회</b>를 가질 수 있습니다.</li></ul><ul><li><b>수정을 반복하며 <code>딴지봇</code>에게 검사를 요청합니다.</b></li><li>딴지봇에 특별한 대단한 지식이 들어있지는 않지만 논리적 허점을 찾아 반박해주는 기능이 있습니다.</li><li>딴지봇이 지적한 허점이 진짜 허점인지 스스로 고민해 보고, 타당하다고 생각되면 수정합니다.</li></ul><h1 id="2023-11-05-06-화-수-발표-준비-Day-1-2"><a href="#2023-11-05-06-화-수-발표-준비-Day-1-2" class="headerlink" title="2023.11.05-06 (화-수) - 발표 준비 Day 1-2"></a>2023.11.05-06 (화-수) - 발표 준비 Day 1-2</h1><h2 id="6-그림모음-ppt-준비"><a href="#6-그림모음-ppt-준비" class="headerlink" title="(6) 그림모음 ppt 준비"></a>(6) 그림모음 ppt 준비</h2><ul><li>발표 준비를 할 때 가장 처음 할 일은 <b>빈 파워포인트 파일을 만드는 것</b>입니다.</li><li>발표에 사용할 그림들을 하나씩 준비하면서 파워포인트 파일에 추가합니다.</li></ul><p><img src="67_AFORE2024_05.png" alt="그림모음 ppt"></p><ul><li>일단은 <b>내 머리 속에 있는 논리를 꺼내놓는 데 집중합니다.</b></li><li>그러려면 데이터를 분석하고 그림을 그리는 코드도 마구잡이로 짜면 곤란합니다.</li><li><b><code>플랏봇</code>을 비롯해 <code>GPT</code>, <code>Claude</code>에게 너무 의존하면 코드가 중구난방이 됩니다.</b></li><li>코딩 AI를 쓰지 말라는 말씀이 아닙니다. 기왕이면 <code>cursor.AI</code> 같은 전용 도구를 쓰는 것이 여러 모로 더 좋습니다.</li></ul><p><img src="67_AFORE2024_06.png" alt="cursor.AI"></p><ul><li>중간중간 <b>markdown cell</b>을 넣으면서 목차도 만듭니다. 전에 짰던 코드를 되짚을 때 좋습니다.</li><li><b>두 번 쓰인다 싶은 기능은 함수로 만듭니다.</b> 작성되는 코드는 반드시 수정되기 때문에 함수가 좋습니다.</li><li><b>함수가 쌓이면 라이브러리로 만듭니다.</b> 메인 코드가 엄청나게 짧아집니다.</li><li>그래프를 파일로 출력해도 좋지만, 파일명 지옥에 빠질 수 있습니다. 저는 꼭 필요한 것이 아니면 웬만하면 복붙합니다.</li></ul><p><img src="67_AFORE2024_07.png" alt="색상표"></p><ul><li>데이터 시각화에서 <b>색상(hue)은 단어에 해당됩니다.</b></li><li>특정 데이터에 대응되는 특정 색상이 있다면 전달력이 강해지고 보기에도 좋습니다.</li><li>자주 사용되는 색에 이름을 붙이면 좋습니다. <b>dictionary</b>를 저장하면 꺼내 쓰기 좋습니다.</li></ul><ul><li><b>명도(lightness)는 데이터의 크기에 해당됩니다.</b></li><li>데이터가 크면 밝은 색, 데이터가 작으면 어두운 색으로 칠하면 직관적으로 데이터를 느낄 수 있습니다.</li><li><b>채도(saturation)는 데이터의 중요도에 해당됩니다.</b></li><li>중요한 데이터는 채도를 높게 칠하고, 중요하지 않은 데이터는 채도를 낮게 칠합니다.</li><li>색상으로 표현해야 할 데이터가 많아 구분이 어려울 때 채도를 함께 조절해주면 숨통이 트입니다.</li></ul><p><img src="67_AFORE2024_08.png" alt="ppt에 붙인 그림"></p><ul><li>이렇게 그려진 그림들은 ppt에 차곡 차곡 붙여 넣습니다.</li><li>이 때, 기왕이면 <b>최종적으로 발표될 장면들을 생각하면서 정리하면 좋습니다.</b></li><li>그림 설명도 발표에 사용하는 폰트와 크기를 이용해 붙여 넣고, 이에 따라 종횡비도 미리 지정합니다.</li><li>저는 16:9 비율 슬라이드를 사용합니다. </li><li>그림 크기를 (20, 10)으로 지정하면 상단에 제목이 들어갈 공간이 남고 글자 크기도 적절합니다.</li></ul><h2 id="7-설명자료-준비"><a href="#7-설명자료-준비" class="headerlink" title="(7) 설명자료 준비"></a>(7) 설명자료 준비</h2><ul><li>머리 속에 마련된 시나리오에 따라 발표자료를 준비하다 보면 <b>확인할만한 정보</b>가 떠오릅니다.</li><li>근거를 강화시켜 줄 레퍼런스가 필요한 경우도 있고, 추가적인 설명이 필요한 경우도 있습니다.</li><li>이럴 때 사용하는 것이 <b>설명자료</b>이며 <code>perplexity</code>를 사용하면 빠르게 확보할 수 있습니다.</li></ul><p><img src="67_AFORE2024_09.png" alt="perplexity 화면 캡처"></p><ul><li><code>perplexity</code>의 library 항목에 가면 과거에 질의한 내역을 모두 볼 수 있습니다.</li><li>그러나 작업을 하다 보면 비슷한 질의를 많이 하게 되어 이 중 어떤 것인지 혼동됩니다.</li><li>또한, 눈에 보이지 않으면 내가 그런 정보를 수집했는지조차 잊어버릴 때가 많습니다. 너무 많은 자료를 검색하니까요.</li><li>이럴 때를 대비하여 <b>쓸만한 답변들을 ppt에 함께 붙여둡니다. 가능한 큰 그림으로요.</b></li></ul><p><img src="67_AFORE2024_10.png" alt="레퍼런스 캡처"></p><ul><li><code>perplexity</code> 답변만 볼 것이 아니라 <b>레퍼런스를 클릭하여 직접 확인해보는 것이 좋습니다.</b></li><li>드물기는 하지만 레퍼런스 자체가 텅 비어 있는 경우가 있기도 하고 - 이럴 때는 답변을 쓰지 않는 것이 좋습니다.</li><li>레퍼런스를 따라 들어가면 <b><code>perplexity</code> 답변보다 좋은 정보를 얻을 수 있을 때가 많습니다.</b></li><li>다른 이들의 자료를 인용할 때는 종류가 무엇이건 출처를 명확히 표기합니다.</li></ul><h1 id="2023-11-07-목-발표-준비-Day-3-amp-발표-당일"><a href="#2023-11-07-목-발표-준비-Day-3-amp-발표-당일" class="headerlink" title="2023.11.07 (목) - 발표 준비 Day 3 &amp; 발표 당일"></a>2023.11.07 (목) - 발표 준비 Day 3 &amp; 발표 당일</h1><h2 id="8-스토리텔링"><a href="#8-스토리텔링" class="headerlink" title="(8) 스토리텔링"></a>(8) 스토리텔링</h2><ul><li>이틀 정도를 꼬박 투자하면서 행정업무도 처리하고, 회의도 들어가고, 전화도 받았습니다.</li><li><b>발표 준비가 웬만큼 진행된다고 느껴지지만 스토리가 나온다고 보기는 어려운 상황입니다.</b></li><li>할 말을 다 한다고 좋은 발표는 아니라고 생각합니다.</li><li><b>슬라이드 쇼</b>를 하는 발표자로서 <b>청중의 이목을 붙잡아 둘 요소</b>가 필요합니다.</li><li>저는 개그가 정말 중요하다고 생각합니다.</li></ul><p><img src="67_AFORE2024_11.png" alt="발표에 동원된 개그짤"></p><ul><li>딱딱하기 딱 좋은 발표에 등장하는 개그는 여러 효능이 있습니다.</li><li><b>청중이 자발적으로 집중</b>하도록 만들고, </li><li><b>어려울 수 있는 내용이 쉽게 느껴지도록 착각</b>을 불러 일으킵니다.</li><li><b>청중과 연사의 감정이 동조</b>되면서 이해도가 좋아지기 때문입니다.</li></ul><ul><li>그렇다고 뜬금없이 몸개그를 할 수는 없는 노릇입니다.</li><li><b>전체적인 스토리에 잘 녹아들면서 뼈가 있는 개그를 선호합니다.</b></li><li>잠시 웃고 정색하며 발표로 넘어가기 좋기 때문입니다.</li></ul><h2 id="9-데이터-밖에서-인사이트-가져오기"><a href="#9-데이터-밖에서-인사이트-가져오기" class="headerlink" title="(9) 데이터 밖에서 인사이트 가져오기"></a>(9) 데이터 밖에서 인사이트 가져오기</h2><ul><li>데이터 분석을 중심으로 발표를 하다 보면 <b>얻고 싶은 답이 보이지 않을 때가 많습니다.</b></li><li>정상입니다. 여러 요인들이 교차하는 가운데 사건이나 문제가 발생하고, 이들의 단편이 데이터이기 때문입니다.</li><li>데이터를 이용해서 현상을 설명하기엔 좋지만 충분한 인사이트를 주기에는 부족할 때가 많습니다.</li><li>데이터 밖으로 눈을 돌려 <b>이 사건이나 문제가 발생한 원인</b>을 짚어보는 것이 필요할 때가 있습니다.</li></ul><p><img src="67_AFORE2024_13.png" alt="88 페이지 분량 보고서"></p><ul><li>이런 추가 정보 소스가 분량이 많을 때가 또 한번의 고비가 됩니다.</li><li>특히 내가 원하는 답이 있음이 분명한 소스를 찾았을 때, 무시할 수도 없고 읽기는 버거운 상황이 발생합니다.</li><li>제가 접한 여러 도구 중 <b><code>NotebookLM</code>만큼 이럴 때 도움이 되는 것이 없는 듯 합니다.</b></li></ul><p><img src="67_AFORE2024_12.png" alt="NotebookLM"></p><ul><li><code>NotebookLM</code>은 대량의 문서를 넣고 질의응답을 하기도 좋지만,</li><li>긴 문서를 남녀 두 사람이 진행하는 podcast처럼 만들어주는 기능이 매우 강력합니다.</li><li><b>Audio Overview</b> 라고 하며 특히 고속버스나 비행기, 운전 등 장기간 이동을 할 때 유리합니다.</li><li><b>귀에 관련 정보를 계속 흘려보내줄 수 있기 때문</b>이며 원본 문서가 길 수록 이 기능이 더욱 강력해집니다.</li></ul><ul><li>현재는 영어밖에 지원하지 않지만 영어로 발표해야 하는 국제학회라면 오히려 좋습니다.</li><li>대화 속에 좋은 영어 표현들이 다수 담겨있거든요.</li></ul><p><img src="67_AFORE2024_14.jpg" alt="귀로 듣고 눈으로 보며 정리한 메모"></p><p><img src="67_AFORE2024_15.png" alt="메모를 정리한 ppt 슬라이드"></p><ul><li>다만, 늘 그렇지만 이런 <b>요약본을 곧이곧대로 쓰면 곤란합니다.</b></li><li>환각이 끼어들 여지가 너무나 많기 때문에 내용 파악용으로만 사용해야 합니다.</li><li>다시 메모지를 펴고 이어폰을 꽂은 채 원본 문서에서 관련 내용을 탐색하며 확인합니다.</li><li>그리고 원본과 오디오에서 정리된 양식과 다르게, <b>나만의 시각으로 ppt에 정보를 재구성합니다.</b></li></ul><ul><li>무대에 올라 발표를 하는 시점에서 이 <b>모든 자료와 내 말에 대한 책임은 스스로 져야 합니다.</b></li><li>진정성을 담아 발표할 수 있도록, 그리고 발표하는 만큼은 암기가 아니라 이해를 할 수 있도록 준비해야 합니다.</li><li>생성AI는 좋은 도구이지만 <b>내가 생성AI를 통제하지 못하면, 생성AI가 나를 통제합니다.</b></li></ul>]]></content:encoded>
      
      
      <category domain="https://jehyunlee.github.io/categories/General/">General</category>
      
      
      <category domain="https://jehyunlee.github.io/tags/openai/">openai</category>
      
      <category domain="https://jehyunlee.github.io/tags/chatgpt/">chatgpt</category>
      
      <category domain="https://jehyunlee.github.io/tags/notebooklm/">notebooklm</category>
      
      
    </item>
    
    <item>
      <title>한국에너지기술연구원 진로 티처 - AI계산과학</title>
      <link>https://jehyunlee.github.io/2024/11/04/General-68_kierinterview/</link>
      <guid>https://jehyunlee.github.io/2024/11/04/General-68_kierinterview/</guid>
      <pubDate>Mon, 04 Nov 2024 14:29:00 GMT</pubDate>
      
        
        
      <description>&lt;ul&gt;
&lt;li&gt;제가 근무하는 한국에너지기술연구원에서 진행하는 진로 티처 영상에 참여하였습니다.&lt;/li&gt;
&lt;li&gt;AI·계산과학이라는 분야에 대해 소개하는 영상으로,&lt;/li&gt;
&lt;li&gt;저 외에 공정/엔지니어링 분야의 박정호 박사님, 배터리 소재의 AI 적</description>
        
      
      
      
      <content:encoded><![CDATA[<ul><li>제가 근무하는 한국에너지기술연구원에서 진행하는 진로 티처 영상에 참여하였습니다.</li><li>AI·계산과학이라는 분야에 대해 소개하는 영상으로,</li><li>저 외에 공정/엔지니어링 분야의 박정호 박사님, 배터리 소재의 AI 적용을 연구하는 이찬우 박사님이 참여했습니다.</li></ul><h3 id="한국에너지기술연구원-에너지AI·계산과학실"><a href="#한국에너지기술연구원-에너지AI·계산과학실" class="headerlink" title="한국에너지기술연구원 에너지AI·계산과학실"></a>한국에너지기술연구원 에너지AI·계산과학실</h3><blockquote><p><a href="https://www.kier.re.kr/board?menuId=MENU01040&siteId=null">한국에너지기술연구원 에너지AI·계산과학실</a></p></blockquote><ul><li>출연연마다 AI 조직을 다른 형식으로 운영하고 있습니다.</li><li>한국에너지기술연구원은 <b>연구부서별로 AI 역량이 있는 인력을 채용하여 운영</b>하는 한편,</li><li><b>에너지AI·계산과학실을 통해 연구원 전 부서의 AI·계산과학 역량을 강화</b>하고 있습니다.</li></ul><ul><li>에너지AI·계산과학실은 이름처럼 <b>AI와 계산과학을 모두 담당</b>합니다.</li><li><b>원자단위 전산모사</b>, <b>전산유체역학(CFD)</b>, <b>공정/엔지니어링</b>, <b>데이터/AI</b>분야의 전문가들로 구성되어,</li><li>적은 인원이지만 원내 수십 개의 연구과제의 참여하여 공동연구를 수행합니다.</li></ul><p><img src="68_kierinterview_01.png" alt="에너지AI·계산과학실"></p><ul><li>연구원에서 <b>진로 티처</b>라는 프로그램으로 우리가 수행하는 업무를 설명하는 기회를 가졌습니다.</li></ul><div class="video-container"><iframe src="https://www.youtube.com/embed/0cLKAVc3r_A" frameborder="0" loading="lazy" allowfullscreen></iframe></div><ul><li>제가 드리고자 한 메시지는 다음과 같습니다.</li><li><b>본인의 데이터를 스스로 분석해서 인사이트를 도출하는 역량은 연구원의 기본기입니다.</b></li><li>심리적 장벽을 넘어 본인의 세계를 넓히시기 바랍니다.</li></ul><p><img src="68_kierinterview_02.png"></p>]]></content:encoded>
      
      
      <category domain="https://jehyunlee.github.io/categories/General/">General</category>
      
      
      <category domain="https://jehyunlee.github.io/tags/youtube/">youtube</category>
      
      <category domain="https://jehyunlee.github.io/tags/KIER/">KIER</category>
      
      
    </item>
    
    <item>
      <title>태재미래교육포럼 - 실용적 AI 업무 활용 방안</title>
      <link>https://jehyunlee.github.io/2024/10/15/General-70_Taejae/</link>
      <guid>https://jehyunlee.github.io/2024/10/15/General-70_Taejae/</guid>
      <pubDate>Tue, 15 Oct 2024 12:29:00 GMT</pubDate>
      
        
        
      <description>&lt;ul&gt;
&lt;li&gt;“교육의 미래: 사라지는 것과 생겨나는 것”이라는 주제로 태재미래교육포럼이 열렸습니다.&lt;/li&gt;
&lt;li&gt;교육에 대해 고민하는 많은 연사들의 발표 가운데, 저도 평소의 고민을 담아 한 말씀을 드렸습니다.&lt;/li&gt;
&lt;li&gt;“안경을 쓴다고 </description>
        
      
      
      
      <content:encoded><![CDATA[<ul><li>“교육의 미래: 사라지는 것과 생겨나는 것”이라는 주제로 태재미래교육포럼이 열렸습니다.</li><li>교육에 대해 고민하는 많은 연사들의 발표 가운데, 저도 평소의 고민을 담아 한 말씀을 드렸습니다.</li><li>“안경을 쓴다고 눈이 좋아지지 않듯, AI를 쓴다고 스스로의 역량이 강화되지 않습니다. 별도의 노력이 필요합니다.”</li></ul><h3 id="실용적-AI-업무-활용-방안"><a href="#실용적-AI-업무-활용-방안" class="headerlink" title="실용적 AI 업무 활용 방안"></a>실용적 AI 업무 활용 방안</h3><blockquote><p><a href="https://www.ancient-origins.net/news-evolution-human-origins/scientists-are-alarmed-shrinking-human-brain-001446">ancient origins: Scientists are alarmed by shrinking of the human brain</a><br><a href="https://doi.org/10.1002/ajpa.22476">Liu et al., American Journal of Physical Anthropology (2014) DOI: 10.1002/ajpa.22476</a></p></blockquote><ul><li>아직 많은 분들이 23년 봄에 언론을 뒤덮은 ChatGPT를 기억하고 계십니다.</li><li><b>빅데이터를 학습해서 무슨 질문을 해도 답을 하는 차세대 검색엔진</b>이라는 맹신입니다.</li><li>반면 조금 고민하면서 써 보신 분들은 <b>말은 재밌게 하는데, 거짓말도 잘 하는, 그래서 실전에는 쓸모가 없는 녀석</b>이라는 실망으로도 기억하십니다.</li></ul><ul><li><b>맹신은 틀렸고 실망은 맞습니다.</b></li><li>그러나 지금의 ChatGPT를 비롯한 AI들은 실망으로 그치지 않습니다.</li><li>실질적 <b>추론 능력</b>을 상당 수준으로 탑재하고 있고 <b>웹 검색</b>을 해서 정보를 찾아보기도 합니다.</li><li>그리고 스스로 코드를 실행하기도 하고 남의 코드와 API 형태로 엮여서 더 강력해졌습니다.</li></ul><div class="video-container"><iframe src="https://www.youtube.com/embed/WpQt0x86A5M" frameborder="0" loading="lazy" allowfullscreen></iframe></div><ul><li>어떤 분들께서는 말씀하십니다.</li><li><b>생성 AI를 통해서 인간의 능력이 강화된다</b>고요.</li><li>또 어떤 분들께서는 말씀하십니다.</li><li><b>생성 AI를 쓰다 보면 사고 능력을 잃어버릴 수 있다</b>는 말씀입니다.</li></ul><ul><li><b>둘 다 맞다고 생각합니다.</b></li><li>결과적으로는 역량이 강화되지만 육체의 능력은 약화됩니다.</li><li>저는 눈이 나빠서 <b>안경</b>을 씁니다.</li><li>눈이 나쁜 저도 안경을 쓰면 글자도 잘 볼 수 있고 반찬도 집어먹을 수 있습니다.</li><li>그렇지만 안경을 오래 썼다고 시력이 좋아지진 않습니다.</li></ul><ul><li><b>신체 능력을 도구에 의탁함으로써 강해진다면</b></li><li>안경을 오래 썼으니 <b>시력이 좋아져야</b> 하고,</li><li>대중교통을 포함한 차를 오래 탔으니 <b>걷기와 달리기 능력은 강해져야 하며,</b></li><li>전화번호를 휴대폰에 저장한 지 오래니 <b>웬만한 전화번호는 다 외워야 합니다.</b></li><li>하지만 현실은 그 반대입니다. </li><li><b>눈은 여전히 나쁘고, 몸에는 살이 찌고, 자주 전화하는 동료의 번호도 외우지 못합니다.</b></li></ul><p><img src="70_Taejae_brain.jpg" alt="Scientists are alarmed by shrinking of the human brain"></p><ul><li><b>인간 개개인의 신체능력은 원시시대에 가장 강했다</b>는 말이 있습니다.</li><li>정확히 말하면, 신체능력이 약한 사람들은 원시시대에서는 살아남을 수 없었을 겁니다.</li><li>제가 안경을 벗고 원시로 간다면 곰에게 잡혀먹히거나, 사냥을 못해서 굶어 죽었을 테니까요.</li><li>이렇게 나약한 저도 <b>생존할 수 있게 해 주는 것이 문명</b>입니다.</li></ul><p><img src="70_Taejae_kangaroo.jpg" alt="살아남은 자가 강한 것이다. (c)BBC"></p><ul><li>시력 저하를 막기 위해 <b>당근과 루테인을 먹고</b></li><li>비만으로 가는 몸뚱이를 조금이라도 정상으로 돌리고자 <b>헬스장에 갑니다.</b></li><li>알던 한자가 갑자기 생각이 안나면 <b>재빨리 검색해서 손으로 써 봅니다.</b></li></ul><ul><li><b>업무에는 AI를 활용</b>해서 생산성을 높이고,</li><li>이렇게 확보한 시간에 <b>독서, 사색, 토론, 예술 감상</b>을 해야 합니다.</li><li>생각한 바를 친우들과 나누며 함께 영혼을 키워나가기를 희망합니다.</li></ul><p><img src="70_Taejae_photo.jpg" alt="연사들 기념 사진"></p><p><img src="70_Taejae_speakers.jpg" alt="연사 목록"></p><p><img src="70_Taejae_program_%ED%8E%98%EC%9D%B4%EC%A7%80_1.png"></p><p><img src="70_Taejae_program_%ED%8E%98%EC%9D%B4%EC%A7%80_2.png" alt="Program"></p>]]></content:encoded>
      
      
      <category domain="https://jehyunlee.github.io/categories/General/">General</category>
      
      
      <category domain="https://jehyunlee.github.io/tags/openai/">openai</category>
      
      <category domain="https://jehyunlee.github.io/tags/chatgpt/">chatgpt</category>
      
      <category domain="https://jehyunlee.github.io/tags/presentation/">presentation</category>
      
      
    </item>
    
    <item>
      <title>EOST2024 - 연구 현장의 생성 AI 활용 현황</title>
      <link>https://jehyunlee.github.io/2024/10/15/General-65_EOST2024/</link>
      <guid>https://jehyunlee.github.io/2024/10/15/General-65_EOST2024/</guid>
      <pubDate>Tue, 15 Oct 2024 09:11:00 GMT</pubDate>
      
        
        
      <description>&lt;ul&gt;
&lt;li&gt;ETRI Open Source Tech Day 2024에서 발표를 드렸습니다.&lt;/li&gt;
&lt;li&gt;2017년에 알파고가 전지전능의 대명사였던 것처럼, 최근에는 생성AI가 그런 느낌입니다.&lt;/li&gt;
&lt;li&gt;조금이나마 정리가 되기를 바라며, </description>
        
      
      
      
      <content:encoded><![CDATA[<ul><li>ETRI Open Source Tech Day 2024에서 발표를 드렸습니다.</li><li>2017년에 알파고가 전지전능의 대명사였던 것처럼, 최근에는 생성AI가 그런 느낌입니다.</li><li>조금이나마 정리가 되기를 바라며, 한국에너지기술연구원의 사례를 공유드렸습니다.</li></ul><div class="video-container"><iframe src="https://www.youtube.com/embed/ug48jhfndO8" frameborder="0" loading="lazy" allowfullscreen></iframe></div><ul><li><p>발표자료를 공유드립니다. (<a href="241015_%EC%9D%B4%EC%A0%9C%ED%98%84_EOST2024.pdf">다운로드</a>)</p></li><li><p>전체 연사 발표자료 (<a href="https://github.com/eostday/eost2024">링크</a>)</p></li></ul><p><img src="65_EOST2024_01.png" alt="EOST 2024 포스터"></p><p><img src="65_EOST2024_02.jpeg" alt="EOST 2024 발표자"></p>]]></content:encoded>
      
      
      <category domain="https://jehyunlee.github.io/categories/General/">General</category>
      
      
      <category domain="https://jehyunlee.github.io/tags/openai/">openai</category>
      
      <category domain="https://jehyunlee.github.io/tags/chatgpt/">chatgpt</category>
      
      <category domain="https://jehyunlee.github.io/tags/EOST/">EOST</category>
      
      <category domain="https://jehyunlee.github.io/tags/nst/">nst</category>
      
      
    </item>
    
    <item>
      <title>활용 업무 기준 AI 분류</title>
      <link>https://jehyunlee.github.io/2024/10/09/General-63_aiforwork/</link>
      <guid>https://jehyunlee.github.io/2024/10/09/General-63_aiforwork/</guid>
      <pubDate>Wed, 09 Oct 2024 07:37:00 GMT</pubDate>
      
        
        
      <description>&lt;ul&gt;
&lt;li&gt;최근 AI 업무 적용을 기획하는 분들을 만날 기회가 많이 생겼습니다.&lt;/li&gt;
&lt;li&gt;그러나 대화를 나누다 보면 같은 ‘인공지능’, ‘AI’라는 단어를 너무 넓게 쓰고 있다는 것이 느껴집니다.&lt;/li&gt;
&lt;li&gt;AI라는 말 자체가 모호하</description>
        
      
      
      
      <content:encoded><![CDATA[<ul><li>최근 AI 업무 적용을 기획하는 분들을 만날 기회가 많이 생겼습니다.</li><li>그러나 대화를 나누다 보면 같은 ‘인공지능’, ‘AI’라는 단어를 너무 넓게 쓰고 있다는 것이 느껴집니다.</li><li>AI라는 말 자체가 모호하긴 하지만 업무를 정의할 수 없을 만큼 모호하기에 정리해 보았습니다.</li></ul><blockquote><p><a href="https://www.nobelprize.org/prizes/physics/2024/press-release/">노벨상: 2024 노벨 물리학상</a><br><a href="https://www.chosun.com/politics/politics_general/2023/01/28/3DVO3AS4CVGDJMAOUCO3N3PAOI/">조선일보: “챗GPT에 신년사 써보게하니 훌륭… 잘 연구해보라”</a></p></blockquote><p><img src="63_aiforwork_01.png" alt="2024년 노벨 물리학상 수상자: 존 홉필드, 제프리 힌튼"></p><ul><li>2024년 노벨 물리학상 수상자는 존 홉필드, 제프리 힌튼입니다.</li><li>알파고때만 해도 일반인들에게는 상당히 멀게 느껴졌던 인공지능 기술이지만</li><li>ChatGPT의 등장 이후 상당히 빠르게, 가까이 다가왔습니다.</li></ul><ul><li><b>그러나 다양한 기술들이 인공지능이라고 불리면서 다소 혼란스럽습니다.</b></li><li>말도 안되는 연구 개발 목표가 제시되거나</li><li>생성AI의 한계를 알지 못하고 활용하여 문제가 되기도 합니다.</li></ul><ul><li><b>이러한 혼란을 해소하기 위해 AI를 활용 업무 기준으로 분류하고자 합니다.</b></li><li>학문적으로 엄밀하지 않음을 미리 밝힙니다.</li><li>또한, 아래 분류는 상호 배타적이 아닙니다. 실제로는 혼합해 사용됩니다.</li><li>그럼에도 불구하고 의사결정자들의 혼란을 줄이기 위해 단순화했습니다.</li></ul><p><img src="241008_%EC%9D%B4%EC%A0%9C%ED%98%84_%EC%97%85%EB%AC%B4%ED%99%9C%EC%9A%A9AI%EB%B6%84%EB%A5%98.png" alt="업무에 적용되는 AI 분류"></p><blockquote><p><a href="241008_%EC%9D%B4%EC%A0%9C%ED%98%84_%EC%97%85%EB%AC%B4%ED%99%9C%EC%9A%A9AI%EB%B6%84%EB%A5%98.pdf"><b>PDF 버전 다운로드</b></a></p></blockquote><ul><li>적어도 AI를 업무에 활용할 때는 인공지능, AI로 불리지 않고 최소한의 분류에 따라 불리기를 희망합니다.</li><li><b>무심코 던진 돌에 개구리가 죽는다</b>는 말이 있습니다.</li><li><b>의사결정자들이 별 의미 없이 좋은 뜻으로 내뱉은 지시가 현업에는 큰 타격이 될 수 있습니다.</b></li></ul>]]></content:encoded>
      
      
      <category domain="https://jehyunlee.github.io/categories/General/">General</category>
      
      
      <category domain="https://jehyunlee.github.io/tags/python/">python</category>
      
      <category domain="https://jehyunlee.github.io/tags/chatgpt/">chatgpt</category>
      
      
    </item>
    
  </channel>
</rss>
