<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Pega Devlog</title>
    <link>https://jehyunlee.github.io/</link>
    
    <atom:link href="https://jehyunlee.github.io/rss2.xml" rel="self" type="application/rss+xml"/>
    
    <description>Pega&#39;s Development log for myself and others</description>
    <pubDate>Fri, 09 Jan 2026 00:06:54 GMT</pubDate>
    <generator>http://hexo.io/</generator>
    
    <item>
      <title>NotebookLM + Nanobanana Pro</title>
      <link>https://jehyunlee.github.io/2026/01/09/General-92_notebooklmnanobanana/</link>
      <guid>https://jehyunlee.github.io/2026/01/09/General-92_notebooklmnanobanana/</guid>
      <pubDate>Thu, 08 Jan 2026 23:15:00 GMT</pubDate>
      
        
        
      <description>&lt;ul&gt;
&lt;li&gt;NotebookLM의 기능이 나날이 강력해지고 있습니다.&lt;/li&gt;
&lt;li&gt;다양한 기능이 추가될 뿐 아니라 있던 기능도 강해지고 있습니다.&lt;/li&gt;
&lt;li&gt;인포그래픽 한글 출력이 조금 아쉽지만 Gemini를 따로 거치면 깔끔해집니다.&lt;/</description>
        
      
      
      
      <content:encoded><![CDATA[<ul><li>NotebookLM의 기능이 나날이 강력해지고 있습니다.</li><li>다양한 기능이 추가될 뿐 아니라 있던 기능도 강해지고 있습니다.</li><li>인포그래픽 한글 출력이 조금 아쉽지만 Gemini를 따로 거치면 깔끔해집니다.</li></ul><h2 id="1-NotebookLM-폴더-foldLM">1. NotebookLM 폴더, <code>foldLM</code></h2><blockquote><p><a href="https://github.com/parasolente/foldLM?tab=readme-ov-file">github: foldLM</a></p></blockquote><p><img src="92_notebooklm_01.png" alt=""></p><ul><li>이제 NotebookLM이 무엇인지는 설명할 필요가 없을 것 같습니다.</li><li>논문과 보고서, 유튜브 링크를 올리고 이를 토대로 정보를 뽑아낼 수 있습니다.</li><li>전 무엇보다 이 내용이 어디 있더라 할 필요가 없다는 점이 제일 좋습니다.</li></ul><p><img src="92_notebooklm_14.png" alt=""></p><ul><li>다만 폴더 단위로 체계적으로 정리가 안 된다는 단점이 있었는데,</li><li>최근 <a href="https://github.com/parasolente/foldLM?tab=readme-ov-file"><strong>foldLM</strong></a>이 출시되어 가능해졌습니다.</li><li>위 링크를 타고 가서 레포를 다운받고,</li><li>chrome이나 edge같은 chrome 호환 브라우저에서 폴더째로 import를 하면 됩니다.</li></ul><p><img src="92_notebooklm_02.png" alt=""></p><ul><li>그러면 아래 그림처럼 <code>Create new folder</code> 버튼이 생기는데,</li></ul><p><img src="92_notebooklm_03.png" alt=""></p><ul><li>이걸 누르면 폴더가 만들어지고</li><li>여기에 다른 노트북들을 드래그해서 넣을 수 있습니다.</li><li>저는 우선 여기저기 섞여 있는 AI 관련 노트북들을 모았습니다.</li></ul><p><img src="92_notebooklm_04.png" alt=""></p><ul><li>폴더 아이콘이 마음에 들지 않는다면,</li><li>오른쪽 위 점 3개 버튼을 클릭해서 바꿀 수 있습니다.</li><li>다양한 이모지를 지원합니다.</li></ul><p><img src="92_notebooklm_05.png" alt=""></p><h2 id="2-NotebookLM-에서-infographic-조금-예쁘게-만들기">2. NotebookLM 에서 infographic 조금 예쁘게 만들기</h2><h3 id="2-1-Nanobanana-프롬프트-모음">2.1. Nanobanana 프롬프트 모음</h3><blockquote><p><a href="https://furoku.github.io/bananaX/projects/infographic-evaluation/">Hiroki Kai: bananaX infographic evaluation</a></p></blockquote><ul><li>많은 분들이 그렇지만 이공계 중에 이미지 생성 프롬프팅을 어려워하는 분들이 많습니다.</li><li>평소 묘사가 많은 소설책을 읽거나 그림을 보거나 그리는 경험이 적기 때문일 것 같습니다.</li><li>이런 분들께 프롬프트 템플릿이 큰 도움이 되는 경우가 많은데,</li><li>최근 어떤 분께서 <a href="https://furoku.github.io/bananaX/projects/infographic-evaluation/"><strong>나노바나나 프롬프트 템플릿</strong></a>을 대량으로 공개했습니다.</li></ul><p><img src="92_notebooklm_15.png" alt=""></p><ul><li>다양한 스타일의 그림을 훑어보다가 마음에 드는 것을 클릭합니다.</li><li>사람들이 매겨놓은 점수와 함께 <strong>프롬프트</strong>가 복사해서 붙여넣기 좋게 되어 있습니다.</li></ul><p><img src="92_notebooklm_06.png" alt=""></p><ul><li>톤, 글씨체, 색 등 이미지의 구성요소별로 구성되어 있습니다.</li><li>중요한 특징이 있는데, <strong>내용과 구도에 대한 정보가 없다</strong>는 겁니다.</li><li>이대로 넣으면 나노바나나에게 맡기게 되지만,</li><li><strong>어떤 내용과 구도를 넣어도 충돌하지 않는다</strong>는 의미가 됩니다.</li></ul><p><img src="92_notebooklm_07.png" alt=""></p><h3 id="2-2-인포그래픽-만들기">2.2. 인포그래픽 만들기</h3><ul><li>프롬프트를 들고 NotebookLM으로 와서,</li><li>우측 상단 Infographic 옆에 있는 연필을 클릭합니다.</li><li>프롬프트 입력 버튼입니다.<br><br></li><li>복사한 프롬프트를 붙여넣고 앞에 담고 싶은 내용을 간략히 적습니다.</li><li>저는 AI Agents for Scientific Research의 발전을 담고 싶었고,</li><li>특히 시간에 따라 진화하는 기능과 실험연구 사례를 담기를 원했습니다.</li><li>그래서 다음과 같이 입력했습니다.</li></ul><p><code>Development of AI Agents for Scientific Research, as a fuction of time.</code><br><code>* focus on the evolution of functions and capacities.</code><br><code>* emphasize the examples of experimental research (if possible)</code></p><p><img src="92_notebooklm_08.png" alt=""></p><ul><li>조금 기다리면 깔끔한 그림이 나옵니다.</li><li><strong>English</strong>를 선택해 영어로 만들었습니다.</li><li>한국어로 설정하면 한글이 나오기는 하지만 잘 깨지기 때문입니다.</li></ul><p><img src="92_notebooklm_09.png" alt=""></p><h3 id="2-3-Gemini-한글-번역">2.3. Gemini 한글 번역</h3><blockquote><p><a href="https://gemini.google/kr/overview/image-generation/?hl=ko-KR&amp;_gl=1*1idoezx*_gcl_au*MjEyMjAwMzQyMC4xNzY3OTEwOTY3*_ga*ODA5MzQyOTMzLjE3Njc5MTA5Njg.*_ga_WC57KJ50ZZ*czE3Njc5MTYzNDIkbzIkZzEkdDE3Njc5MTY0NDEkajU1JGwwJGgw">Google: Nanobanana Pro 사용법</a></p></blockquote><ul><li>그림 원본을 내려받고 Gemini를 켭니다.</li><li><strong>이미지 생성</strong>과 **사고 모드(Thinking)**을 선택합니다.</li><li>Nanobanana (Gemini 2.5 Flash Image)가 아니라</li><li>Nanobanana Pro (Gemini 3.0 Pro Image)를 사용하는 설정입니다.<br><br></li><li>전문용어까지 번역하기는 원치 않습니다.</li><li>NotebookLM이 만들어 준 이미지를 올리고 다음 프롬프트를 넣습니다.<br><code>Translate English scripts to Korean.</code><br><code>Keep in your mind that the Jargons should be kept. Do not try to explain in easy words.</code><br><code>All other features should be kept.</code></li></ul><p><img src="92_notebooklm_12.png" alt=""></p><ul><li>잠시 후, 나노바나나 프로가 그림을 번역해 줍니다.</li><li>Gemini 화면에서는 일부 글자가 깨져보일 수 있습니다.</li><li>원본을 내려받으면 글자가 깔끔하게 보입니다.</li></ul><p><img src="92_notebooklm_10.png" alt=""></p><h3 id="2-4-Google-AI-Studio-한글-번역">2.4. Google AI Studio 한글 번역</h3><ul><li>Nanobanana Pro는 Google AI Studio에서 사용할 수도 있습니다.</li><li>Google API Key를 발급받아 넣고 약간의 비용을 들여야 하는데,</li><li>이번 그림의 경우 259 token. 약 5원 정도로 산정되었으니 무료나 마찬가지입니다.</li></ul><p><img src="92_notebooklm_13.png" alt=""></p><ul><li>그런데 <strong>결과는 실망스럽습니다.</strong></li><li>Gemini에서 호출한 Nanobanana Pro와 뭐가 다른지 잘 모르겠지만 결과가 영 좋지 않네요.</li><li>한글이 일부 깨진 곳도 있을 뿐 아니라</li><li>왼쪽 상단은 내용이 바뀌었습니다 - 일종의 환각이라고 볼 수 있을 것 같습니다.</li></ul><p><img src="92_notebooklm_11.jpeg" alt=""></p><ul><li>Temperature를 1로 설정해서 그런가 싶기도 했지만,</li><li>Temperature를 0으로 설정했을 때도 번역을 하다 말고 영문 철자를 바꿔놓았습니다.</li><li>종종 테스트를 해 보겠지만, 한동안 이런 작업은 Gemini에서 해야겠습니다.</li></ul>]]></content:encoded>
      
      
      <category domain="https://jehyunlee.github.io/categories/General/">General</category>
      
      
      <category domain="https://jehyunlee.github.io/tags/notebooklm/">notebooklm</category>
      
      <category domain="https://jehyunlee.github.io/tags/gemini/">gemini</category>
      
      <category domain="https://jehyunlee.github.io/tags/nanobanana/">nanobanana</category>
      
      <category domain="https://jehyunlee.github.io/tags/foldlm/">foldlm</category>
      
      
    </item>
    
    <item>
      <title>2026년 붉은 말의 해</title>
      <link>https://jehyunlee.github.io/2026/01/04/General-91_2026_redhorse/</link>
      <guid>https://jehyunlee.github.io/2026/01/04/General-91_2026_redhorse/</guid>
      <pubDate>Sat, 03 Jan 2026 21:34:00 GMT</pubDate>
      
        
        
      <description>&lt;ul&gt;
&lt;li&gt;2026년 병오년 새해가 밝았습니다.&lt;/li&gt;
&lt;li&gt;단톡방에는 여러 분들께서 달리는 붉은 말 그림을 만들어 공유해 주십니다.&lt;/li&gt;
&lt;li&gt;조금 다르게 그려보고 싶었습니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;1-신년그림&quot;&gt;1. 신년</description>
        
      
      
      
      <content:encoded><![CDATA[<ul><li>2026년 병오년 새해가 밝았습니다.</li><li>단톡방에는 여러 분들께서 달리는 붉은 말 그림을 만들어 공유해 주십니다.</li><li>조금 다르게 그려보고 싶었습니다.</li></ul><h2 id="1-신년그림">1. 신년그림</h2><ul><li>해마다 새해를 상징하는 동물을 그립니다.</li><li>처음에는 종이에 샤프로, 나중에는 스마트폰에 S펜으로, 지금은 AI로 그립니다.</li><li>60갑자에는 동물 뿐 아니라 오방색 중 하나가 붙기 때문에 좋은 소재가 됩니다.<br><br></li><li>공개를 하기도 하고 하지 않기도 합니다만,</li><li>모든 그림엔 새해의 고민과 결심이 녹아있습니다.<br><br></li><li>2010년 <strong>하품을 하는 호랑이</strong>는 프로젝트의 시작을 의미하는 기지개였습니다.</li><li>2014년 <strong>푸른 말</strong>은 잘 보이지도 않습니다.</li><li>한 선비를 태우고 왼쪽 절벽 위에서 뛰어넘을까 말까 고민하고 있는데,</li><li>전공을 버리고 플래시로 갈아타야 하는 고민이 담겨 있습니다.<br><br></li><li><strong>멍때리고 있는 2015년의 양</strong> 뒤 배경은 일출이 펼쳐지는 하늘입니다.</li><li>큰 결심을 하고 전공을 바꿨는데 3D 시각화로 또 바꾸라고 해서 허탈해하는 모습입니다.</li><li>배경에 아무것도 없는 것은 산 꼭대기에 올라 있기 때문입니다.</li><li>낯선 분야에서 나름의 성과를 내고 조금은 인정받던 저를 칭찬해주고 싶었습니다.</li></ul><p><img src="91_2026redhorse_02.png" alt=""><br><img src="91_2026redhorse_03.png" alt=""></p><ul><li>심각한 이야기만 담는 건 아닙니다.<br><br></li><li><strong>2020년 소 머리 위에 탄 쥐</strong>는 전래동화 속 한 장면입니다.</li><li><strong>2023년 검은 토끼는 2022년의 호랑이와 함께</strong> 있습니다.</li><li>호랑이가 토끼를 데려왔다는 설정입니다.<br><br></li><li>모든 해를 그리지는 못했습니다.</li><li>아이가 태어나 휴일에도 육아에 정신이 없던 때도 있었고,</li><li>일정이 꼬이거나 해서 그림을 그릴 짬을 내지 못한 때도 있었습니다.<br><br></li><li>손으로 그릴 때는 색을 넣기 어려웠습니다.</li><li>명절에 잠시 짬을 내서 그려서 시간과 공간의 제약이 컸기 때문입니다.</li><li>스마트폰 앱으로 그리면서 색을 넣을 수 있게 되었고</li><li>AI로 그리면서 시간이 줄었습니다. 복잡한 그림들이 등장합니다.<br><br></li><li><strong>하지만, 누구나 AI로 그림을 그릴 수 있게 되었습니다.</strong></li><li>여러 단톡방에 AI로 그린 새해 인사가 많아졌습니다. 이벤트도 합니다.</li><li><strong>거의 비슷한 그림들</strong>입니다. 재미가 없습니다.</li><li>원래도 그랬지만 조금 더 다르게 그림을 그리고 싶어졌습니다.</li><li>그렇지 않으면 그리나 마나입니다. 내 그림이 있으나 없으나 티가 안 납니다.</li></ul><p><img src="91_2026redhorse_04.webp" alt=""></p><ul><li><strong>2025년도의 푸른 뱀</strong>을 그린 프롬프트는 이렇습니다.<br><code>Abstractive illustration of a blue snake jumping over between tree branches. The snake is stretching its body from coiling. Behind the snake wide clear and blue sky is seen. Motion blur. Bottom view, lens flare</code><br><br></li><li>단톡방이 또아리를 틀고 있거나 정면을 보고 웃고 있는 뱀으로 가득찰 것이 예상됐습니다.</li><li>소망을 담아 독특한 그림을 그리고 싶었습니다.</li><li><code>jumping over between tree branches</code>라는 문구에 담긴 소망은 <strong>이직</strong>입니다.</li><li>첫 결과를 다듬다 보니 분명 뱀인데 용처럼 생긴 뱀이 나왔습니다. 마음에 들었습니다.</li><li>한 해 동안 제 노트북 잠금화면이 되어 준 그림이고, 소망대로 이직을 했습니다.<br><br></li><li>사실 <strong>2013년의 검은 뱀</strong>도 곁눈질을 하고 있습니다.</li><li>교수를 꿈꾸다 회사에 들어갔을 때라, 같은 마음으로 담은 그림입니다.</li></ul><h2 id="2-2026년-병오년">2. 2026년 병오년</h2><ul><li><strong>2026년의 붉은 말</strong>도 마찬가지입니다.</li><li>사람들이 어떤 그림을 그릴지 예상을 해봤습니다.</li><li>달리는 말, 적토마, 귀여운 말, 세배하는 말이 떠오릅니다.<br><br></li><li>이 모든 것들을 후보에서 배제하고 다른 그림을 떠올립니다.</li><li><strong>2026년은 제게 새 직장에서 새로운 시작을 하는 해입니다.</strong></li><li><strong>운명의 수레바퀴</strong>라는 단어가 떠올랐습니다.<br><br></li><li>말의 붉은 색은 <strong>불꽃</strong>으로 표현하기로 했습니다.</li><li>더 성장해보겠다고 나름 자리를 잡았다고 느낀 전 직장을 나왔습니다.</li><li>새 직장에서 전 직장보다 잘 될지는 몰라도 고생은 확정입니다.</li><li>좋은 선택이기를 바라며 불길 속에서 새로 태어난다는 불사조 이야기를 입혀봤습니다.</li><li><strong>한 해를 불태운다</strong>는 의미와도 잘 맞는다는 생각이 듭니다.<br><br></li><li>그렇다면 작년, 재작년은 어땠을까. 라는 생각이 들었습니다.</li><li>다른 이들에게 어떻게 보였을지 모르겠지만 저는 열심히 살았습니다.</li><li>성취하지 못한 일들이 적지 않지만 제 능력의 부족이지 노력의 부족은 아닌 것 같습니다.</li><li>해가 바뀌었으니 지금 타기보다 <strong>다 타고 꺼진 불</strong>로 표현하고 싶었습니다.<br><br></li><li>시간이 지나면서 회복이 되겠지만, 몇 해는 지나야 되는 듯 합니다.</li><li>앞에 있는 뱀, 용, 호랑이 정도까지 그을음이 묻은 장면이 떠오릅니다.</li><li>멀리 있을수록 회복이 되었을 것이고, 작년도 데미지는 아직 덜 나았습니다.</li><li>실제로 병원 신세를 많이 졌던 한 해입니다.<br><br></li><li>이쯤 되니 한 장면이 어느 정도 완성되어 나노바나나에 프롬프트를 입력했습니다.<br><code>photography of a thick round wooden round plate fixed on old stone wall. along the perimeter of the plate, abstracted symbols of 12 animals {mouse, cow, tiger, rabbit, chinese dragon, snake, horse, sheep, monkey, chicken, dog, swine} made of bronze are allocated in clockwise direction, starting from the top. The symbol of horse is surrounded by vivid flame, therefore the wood very top of the symbol and the nearby region is burnt to black. The symbols and wooden plates around the symbols of the snake, dragon and rabbit are also slightly burnt as if just extinguished, more severe as close to the horse. 1:1 aspect ratio</code></li></ul><p><img src="91_2026redhorse_05.png" alt=""></p><ul><li>올 한 해도 잘 지내보겠습니다.</li><li>후회 없이 불태우려고 합니다.</li><li>새해 원하시는 일들 모두 이루시기 바랍니다.</li></ul>]]></content:encoded>
      
      
      <category domain="https://jehyunlee.github.io/categories/General/">General</category>
      
      
      <category domain="https://jehyunlee.github.io/tags/gemini/">gemini</category>
      
      <category domain="https://jehyunlee.github.io/tags/nanobanana/">nanobanana</category>
      
      
    </item>
    
    <item>
      <title>AI for Science 도메인 특화 Agent 공개 사례</title>
      <link>https://jehyunlee.github.io/2025/08/24/General-90_rndagent_cases/</link>
      <guid>https://jehyunlee.github.io/2025/08/24/General-90_rndagent_cases/</guid>
      <pubDate>Sun, 24 Aug 2025 05:51:00 GMT</pubDate>
      
        
        
      <description>&lt;ul&gt;
&lt;li&gt;지난 R&amp;amp;D AI Agent 발표 이후 상위기관에 관련 자료를 정리해드릴 기회가 있었습니다.&lt;/li&gt;
&lt;li&gt;&#39;24~&#39;25년에 주로 출간된 R&amp;amp;D AI Agent 논문 25편의 주요 내용을 정리했습니다.&lt;/li&gt;
&lt;li&gt;</description>
        
      
      
      
      <content:encoded><![CDATA[<ul><li>지난 R&amp;D AI Agent 발표 이후 상위기관에 관련 자료를 정리해드릴 기회가 있었습니다.</li><li>'24~'25년에 주로 출간된 R&amp;D AI Agent 논문 25편의 주요 내용을 정리했습니다.</li><li>여러 연구의 독창성과 단점을 함께 정리하여 돌파해야 할 난제를 찾고자 했습니다.</li></ul><h2 id="1-정리자료">1. 정리자료</h2><ul><li>정리자료를 공유드립니다: [<a href="250820_RndAIAgents_domain.pdf">다운로드</a>]</li></ul><h2 id="2-AI-for-Science-기술">2. AI for Science 기술</h2><blockquote><p><a href="http://arxiv.org/abs/2507.01903">AI4Research: A Survey of Artificial Intelligence for Scientific Research</a></p></blockquote><p><img src="90_rndagent_cases_01.png" alt=""></p><ul><li><p>이제까지 출판된 <b>950편의 논문</b>을 리뷰한 논문을 비롯해</p></li><li><p><b>VASP, MOF 등 특정 기술</b>에 대한 논문을 찾아보았습니다.</p></li><li><p>수많은 논문들이 다섯 개의 카테고리로 나뉘어 자세하게 설명된 좋은 논문입니다만 <b>120 페이지</b>로 많이 두껍습니다.</p></li><li><p>NotebookLM에서는 읽을 수 있지만 scispace에서는 제대로 읽지 못할 정도입니다.</p></li><li><p>게다가 참고자료들이 논문에 대개 한 줄로 간략하게 소개되어 있기 때문에 <b>특장점을 잘 알아보기 어렵습니다.</b></p></li></ul><h2 id="3-도메인-특화-기술">3. 도메인 특화 기술</h2><blockquote><p><a href="https://chatgpt.com/g/g-CPDHsnGd4-nonmunbos">GPTs: 논문봇</a></p></blockquote><ul><li>제가 관심있는 도메인 특화 기술들을 중심으로 원문을 내려받아 훑어보았습니다.</li><li>정독을 하면 좋았겠지만 저 또한 시간의 제약으로 그러지는 못했습니다.</li><li><b><a href="https://chatgpt.com/g/g-CPDHsnGd4-nonmunbos">논문봇</a></b>과 <b><a href="https://scispace.com/">scispace</a></b>, <b><a href="https://notebooklm.google.com/">NotebookLM</a></b>를 이용해 훑었습니다.</li><li>논문봇에 넣는 것만으로도 웬만한 답변이 나오지만, <b>추가질의가 더 중요합니다.</b></li></ul><p><img src="90_rndagent_cases_02.png" alt=""></p><p><img src="90_rndagent_cases_03.png" alt=""></p><ul><li>대다수의 논문들이 GPT나 DeepSeek 같은 <b>LLM에 의존</b>하고 있습니다.</li><li>자연어 활용 인터페이스와 창발 현상을 이용하고자 하니 당연한 일입니다.</li><li>하지만 한편으로 이런 경우 <b>LLM의 성능에 에이전트의 성능이 크게 좌우됩니다.</b></li></ul><h3 id="Context-Window-Model-Size">Context Window &amp; Model Size</h3><blockquote><p><a href="https://arxiv.org/abs/2203.08913">arXiv: Memorizing Transformers (2022)</a><br><a href="https://arxiv.org/abs/2503.19786">arXiv: Gemma3 Technical Report (2025)</a><br><a href="https://arxiv.org/abs/2501.15383">arXiv: Qwen2.5-1M Technical Report (2025)</a><br><a href="https://www.nature.com/articles/s41524-025-01564-y">npj Computational Materials: Fine-tuning large language models for domain adaptation: exploration of training strategies, scaling, model merging and synergistic capabilities (2025)</a><br><a href="http://arxiv.org/abs/2310.06770">arXiv: SWE-bench: Can Language Models Resolve Real-World GitHub Issues?</a><br><a href="http://arxiv.org/abs/2502.15840">arXiv: Vending-Bench: A Benchmark for Long-Term Coherence of Autonomous Agents</a><br><a href="https://arxiv.org/abs/2506.02153">arXiv: Small Language Models are the Fugure of Agentic AI</a></p></blockquote><ul><li><p>복잡한 R&amp;D 작업은 여러 단계를 거치는 <b>추론과 계획</b>을 필요로 합니다.</p></li><li><p><b>컨텍스트 윈도우</b>가 크면 에이전트가 이전 추론 단계, 도구 사용 결과, 환경 관찰 등을 종합적으로 고려하여 다음 행동을 결정하는 데 도움이 됩니다.</p></li><li><p>특히 고급 추론에는 큰 컨텍스트 윈도우가 필요합니다.</p></li><li><p>작은 규모의 sLM은 대개 컨텍스트 윈도우가 LLM에 비해 작습니다.</p></li><li><p>과거에는 <b><a href="https://arxiv.org/abs/2203.08913">Memorizing Transformers</a></b>처럼 아키텍쳐를 변경해 262K까지 크기를 키웠고,</p></li><li><p>최근에는 <b><a href="https://arxiv.org/abs/2503.19786">Gemma3</a> 등이 128K</b>까지, <b><a href="https://arxiv.org/abs/2501.15383">Qwen2.5-1M</a></b>가 1M까지 지원하는 등 컨텍스트 윈도우를 키워가는 추세입니다.</p></li></ul><p><img src="90_rndagent_cases_04.png" alt=""></p><ul><li><p>추론 및 지식 회상 같은 창발 현상에 임계값이 필요하여 <b><a href="https://www.nature.com/articles/s41524-025-01564-y">너무 작은 모델에서는 잘 구동이 되지 않는다는 연구</a></b>가 많습니다.</p></li><li><p>Lu(2025)의 연구에서 <b>가장 좋은 효과를 보인 모델은 Llama-3.1 8B</b>이며, <b>MoE를 해도 효과가 없던 모델은 Smollm-Base-1.7B</b>으로 두 모델의 context size는 모두 128K입니다.</p></li><li><p>모델 크기가 8B만 돼도 충분히 활용할 수 있다는 의미이기도 합니다.</p></li><li><p>한편으로, context size가 큰 모델은 입력이 길 때 정보를 효과적으로 활용하지 못하고 <b>중간에서 길을 잃는(lost in the middle)현상</b>이 보고되기도 합니다.</p></li><li><p>에이전트의 성능 저하가 반드시 컨텍스트 윈도우가 다 찼기 때문은 아닐 수 있으며, <b>장기 일관성(long-term coherence)</b>이 더 근본적인 문제일 수 있음을 시사합니다.</p></li></ul><p><img src="90_rndagent_cases_05.png" alt=""></p><ul><li>NVIDIA에서는 <b>Small Language Models are the Future of Agentic AI</b>라는 이름의 논문을 발표했습니다.</li><li>최근의 sLM 발전상을 반영한 논문이기 때문에 sLM의 성능 향상이 충실히 반영되어 있는데,</li><li><b>sLM은 이전 세대의 대규모 모델과 유사 또는 이상의 성능</b>을 보이며 <b>상식 추론</b>, <b>도구 호출</b>, <b>코드 생성</b>, <b>지시 이행</b> 등 <b>에이전트 작업에 필수적인 임무</b>를 충실히 수행합니다.</li><li>운용 비용과 운영 유연성, 최근 여러 개의 하위 작업으로 구성된 멀티 에이전트 시스템을 운영하는 점을 고려하면 더욱 그렇습니다.</li></ul><p><img src="90_rndagent_cases_06.png" alt=""></p><ul><li><p>그러나 여전히 <b>context size가 큰 LLM이 적합한 작업</b>이 있음을 동시에 지적합니다.</p><ol><li><b>복잡한 문제를 추상적으로 이해하여 하위 문제로 분해하는 작업</b></li><li><b>컨텍스트 유지가 필요한 다단계 추론, 아키텍처 추론 및 적응형 계획</b></li><li><b>비정형적 오류 해결</b></li><li><b>일반적 추론 및 개방형 도메인 대화</b> 등입니다.</li></ol></li><li><p>저자들은 개별 업무에 sLM을 적용하고 선택적으로 LLM을 호출하는 <b>heterogeneous 에이전트 시스템을 제안</b>하며</p></li><li><p>한편으로 LLM이 담당해주어야 할 <b>장기적 일관성 능력을 향상</b>해주기를 촉구하고 있습니다.</p></li></ul><h2 id="결론">결론</h2><ul><li>AI for Science, 또는 R&amp;D AI Agent는 이제 시작입니다.</li><li>언어모델이 도구를 다룰 수 있게 된 것도 얼마 되지 않았고, 최근에서야 MCP와 A2A 등이 나와 팔다리 연결이 수월해지고 있습니다.</li><li><b>최신 발전</b>을 따라가야겠지만,</li><li>종착지는 <b>연구 본연의 목표</b>여야 하겠습니다.</li><li>우리에게 중요한 것은 Agent가 아니라 R&amp;D입니다.</li></ul><p><img src="90_rndagent_cases_07.gif" alt=""></p>]]></content:encoded>
      
      
      <category domain="https://jehyunlee.github.io/categories/General/">General</category>
      
      
      <category domain="https://jehyunlee.github.io/tags/research/">research</category>
      
      <category domain="https://jehyunlee.github.io/tags/agent/">agent</category>
      
      
    </item>
    
    <item>
      <title>AI프렌즈-Agent가 바꾸는 RnD의 미래</title>
      <link>https://jehyunlee.github.io/2025/07/31/General-89_rndagent/</link>
      <guid>https://jehyunlee.github.io/2025/07/31/General-89_rndagent/</guid>
      <pubDate>Thu, 31 Jul 2025 11:06:00 GMT</pubDate>
      
        
        
      <description>&lt;ul&gt;
&lt;li&gt;대전의 출연연들 AI 담당자들이 모여 각 기관에서 R&amp;amp;D AI agent를 만드는 경험을 공유했습니다.&lt;/li&gt;
&lt;li&gt;R&amp;amp;D AI agent의 본질은 AI가 아니라 R&amp;amp;D일 것이고, AI 이전에 우리가 agent</description>
        
      
      
      
      <content:encoded><![CDATA[<ul><li>대전의 출연연들 AI 담당자들이 모여 각 기관에서 R&amp;D AI agent를 만드는 경험을 공유했습니다.</li><li>R&amp;D AI agent의 본질은 AI가 아니라 R&amp;D일 것이고, AI 이전에 우리가 agent입니다.</li><li>최근 AI agent의 발전이 인간사회를 닮아가는 듯 합니다. 조금은 넓은 관점에서 의견을 드렸습니다.</li></ul><h2 id="1-강연-영상과-강의자료">1. 강연 영상과 강의자료</h2><blockquote><p><a href="https://www.youtube.com/@aifrenz">YouTube: AI프렌즈</a><br><a href="https://www.youtube.com/playlist?list=PLZasDVDRptrnMFObj1Yo8FL733WaPEHEb">Youtube: Agent가 바꾸는 R&amp;D의 미래 워크샵</a></p></blockquote><p><img src="89_rndagent_01.jpg" alt="행사 포스터"><br></p><ul><li>이번 행사는 출연연 네 곳과 기업 한 곳에서 발표를 했습니다.</li><li>순서대로 <strong>에너지연</strong>, <strong>KISTI</strong>, <strong>AI Factory</strong>, <strong>철도연</strong>, <strong>원자력연</strong>입니다.</li><li>제가 한 시간 가량 개념과 역사 등을 말씀드리고, 네 곳에서 각자의 노력과 성과를 선보였습니다.</li></ul><h3 id="1-1-에너지연-이제현">1.1. 에너지연 이제현</h3><ul><li>강의자료 공유드립니다: [<a href="250731_%EC%9D%B4%EC%A0%9C%ED%98%84_%EC%97%B0%EA%B5%ACAgent.pdf">다운로드</a>]</li></ul><div class="video-container"><iframe src="https://www.youtube.com/embed/dhfYumoZXkk" frameborder="0" loading="lazy" allowfullscreen></iframe></div><h3 id="1-2-KISTI-장광선-박사">1.2. KISTI 장광선 박사</h3><div class="video-container"><iframe src="https://www.youtube.com/embed/0P4LzO5sfLM" frameborder="0" loading="lazy" allowfullscreen></iframe></div><h3 id="1-3-AI-Factory-김태영-대표">1.3. AI Factory 김태영 대표</h3><div class="video-container"><iframe src="https://www.youtube.com/embed/1EOuslsBvVo" frameborder="0" loading="lazy" allowfullscreen></iframe></div><h3 id="1-4-철도연-원종운-박사">1.4. 철도연 원종운 박사</h3><div class="video-container"><iframe src="https://www.youtube.com/embed/lzCiXD2-Yc4" frameborder="0" loading="lazy" allowfullscreen></iframe></div><h3 id="1-5-원자력연-유용균-박사">1.5. 원자력연 유용균 박사</h3><div class="video-container"><iframe src="https://www.youtube.com/embed/c4FPY3xLzNw" frameborder="0" loading="lazy" allowfullscreen></iframe></div><h2 id="2-R-D-AI-그리고-Agent">2. R&amp;D, AI, 그리고 Agent</h2><ul><li><p>젠슨 황으로 인해 <strong>Agentic AI</strong>, <strong>Physical AI</strong>가 유명해졌습니다.</p></li><li><p>그러나 이런 일에는 부작용이 반드시 따르는데,</p></li><li><p>아무데나 agent를 붙이는 <strong>agent washing</strong>이 그 중 하나,</p></li><li><p>그리고 agent에만 주목하다 보니 정작 중요한 <strong>AI, data는 소홀해지는 현상</strong>이 하나입니다.</p></li><li><p>첫 발표를 맡아 전반적으로 <strong>올바른 개념</strong>을 전달드려야겠다는 생각을 했습니다.</p></li><li><p>그리고 <strong>Agent</strong>, <strong>R&amp;D</strong>, 그리고 <strong>R&amp;D AI agent</strong> 순서로 구성을 했습니다.</p></li></ul><h3 id="2-1-AI-Agent">2.1. AI Agent</h3><ul><li><p>AI agent를 다루기 전에 <strong>agent에 대한 개념</strong>을 명확히 합니다.</p></li><li><p>agent는 <strong>행동한다</strong>는 뜻의 라틴어 agere에서 파생된 말로, <strong>대리인</strong>을 의미합니다.</p></li><li><p>뿌리가 같은 말로 <strong>의제</strong>를 뜻하는 <strong>agenda</strong>가 있습니다.</p></li><li><p>인간사회는 문명화와 동시에 <strong>관료제</strong>를 채택했습니다.</p></li><li><p>한 사람이 모든 것을 다 하는 것이 아니라 전문 영역을 나누어 담당하고 책임지는 구조입니다.</p></li><li><p>여기서 일을 지시하는 사람(principal)과 수행하는 사람(agent)간의 관계가 생깁니다.</p></li><li><p><strong>principal-agent relationship</strong>이라고 하며 보통 <strong>주인-대리인 관계</strong>로 번역됩니다.</p></li></ul><p><img src="89_rndagent_02.jpg" alt=""><br></p><ul><li><p>agent는 자기 밑에 다른 agent를 둡니다. sub-agent라고 합니다.</p></li><li><p>현대 사회에서는 직업을 가진 이라면 <strong>누구나 agent</strong>입니다.</p></li><li><p>스스로의 욕망을 위해 행동하기보다 계약 관계에 있는 타인을 위해 종사하고 보수를 받습니다.</p></li><li><p>agent는 <strong>전문성</strong>을 가지고, <strong>필요한 도구</strong>를 이용해, <strong>자율적</strong>으로 움직입니다.</p></li><li></li><li><p>시간이 허락한다면 이 구조에서 발생하는 여러 문제도 다루고 싶었지만 생략했습니다.</p></li><li><p><strong>principal-agent problem(주인-대리인 문제)</strong> 라고 하며</p></li><li><p>agent가 principal의 이익을 충분히 대변하지 않고,</p></li><li><p>정보 비대칭, 감시의 한계 속에서 자신의 이익을 추구함으로써 발생하는 제반 문제입니다.</p></li><li><p>주주와 전문경영인, 국민과 국회의원, 고용주와 직원 등 곳곳에서 생기는 많은 문제의 근원입니다.</p></li></ul><p><img src="89_rndagent_03.jpg" alt=""><br><img src="89_rndagent_04.jpg" alt=""><br></p><ul><li>연구원들은 자각하지 못하겠지만 <strong>연구기관과 개별 연구원 모두 agent</strong>입니다.</li><li>그것도 <strong>여럿이 모여 함께 일하는 multi-agent</strong>입니다.</li></ul><h3 id="2-2-R-D-Agent-AI">2.2. R&amp;D Agent + AI</h3><ul><li>인간 agent가 할 수 없는 일을 할 수 있는 AI agent가 있다면,</li><li>인간 agent가 할 수 있는 일을 더 저렴하고 빠르게 할 수 있는 AI agent가 있다면,</li><li><strong>AI agent가 인간 agent를 대체하거나 인간 agent와 협업</strong>하는 것은 매우 자연스러운 일입니다.</li></ul><p><img src="89_rndagent_05.jpg" alt=""><br></p><ul><li>다른 일들이 그렇듯 연구는 쉬운 일이 아닙니다.</li><li>우리 인간 R&amp;D Agent는 유사 이래 다양한 도구들을 활용하며 더 나은 환경을 만들어 왔습니다.</li><li>최근에는 <strong>인간이기에 해결하기 어려운 문제</strong>를 해결하고자 <strong>AI Agent에게 도움을 청하기 시작</strong>했습니다.</li></ul><p><img src="89_rndagent_06.jpg" alt=""><br></p><ul><li><strong>연구 재현성 문제</strong>가 그 중 하나,</li><li><strong>연구 데이터 수집 문제</strong>가 또 하나입니다.</li><li>이 중 연구 데이터 수집은 좋은 의도가 부족한 생각과 만나 잘못된 결론에 이르는 예입니다.</li><li>연구 데이터는 수집과 재현이 용이한 <strong>관측 데이터</strong>가 있는 한편,</li><li>수집도 어렵고 재현도 잘 되지 않는 <strong>실험 데이터</strong>가 있습니다.</li></ul><p><img src="89_rndagent_07.jpg" alt=""><br></p><ul><li>관측 데이터는 여기저기서 한데 모으면 힘이 강해지지만</li><li>실험 데이터는 엄밀하게는 모두 다른 데이터라 한데 모으면 서로에게 노이즈로 작용합니다.</li><li>엄격하게 관리되는 대기업의 생산라인 데이터도 그렇습니다.</li><li>훨씬 적은 연구비로 자체제작을 곁들여 만든 연구원의 데이터는 말할 것도 없습니다.</li></ul><p><img src="89_rndagent_08.jpg" alt=""><br></p><ul><li>연구자들에게 <strong>연구 노트</strong>를 더 열심히 쓰게 해서 모은다는 것은 어불성설입니다.</li><li>가장 오염되기 쉽고 디지털화하기 어려운 것이 <strong>연구 노트</strong>입니다.</li><li>실험을 하고 기록하는 것이 아니라 <strong>기록한 대로 실험이 되는 시스템</strong>을 먼저 만들어야 합니다.</li><li>이렇게 수집되지 않은 데이터는 신뢰할 수 없습니다.</li></ul><h3 id="2-3-R-D-AI-Agent">2.3. R&amp;D AI Agent</h3><p><img src="89_rndagent_09.jpg" alt=""><br></p><ul><li>최근 LLM의 놀라운 발전으로 많은 벽을 넘었습니다.</li><li><strong>글 뿐 아니라 그림과 표를 함께 읽어</strong> 지식을 획득할 수 있게 되었습니다.</li><li><strong>추론 능력</strong>이 강해져서 가설을 세울 수 있고, 논문 작성과 리뷰도 맡깁니다.</li><li>언어능력이 발전하면서 신세계가 열렸습니다.</li></ul><p><img src="89_rndagent_10.jpg" alt=""><br></p><ul><li>그러나 언어 기능이 필수는 아닙니다.</li><li>R&amp;D에는 말로 하는 일보다 <strong>숫자로 하는 일</strong>이 훨씬 많습니다.</li><li>이런 일을 하는 <strong>파운데이션 모델</strong>이 많습니다.</li><li>Meta의 SAM(Segment Anything Model)도 이런 부류입니다.</li><li>말은 한 마디도 못 하지만 실질적으로 연구에 큰 도움이 되는 모델입니다.</li></ul><p><img src="89_rndagent_11.jpg" alt=""><br></p><ul><li>R&amp;D 프로세스는 매우 복잡 다단합니다.</li><li>다양한 곳에 다양한 모델이 필요하고, 꼭 언변이 좋을 필요는 없습니다.</li><li>묵묵히 자기 실험을 열심히 하는 연구원처럼, 맡은 일을 똑부러지게 하고 결과를 전달하면 됩니다.</li><li><strong>agent의 언어적, 사회적 의미</strong>를 생각해 보면 전혀 이상하지 않습니다.</li></ul><h2 id="3-정리">3. 정리</h2><ul><li><p>R&amp;D AI Agent를 어떻게 만들까?를 고민하기 이전에,</p></li><li><p><strong>내가 맡은 연구를 어떻게 잘 할 수 있을까</strong>를 고민하는 것이 맞습니다.</p></li><li><p>R&amp;D AI Agent의 본질은 연구이기 때문입니다.</p></li><li><p>그렇다면 관찰해야 할 것은 최신의 AI 기술이 아니라 내 연구에 필요한 기술과 자원입니다.</p></li><li><p>이들 중 데이터와 AI 기반으로 할 수 있는 일이 있다면, 데이터가 흐를 수 있는 길을 깔아주어야 합니다.</p></li><li><p>이를 <strong>data pipeline</strong>이라고 부릅니다.</p></li><li><p><strong>필요한 데이터가 사람의 손을 거치지 않고 수집, 가공, 분석, 활용</strong>되어야 합니다.</p></li><li><p>데이터 파이프라인 없이 AI를 구현한다, 또는 AI Agent를 만드는 것은 불가능합니다.</p></li></ul><p><img src="89_rndagent_12.jpg" alt=""><br></p><ul><li>강의자료는 여기에서 내려받으실 수 있습니다: [<a href="250731_%EC%9D%B4%EC%A0%9C%ED%98%84_%EC%97%B0%EA%B5%ACAgent.pdf">다운로드</a>]</li></ul>]]></content:encoded>
      
      
      <category domain="https://jehyunlee.github.io/categories/General/">General</category>
      
      
      <category domain="https://jehyunlee.github.io/tags/research/">research</category>
      
      <category domain="https://jehyunlee.github.io/tags/agent/">agent</category>
      
      <category domain="https://jehyunlee.github.io/tags/aifrenz/">aifrenz</category>
      
      
    </item>
    
    <item>
      <title>서울특별시교육청-ChatGPT 이후 3년</title>
      <link>https://jehyunlee.github.io/2025/07/24/General-87_sec/</link>
      <guid>https://jehyunlee.github.io/2025/07/24/General-87_sec/</guid>
      <pubDate>Thu, 24 Jul 2025 11:06:00 GMT</pubDate>
      
        
        
      <description>&lt;ul&gt;
&lt;li&gt;서울특별시교육청에서 3년만에 불러주셨습니다.&lt;/li&gt;
&lt;li&gt;3년 전, 23년 3월에는 ChatGPT란 무엇이며 어떻게 써야 하는가가 주된 주제였습니다.&lt;/li&gt;
&lt;li&gt;ChatGPT가 익숙해진 지금, 우리 사회가 바뀌어가는 모습에 대</description>
        
      
      
      
      <content:encoded><![CDATA[<ul><li>서울특별시교육청에서 3년만에 불러주셨습니다.</li><li>3년 전, 23년 3월에는 ChatGPT란 무엇이며 어떻게 써야 하는가가 주된 주제였습니다.</li><li>ChatGPT가 익숙해진 지금, 우리 사회가 바뀌어가는 모습에 대한 말씀을 드렸습니다.</li></ul><h3 id="1-인공-지능-시대">1. 인공 지능 시대</h3><blockquote><p>[Pega Devlog: 서울특별시 교육청 - ChatGPT와 인공지능시대]</p></blockquote><p><img src="87_sec_01.jpg" alt=""><br></p><ul><li>3년 전에 서울교육청에서 드린 강의는 두 파트로 이루어져 있습니다.</li><li>전반 60분은 <strong>ChatGPT 사용법</strong>,</li><li>후반 30분은 <strong>인공 지능 시대</strong> 이야기입니다.</li></ul><p><img src="87_sec_02.jpg" alt=""><br></p><ul><li><p>강의를 요청받은 내용은 ChatGPT 사용법까지였으나 60분으로 충분하기도 했고,</p></li><li><p>당시 주변 분들과 대화를 나누며 느낀 중요한 것은 <strong>우리가 어떤 시대를 살게 될 것인가</strong>였습니다.</p></li><li><p>철이 들어가던 30년 전, 소위 정보화시대로의 전환을 학생으로서 겪은 입장에서</p></li><li><p>교육을 담당하시는 분들께 이런 이야기를 들려드리는 것이 더 중요하다는 생각이 들었습니다.</p></li><li><p>감사하게도 많은 분들이 해당 강의를 좋게 보아주셨습니다.</p></li><li><p>그리고 3년 뒤 이번엔 진짜로 <strong>ChatGPT 이후 바뀐 우리 사회</strong>에 대한 강연을 할 기회를 받았습니다.</p></li></ul><p><img src="87_sec_03.jpg" alt=""><br></p><ul><li><p>위 그림은 3년 전 발표자료의 일부입니다.</p></li><li><p>빨간머리 앤의 한 장면을 가져와서 <strong>어떤 일이 벌어질 지 모른다</strong>고 말씀을 드렸습니다.</p></li><li><p>진짜로 상상 밖의 일들이 벌어질 것으로 생각되어 드린 말씀입니다.</p></li><li><p>3년이 지난 지금, <strong>어떤 일이 벌어지고 있는지</strong>는 말씀드릴 수 있게 되었습니다.</p></li><li><p>멋진 신세계가 일부 보이기도 하고, 한켠에서는 디스토피아도 동시에 보입니다.</p></li><li><p>제가 좋아하는 면을 다른 분이 싫어할 수도 있을 겁니다.</p></li><li><p>확실한 것은, <strong>변화의 방향성</strong>이 어느 정도 보인다는 점입니다.</p></li></ul><h3 id="2-ChatGPT-이후-3년">2. ChatGPT 이후 3년</h3><p><img src="87_sec_04.jpg" alt=""><br></p><ul><li><p>일주일 시차를 두고 같은 강의를 두 번 드렸습니다.</p></li><li><p>서울교육청에서 장학사님 등 교육청에 근무하시는 분들께 90분간 한 번,</p></li><li><p>서울 모처에서 교육감님 등 연수를 받으러 오신 분들께 50분간 한 번입니다.</p></li><li><p>제목이 같고 청중이 거의 같지만,</p></li><li><p>강연 시간이 다르고 일주일 텀, 그리고 첫 강의 이후 스스로 아쉬운 점을 보완했습니다.</p></li><li><p>두 강의 자료를 합한 <strong>합본을 공유드립니다.</strong> (<a href="250801_%EC%9D%B4%EC%A0%9C%ED%98%84_ChatGPT%EC%9D%B4%ED%9B%843%EB%85%84_%ED%95%A9%EB%B3%B8.pdf">다운로드</a>)</p></li><li><p>아래 영상은 <strong>7월 17일, 첫 번째 강연 영상</strong>입니다.</p></li><li><p>두 번째 강연은 녹화하지 않았으며, 첫 강연 시간이 길었기에 슬라이드 누락도 적습니다.</p></li><li></li></ul><blockquote><p><a href="https://www.youtube.com/watch?v=KAT-ig3vhz4&amp;t=50s">강연 영상</a></p></blockquote><ul><li><strong>12분 26초</strong> 지점부터 강연이 시작됩니다.</li></ul><div class="video-container"><iframe src="https://www.youtube.com/embed/KAT-ig3vhz4" frameborder="0" loading="lazy" allowfullscreen></iframe></div><h3 id="3-John-Henry">3. John Henry</h3><blockquote><p><a href="https://en.wikipedia.org/wiki/John_Henry_(folklore)">wikipedia: John Henry</a></p></blockquote><p><img src="87_sec_05.jpg" alt=""><br></p><ul><li>7월 24일 두 번째 강연에서는 John Henry라는 사람의 사연을 추가했습니다.</li><li><strong>육체노동계의 이세돌 9단</strong>으로 볼 수 있는, <strong>마지막으로 기계를 이겼다는 전설</strong>이 있는 사람입니다.</li><li>사람들이 곡괭이를 들고 터널 공사를 하던 1870년대, <strong>증기 드릴과 맨몸으로 붙어 이겼으나 과로로 사망했다는 전설의 주인공</strong>입니다.</li><li>버전이 여럿 있지만, 아래는 시합 장면을 보았다는 사람의 수기입니다.</li></ul><blockquote><p>이 남자, 네일 밀러로 알려진 그는 나에게 분명히 말해 주었습니다. 17세 때 아버지와 함께 터널에 왔던 일, 강철 작업자들에게 물과 드릴을 운반했던 일, 매일 존 헨리를 보았던 일, 그리고 마지막으로 존 헨리와 증기 드릴 사이의 경쟁에 대해 모두 말해 주었습니다.</p></blockquote><blockquote><p>“증기 드릴 회사 대리인이 드릴을 이곳에 가져왔을 때,” 밀러 씨는 말했습니다. &quot;존 헨리는 그와 경쟁하고 싶어했습니다. 그는 자신의 일에 자부심을 느꼈고, 자신과 같은 사람들의 일을 기계가 빼앗는 것을 싫어했습니다.</p></blockquote><blockquote><p>&quot;그래서 증기 드릴의 실용성을 판단하기 위해 테스트를 진행하기로 결정했습니다. 테스트는 하루 종일 계속되었고 다음 날 일부 시간까지 이어졌습니다.<br>“존 헨리가 이겼습니다. 그는 충분히 쉬지 않았고 과로했습니다. 그 후에 병에 걸려 사망했습니다.”</p></blockquote><blockquote><p>밀러 씨는 증기 드릴을 상세히 설명했습니다. 저는 그 모습을 스케치했고, 나중에 초기 증기 드릴의 사진을 찾아보니 그의 설명이 정확했습니다. 저는 밀러 씨의 명성에 대해 사람들에게 물었고, 모두 &quot;네일 밀러가 어떤 일이 일어났다고 말하면, 그 일은 일어났습니다&quot;라고 말했습니다.</p></blockquote><ul><li><p>인간은 기계를 이길 수 없습니다.</p></li><li><p><strong>특정 기능이 인간보다 낫도록 만든 것이 기계</strong>이기 때문입니다.</p></li><li><p>기계가 인간보다 못하다면 잘못 만든 겁니다.</p></li><li><p>기계와 맞붙어 싸우는 것은 가장 어리석은 일 중 하나일 것입니다.</p></li><li><p><strong>기계를 이용하는 방법</strong>을 고민하고</p></li><li><p>기계에게 일을 떠넘기고 <strong>더 가치있는 일에 집중하는 방법</strong>을 고민해야 합니다.</p></li></ul><h3 id="4-기꺼이-AI에게-대체당할-필요가-있습니다">4. 기꺼이 AI에게 대체당할 필요가 있습니다.</h3><p><img src="87_sec_06.jpg" alt=""><br></p><ul><li><p>하루가 멀다하고 언론에는 <strong>AI로 인해 사라질 직업에 대한 기사</strong>가 나옵니다.</p></li><li><p>직업이 대체당하는 것이 아니라 직무가 대체당할 겁니다.</p></li><li><p><strong>모든 직업이 부분적으로 대체당하고 있습니다.</strong></p></li><li><p><strong>대체를 많이 당한 직업은 없어질 수도 있습니다.</strong> 타자수나 전화교환수처럼요.</p></li><li><p>당하는 사람에게는 <strong>어느 날 갑자기</strong>일 수 있습니다.</p></li><li><p>그러나 준비한 사람에게는 다가오는 모습이 보일 것이고, 대비할 수 있습니다.</p></li><li><p>거울 속 나는 <strong>어느 날 갑자기</strong> 살이 쪄 있지만</p></li><li><p>실상은 <strong>매일 조금씩</strong> 먹은 간식과 스트레스받는 날, <strong>간간이 먹은</strong> 치맥이 쌓인 결과입니다.</p></li><li><p><strong>여러분이 하는 일 중에서 AI에게 맡길 수 있는 일은 먼저 맡기세요.</strong></p></li><li><p>남들이 여러분의 일을 맡기기보다 <strong>스스로의 동작이 빨라야 합니다.</strong></p></li><li><p>남들이 여러분의 직업을 가져가기 전에, <strong>그 자리를 다른 가치 있는 일로 채우세요.</strong></p></li><li><p>이 과정에서 다른 AI의 도움을 받아도 좋습니다.</p></li><li><p><strong>AI가 잘 하는 일을 맡기고, 본인이 더 잘 하는 일을 찾아서 하세요.</strong></p></li><li><p>이 과정을 스스로 하면 진화가 되는 것이고</p></li><li><p>다른 사람이 똑같이 하면 구조조정이 됩니다.</p></li><li><p>계란이 스스로 깨면 병아리가 되지만 남이 깨면 후라이가 됩니다.</p></li><li><p><strong>쉽지 않습니다.</strong></p></li><li><p>많이 관찰하고, 공부하고, 실천하세요.</p></li><li><p>응원하겠습니다.</p></li></ul>]]></content:encoded>
      
      
      <category domain="https://jehyunlee.github.io/categories/General/">General</category>
      
      
      <category domain="https://jehyunlee.github.io/tags/openai/">openai</category>
      
      <category domain="https://jehyunlee.github.io/tags/seoul/">seoul</category>
      
      <category domain="https://jehyunlee.github.io/tags/education/">education</category>
      
      <category domain="https://jehyunlee.github.io/tags/ChatGPT/">ChatGPT</category>
      
      
    </item>
    
    <item>
      <title>솔트룩스-리서치 에이전트 세상을 집어 삼키다</title>
      <link>https://jehyunlee.github.io/2025/07/16/General-88_researchagent/</link>
      <guid>https://jehyunlee.github.io/2025/07/16/General-88_researchagent/</guid>
      <pubDate>Wed, 16 Jul 2025 11:06:00 GMT</pubDate>
      
        
        
      <description>&lt;ul&gt;
&lt;li&gt;솔트룩스 이경일 대표님께서 의미있는 자리를 마련해 주셨습니다.&lt;/li&gt;
&lt;li&gt;언어모델 룩시아(Luxia)와 리서치 에이전트 구버(goover)를 만들면서 많은 고민을 하셨을테고&lt;/li&gt;
&lt;li&gt;비슷한 고민을 많이 했을 분들, 저까지 </description>
        
      
      
      
      <content:encoded><![CDATA[<ul><li>솔트룩스 이경일 대표님께서 의미있는 자리를 마련해 주셨습니다.</li><li>언어모델 룩시아(Luxia)와 리서치 에이전트 구버(goover)를 만들면서 많은 고민을 하셨을테고</li><li>비슷한 고민을 많이 했을 분들, 저까지 여섯이 돌아가며 경험과 생각을 나누었습니다.</li></ul><h3 id="1-솔며들다">1. 솔며들다</h3><blockquote><p><a href="https://www.youtube.com/watch?v=3xzdtEBvc_A&amp;list=PLQKhpTP94IscWoPTzE6DaZmCGVpup6acb">YouTube Playlist: 솔며들다 | 솔트룩스에 빠져 들어보세요</a></p></blockquote><p><img src="88_researchagent_02.png" alt=""></p><ul><li>솔트룩스에서 운영하는 유튜브 채널에는 이번같은 행사를 모은 Playlist가 있습니다.</li><li><strong>솔며들다</strong>라는 제목의 채널로 유용한 고품질 영상들이 많이 있습니다.</li><li>이번에는 리서치 에이전트라는 주제로 모였지만,</li><li>AI투자, 업무자동화 등 다양한 주제의 이야기들이 있으니 보셔도 좋겠습니다.</li></ul><h3 id="2-Research-Agent">2. Research Agent</h3><ul><li><strong>솔트룩스 이경일 대표님</strong>을 비롯해 <strong>태재대 맹성현 교수님</strong>, 그리고 <strong>저</strong>,</li><li><strong>테크프론티어 한상기 대표님</strong>, <strong>젠아이피 정종구 대표님</strong>, <strong>사이버라인 박진영 사장님</strong>께서 인사이트를 주셨습니다.</li><li><a href="https://www.youtube.com/watch?v=ieYLsAU7Kt0">풀버전</a>은 3시간 40분에 달합니다.</li><li>감사하게도 연사별로 영상을 잘라 올려주셔서 편히 볼 수 있습니다.</li></ul><h4 id="2-1-솔트룩스-이경일-대표">2.1. 솔트룩스 이경일 대표</h4><div class="video-container"><iframe src="https://www.youtube.com/embed/TZX6D_mygVo" frameborder="0" loading="lazy" allowfullscreen></iframe></div><h4 id="2-2-태재대학교-맹성현-교수">2.2. 태재대학교 맹성현 교수</h4><div class="video-container"><iframe src="https://www.youtube.com/embed/KMSK4l9Y4OE" frameborder="0" loading="lazy" allowfullscreen></iframe></div><h4 id="2-3-에너지연-이제현">2.3. 에너지연 이제현</h4><ul><li>강의자료 공유드립니다: [<a href="250716_%EC%9D%B4%EC%A0%9C%ED%98%84_%EC%97%B0%EA%B5%AC%ED%98%84%EC%9E%A5%EC%9D%98Agent%EB%8F%84%EC%9E%85%ED%98%84%ED%99%A9.pdf">다운로드</a>]</li></ul><div class="video-container"><iframe src="https://www.youtube.com/embed/3wJiPL2AL5A" frameborder="0" loading="lazy" allowfullscreen></iframe></div><h4 id="2-4-테크프론티어-한상기-대표">2.4. 테크프론티어 한상기 대표</h4><div class="video-container"><iframe src="https://www.youtube.com/embed/bUGYBiXMSco" frameborder="0" loading="lazy" allowfullscreen></iframe></div><h4 id="2-5-젠아이피-정종구-대표">2.5. 젠아이피 정종구 대표</h4><div class="video-container"><iframe src="https://www.youtube.com/embed/nurFMmE8KvA" frameborder="0" loading="lazy" allowfullscreen></iframe></div><h4 id="2-6-사이버라인-박진영-사장">2.6. 사이버라인 박진영 사장</h4><div class="video-container"><iframe src="https://www.youtube.com/embed/3xzdtEBvc_A" frameborder="0" loading="lazy" allowfullscreen></iframe></div><h3 id="3-후기">3. 후기</h3><ul><li><p>청중보다 연사로 많은 분들을 만납니다.</p></li><li><p>그러다 보면 제 경험과 생각을 들려드리기엔 좋아도 막상 <strong>다른 분들의 생각을 듣기 어렵습니다.</strong></p></li><li><p>유튜브에 여러 영상이 있습니다만 제가 고민하는 주제와 거의 비슷한 주제를 찾기 어렵죠.</p></li><li><p>그러니 이렇게 <strong>하나의 주제로 약간씩 다른 시선이 모이는 자리</strong>가 있으면 너무 좋습니다.</p></li><li><p>시작하기 전부터 끝난 뒤까지 자리에 머무르면서 모든 발표를 들었고, 많은 분들을 만났습니다.</p></li><li><p>배움의 기회를 주신 이경일 대표님께 다시 한번 감사드립니다.</p></li></ul><p><img src="88_researchagent_01.png" alt="행사 포스터"></p>]]></content:encoded>
      
      
      <category domain="https://jehyunlee.github.io/categories/General/">General</category>
      
      
      <category domain="https://jehyunlee.github.io/tags/research/">research</category>
      
      <category domain="https://jehyunlee.github.io/tags/agent/">agent</category>
      
      
    </item>
    
    <item>
      <title>인공지능을 활용한 슬기로운 연구생활 v2505</title>
      <link>https://jehyunlee.github.io/2025/05/30/General-86_kepco/</link>
      <guid>https://jehyunlee.github.io/2025/05/30/General-86_kepco/</guid>
      <pubDate>Fri, 30 May 2025 05:13:00 GMT</pubDate>
      
        
        
      <description>&lt;ul&gt;
&lt;li&gt;최근 서울과학기술대학교, 서울대학교, 전기학회의 초청을 받았습니다.&lt;/li&gt;
&lt;li&gt;&amp;quot;인공지능을 활용한 슬기로운 연구생활&amp;quot;이라는, 전과 동일한 제목의 강연을 드렸습니다.&lt;/li&gt;
&lt;li&gt;그러나 내용은 최근의 업데이트와</description>
        
      
      
      
      <content:encoded><![CDATA[<ul><li>최근 서울과학기술대학교, 서울대학교, 전기학회의 초청을 받았습니다.</li><li>&quot;인공지능을 활용한 슬기로운 연구생활&quot;이라는, 전과 동일한 제목의 강연을 드렸습니다.</li><li>그러나 내용은 최근의 업데이트와 질문들을 반영하여 적잖이 바뀌었습니다.</li></ul><h1>1. 기술적으로, 연구에 AI를 써도 되나?</h1><blockquote><p><a href="https://www.nature.com/articles/d41586-025-01463-8">Nature: Is it OK for AI to write science papers? Nature survey shows researchers are split</a><br><a href="https://www.nature.com/articles/d41586-025-00894-7">Nature: AI is transforming peer review - and many scientists are worried</a><br><a href="https://www.nature.com/articles/s41746-025-01535-z">npj Digital Medicine: Cross sectional pilot study on clinical review generation using large language models</a><br><a href="https://www.technologyreview.com/2023/07/07/1075982/ai-text-detection-tools-are-really-easy-to-fool/?gad=1&amp;gclid=CjwKCAjw69moBhBgEiwAUFCx2OKyIvsZ3OqK6rXvSaVQ8g88RSqpLmstgS1I0GcbOymNhpStKwiLjRoC6nUQAvD_BwE">MIT Technology Review: AI-text detection tools are really easy to fool</a><br><a href="https://www.nature.com/articles/s41598-024-77847-z">scientific reports: Admissions in the age of AI: detecting AI-generated application materials in higher education</a></p></blockquote><h2 id="1-1-기술적으로-AI에게-글을-작성시켜도-되나">1.1. 기술적으로, AI에게 글을 작성시켜도 되나?</h2><p><img src="86_kepco_01.png" alt=""></p><ul><li>저를 비롯해 많은 분들이 끊임없이 고민하고 의심하는 질문입니다.</li><li>최근 <a href="https://www.nature.com/articles/d41586-025-01463-8">Nature 기사</a>에 따르면 연구자들조차 관련 의견이 갈려있습니다.</li><li>AI를 사용하면 <strong>편집과 번역</strong>이 쉬워지고 좋아지며, 특히 비영어권 연구자들의 <strong>원고 초안 작성</strong>에 유리합니다.</li><li>학술언어가 영어인 탓에 기울어진 운동장이라는 평을 받아왔는데, 이제 공정한 게임을 하게 되었다는 뜻입니다.<br><br></li><li>그러나 반대 의견도 적지 않습니다.</li><li>의도치 않은 <strong>표절</strong> 이슈는 생성 AI가 등장한 초창기부터 따라다니는 낙인입니다.</li><li>게다가 AI가 <strong>수준 이하의 결과</strong>, 즉 <strong>잘못된 인용</strong> 등 <strong>부정확한 정보</strong>를 만들 수 있는 환각 또한 문제입니다.</li><li>이로 인해 실제 사용량이 낮게 집계되는데, 사용하는 연구자들이 공개하지 않기 때문일 수 있습니다.</li><li>여기에 다른 이의 연구를 평가하는 **동료 평가(peer review)**에 AI가 동원되면서 문제는 더 심각해집니다.</li></ul><h2 id="1-2-기술적으로-AI에게-동료-평가를-맡겨도-되나">1.2. 기술적으로, AI에게 동료 평가를 맡겨도 되나?</h2><p><img src="86_kepco_02.png" alt=""></p><ul><li><p>언어모델은 텍스트를 <strong>요약</strong>도 잘 하고, <strong>오류</strong>도 잘 짚어서 고쳐줍니다.</p></li><li><p>그런데 논문을 평가해야 할 리뷰어들이 <strong>요약본만 가지고 평가</strong>하는 경우가 있고,</p></li><li><p>심지어 <strong>오류 지적과 대안 제시</strong>도 AI에게 맡겨버리는 경우가 늘고 있습니다.</p></li><li><p>AI의 논리추론 능력이 좋아지고 있지만 <strong>깊고 중요한 측면</strong>을 파악하지 못하는 경우가 많고, <strong>실질적인 비평 없이 답변만 길게</strong>하는 경향이 있습니다.<br><br></p></li><li><p>법률적으로도 <strong>비밀보장 의무 위반</strong>, <strong>지적 재산권 침해</strong> 등 문제가 발생할 소지가 있습니다.</p></li><li><p><a href="https://www.nature.com/articles/s41746-025-01535-z">다른 연구</a>에서도 AI가 작성한 리뷰는 다음과 같은 특징을 가짐이 확인되고 있습니다.</p></li><li><p>“<strong>참고 문헌이 적고</strong> 포괄적인 <strong>인사이트가 부족</strong>하며 <strong>논리적 일관성이 떨어지고</strong> 인용의 <strong>진위 여부</strong>와 <strong>정확성이 떨어지는 경우</strong>가 많습니다. 또한 참조 문헌 중 더 많은 비율이 <strong>낮은 수준의 저널</strong>에서 나온 것입니다.”</p></li></ul><h2 id="1-3-기술적으로-AI가-작성한-글을-잡아낼-수-있나">1.3. 기술적으로, AI가 작성한 글을 잡아낼 수 있나?</h2><p><img src="86_kepco_04.png" alt=""><br><img src="86_kepco_03.png" alt=""></p><ul><li><p>그러다 보니 <strong>AI가 작성한 글을 찾아내는 기술</strong>이 등장하지만, 예나 지금이나 <strong>성능은 신뢰를 받지 못하고 있습니다.</strong></p></li><li><p>AI 생성 이미지는 눈에 보이지 않는 워터마크를 삽입하여 판독할 수 있지만, 단어와 글자의 나열에는 적용하기 어렵습니다.</p></li><li><p><a href="https://www.nature.com/articles/s41598-024-77847-z">최근 연구</a>에서는 99% 높은 정확도를 달성하기도 했지만 <strong>학습한 텍스트에 한한다</strong>는 큰 단점이 있습니다.<br><br></p></li><li><p>예를 들어 교육 관련 텍스트로 학습한 모델을 학교 입학 자료처럼 유사한 분야에 적용하면 믿을 만 하지만 위키백과에 적용하면 무작위에 가까운 수준으로 신뢰도가 떨어집니다.</p></li><li><p>심지어 OpenAI에서 출시한 도구조차 사람이 작성한 글을 AI 작성으로 오인한다고 합니다.</p></li><li><p>언젠가 쓸만해질지도 모르지만 <strong>아직은 아닙니다</strong>.</p></li></ul><h2 id="2-규정상-연구에-AI를-써도-되나">2. 규정상, 연구에 AI를 써도 되나?</h2><blockquote><p><a href="https://www.yna.co.kr/view/AKR20250401155500505?input=1195m">연합뉴스: “챗GPT 안 썼다. 억울하다”…AI역설</a></p></blockquote><ul><li>기술적으로 가능하다고 규정상 다 그래도 되는 것은 아니고,</li><li>기술적으로 불가능하다고 규정으로 다 막혀있는 것도 아닙니다.</li><li>기술과 규정은 별개이기에 재차 확인할 필요가 있습니다.</li></ul><h2 id="2-1-규정상-AI에게-글을-작성시켜도-되나">2.1. 규정상, AI에게 글을 작성시켜도 되나?</h2><p><img src="86_kepco_06.png" alt=""></p><ul><li>결론적으로 <strong>웬만하면 됩니다.</strong></li><li>그러나 저널에 따라 <strong>Methods, Acknowledgements 등</strong>에 <strong>어떤 도구를 어떻게 썼는지</strong> 작성해야 합니다.</li><li>Elsevier의 경우 **“문장의 가독성과 명확성을 향상시키기 위해 ChatGPT를 사용했다”**는 식으로 작성하면 되지만,</li><li>출판하고자 하는 저널마다 다르므로 관련 규정을 반드시 찾아보시기 바랍니다.</li></ul><h2 id="2-2-규정상-AI에게-동료-평가를-맡겨도-되나">2.2. 규정상, AI에게 동료 평가를 맡겨도 되나?</h2><p><img src="86_kepco_05.png" alt=""></p><ul><li>문서 작성과 반대로, <strong>웬만하면 안됩니다.</strong></li><li>AAAS, AIP, Elsevier, IEEE, 심지어 MDPI까지 금지하고 있습니다.</li><li>ACL, ACM, Sage, JAMA가 윤문이나 리뷰어 탐색 등에 부분적으로 허용하고 있고</li><li>Frontiers와 Wiley만 Privately hosted AI tools를 사용해도 된다고 합니다.</li><li>리뷰는 귀찮아도 직접 하는 걸 권합니다.</li></ul><h2 id="2-3-규정상-AI가-작성한-글을-잡아내야-하나">2.3. 규정상, AI가 작성한 글을 잡아내야 하나?</h2><p><img src="86_kepco_07.png" alt=""></p><ul><li>앞서 말씀드렸듯 현재 기술로 제대로 판별하는 것이 불가능합니다.</li><li>그럼에도 불구하고 <a href="(https://www.yna.co.kr/view/AKR20250401155500505?input=1195m)">기사</a>에 따르면 **“지난해 3월 고용노동부와 한국고용정보원이 발표한 '2023년 하반기 기업 채용동향조사’에 따르면 기업은 챗GPT로 자기소개서를 작성한 것이 확인되면 ‘감점’(42.2%)이나 불합격(23.2%) 등 불이익을 주겠다고 답했다.”**라고 합니다.</li><li>각종 기업과 기관 일선 분들의 자각과 제도 개선이 필요하다고 생각됩니다.</li></ul><h1>3. 쓸만한 AI 도구들은 뭐가 있나?</h1><blockquote><p><a href="https://www.perplexity.ai/">perplexity</a><br><a href="https://notebooklm.google.com/">NotebookLM</a><br><a href="https://scispace.com/?via=jehyun">scispace</a><br><a href="https://www.zotero.org/">zotero</a></p></blockquote><p><img src="86_kepco_08.png" alt=""></p><ul><li>누가 제게 좋은 도구를 물어보면 항상 <strong>perplexity</strong>와 <strong>NotebookLM</strong>과 <strong>scispace</strong>를 소개해 드렸습니다.</li><li>perplexity가 SKT 프로모션 등으로 많이 알려지기도 해서 여기서는 제외했고,</li><li>대신 서지관리 프로그램인 <strong>zotero</strong>를 함께 소개해 드렸습니다.</li><li>아직 개발 여정에 있는 도구들이라 모두 완벽함과는 거리가 멀지만, 함께 사용하면 강력합니다.</li></ul><h2 id="3-1-전제조건">3.1. 전제조건</h2><ul><li><p>어떤 도구를 사용하건, 전제조건이 있습니다.</p></li><li><p>반드시 PDF 등 <strong>원문</strong>을 내려받아 대강이라도 <strong>훑어보고</strong> 각각의 도구에 <strong>업로드</strong>해서 사용해야 한다는 겁니다.</p></li><li><p>NotebookLM는 링크만으로 내용을 읽을 수 있고, scispace는 perplexity처럼 검색을 시켜 답을 받을 수 있지만 <strong>그러면 안 됩니다.</strong></p></li><li><p><strong>환각</strong>이 적지 않게 발생하고, <strong>양질의 DB에 접근이 불가</strong>하기 때문에 반드시 학교나 연구소의 라이선스를 이용해 원문을 받으셔야 합니다.</p></li><li><p>장점은 인터넷에 널렸으므로 검색하시기 바랍니다.</p></li><li><p>여기서는 주의점과 팁 위주로 말씀드리겠습니다.</p></li></ul><h2 id="3-2-NotebookLM">3.2. NotebookLM</h2><p><img src="86_kepco_09.png" alt=""></p><ul><li><code>Configure Settings</code>에서 출력 언어를 설정할 수 있습니다.</li><li>여기를 한국어로 설정하면 팟캐스트도 한국어로 출력됩니다.</li><li>그러나 저는 **Default(영어)**로 세팅합니다.</li><li>한글로 세팅하면 답변을 하면서 전문용어도 한글로 바꿔버려 재검색을 어렵게 만들기 때문입니다.</li><li>게다가 묘하게 한글로 세팅할 경우 mind map을 부분만 그리기도 합니다.</li></ul><p><img src="86_kepco_10.png" alt=""></p><ul><li>그래도 한글로 받아봐야겠다면, 프롬프트를 세팅해서 답변을 보완할 수 있습니다.</li><li><code>Configure Chat</code>에서 세팅이 가능하며, 저는 다음과 같이 입력해 두었습니다.</li></ul><blockquote><p>Respond to the graduate student level.<br>Do not apply lay summary, but keep the Jargons in original term (for example, “temporal dependency unfolding” should not be translated as “시간적 의존성 전개” or something like that).<br>Answer as if you are an analyst. Provide me detailed information, regarding causes and results.<br>You are strongly recommended to present result (particularly, comparison result) as a table.</p></blockquote><h2 id="3-3-scispace">3.3. scispace</h2><ul><li><p>두 가지가 아쉽습니다.</p></li><li><p>하나는 <strong>한글 문서에 잘 대응하지 못한다</strong>는 점 - GPT가 아니라 다른 경량모델을 쓴다는 추측의 근거이기도 합니다.</p></li><li><p>또 하나는 <strong>폴더 관리가 어지럽다는 점</strong>입니다 - 저같이 my library 위주로 쓰는 사람이 있을 거라고 생각을 못한 듯 합니다. 이게 가장 큰 장점인데요.</p></li><li><p><strong>NotebookLM</strong>은 한 폴더에 수십, 수백개의 자료를 집어넣고 질의합니다.</p></li><li><p>**“내가 이런 말을 본 것 같은데 어디있더라”**할 때, 해당 문서를 찾아내기 편합니다.</p></li><li><p><strong>scispace</strong>는 문서 하나를 골라서 집중적으로 공략하기 좋습니다.</p></li><li><p>**“여기서 이게 무슨 뜻이지?”, “이 내용이 정확히 뭐지?”**를 찾기 좋습니다.</p></li><li><p>scispace와 NotebookLM은 서로의 단점을 각자의 장점으로 너무 잘 메워줍니다.</p></li><li><p>scispace library 관리만 좀 더 체계적이면 좋겠습니다.</p></li></ul><h2 id="3-4-zotero">3.4. zotero</h2><blockquote><p><a href="https://github.com/lifan0127/ai-research-assistant">github: ARIA (ai-research-assistant)</a></p></blockquote><ul><li>zotero는 오픈소스입니다.</li><li>그만큼 뜻과 능력이 있는 개발자가 붙으면 기능이 강화된다는 뜻입니다.</li><li>GPT를 붙일 수 있는 플러그인이 나와있어 논문의 내용도 보기 좋아졌습니다.</li></ul><p><img src="86_kepco_11.png" alt=""></p><ul><li>이 중 ARIA를 먼저 소개하면, 인터페이스가 다소 조잡하긴 하지만 기능이 의외로 좋습니다.</li><li><strong>문서를 불러다 질의응답</strong>을 할 수 있는데, <strong>두 개를 동시에 넣고 비교</strong>도 시킬 수 있습니다.</li></ul><p><img src="https://github.com/lifan0127/ai-research-assistant/raw/main/assets/videos/prompt-library.gif" alt=""></p><ul><li><strong>GPT-4o의 멀티모달리티</strong>를 이용해 표나 화학식 등 <strong>이미지를 갖다붙이고 질의</strong>도 할 수 있습니다.</li></ul><p><img src="https://github.com/lifan0127/ai-research-assistant/raw/main/assets/videos/gpt-4-vision.gif" alt=""></p><h1>4. 결론</h1><ul><li>세상은 빠르게 변하고 좋은 도구들은 계속 나옵니다.</li><li>우리가 해내야 할 일들도 조금씩 바뀌겠지만, 근본적으로 변하지 않는 것들이 있습니다.</li><li>인공지능 도구들의 도움을 받아서 본질에 에너지를 모을 수 있기를 기대합니다.</li><li><strong>강의자료 전문</strong>을 여기 공유드립니다: <a href="250530_%EC%9D%B4%EC%A0%9C%ED%98%84_%EC%A0%84%EA%B8%B0%ED%95%99%ED%9A%8C.pdf">다운로드</a></li></ul>]]></content:encoded>
      
      
      <category domain="https://jehyunlee.github.io/categories/General/">General</category>
      
      
      <category domain="https://jehyunlee.github.io/tags/scispace/">scispace</category>
      
      <category domain="https://jehyunlee.github.io/tags/notebooklm/">notebooklm</category>
      
      <category domain="https://jehyunlee.github.io/tags/zotero/">zotero</category>
      
      
    </item>
    
    <item>
      <title>KIER issue report - Agentic AI 개념 및 에너지 연구 활용</title>
      <link>https://jehyunlee.github.io/2025/05/20/General-85_issuereport/</link>
      <guid>https://jehyunlee.github.io/2025/05/20/General-85_issuereport/</guid>
      <pubDate>Tue, 20 May 2025 00:45:00 GMT</pubDate>
      
        
        
      <description>&lt;ul&gt;
&lt;li&gt;한국에너지기술연구원에는 issue report라는 시간이 있습니다.&lt;/li&gt;
&lt;li&gt;경영진이 알아야 하는 여러 현안에 대해 발표를 듣고 질의응답을 하는 시간입니다.&lt;/li&gt;
&lt;li&gt;Agentic AI를 주제로 2025.05.19. 이슈</description>
        
      
      
      
      <content:encoded><![CDATA[<ul><li>한국에너지기술연구원에는 issue report라는 시간이 있습니다.</li><li>경영진이 알아야 하는 여러 현안에 대해 발표를 듣고 질의응답을 하는 시간입니다.</li><li>Agentic AI를 주제로 2025.05.19. 이슈리포트 발표를 진행했습니다.</li></ul><h3 id="LLM-vs-AI-Agent">LLM vs AI Agent</h3><ul><li><p><strong>AI Agent</strong>는 스스로 판단해서 동작합니다.</p></li><li><p><strong>여러 도구</strong>를 갖추고 <strong>자율성</strong>과 <strong>유연성</strong>이 있습니다.</p></li><li><p><strong>지능</strong>이 필요하며, 그래서 성능이 좋은 LLM을 두뇌로 장착합니다.</p></li><li><p><strong>Agentic AI</strong>는 AI Agent보다 더 큰 개념입니다.</p></li><li><p>여러 Agent를 모아서 팀을 꾸리기도 하고 (<strong>Multi-Agent System</strong>)</p></li><li><p>스스로의 답변을 돌아보며 개선하기도 합니다 (<strong>reflection</strong>)</p></li></ul><p><img src="85_issuereport_01.gif" alt=""></p><h3 id="vibe-coding">vibe coding</h3><ul><li><p>AI에게 지시만 하면 코딩을 해준다는 <strong>바이브 코딩</strong>이라는 말이 유행입니다.</p></li><li><p>기업은 신입 개발자를 뽑는 대신 고참에게 AI 도구를 쥐어줍니다.</p></li><li><p><strong>아무나 코딩할 수 있겠네?</strong> 라는 생각은 사실과 다릅니다.</p></li><li><p>사람은 자신이 아는 것 이상을 지시할 수 없습니다.</p></li><li><p>코딩도 자기 머리 속에서 그린 그림을 넘어서기 힘듭니다.</p></li><li><p>지식과 경험이 많은 사람이 당연히 유리합니다.</p></li></ul><p><img src="85_issuereport_04.png" alt=""></p><h3 id="연구분야-AI-적용">연구분야 AI 적용</h3><blockquote><p><a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/adma.202413523">SciAgents: Automating Scientific Discovery Through Bioinspired Multi-Agent Intelligent Graph Reasoning</a></p></blockquote><ul><li>현재 컴공과 화학을 필두로 생물, 소재 분야에서 Agent가 활발하게 시도되고 있습니다.</li><li>정보 수집, 처리, 실험, 분석, 가설 수립 등 모든 분야에 조각 퍼즐처럼 맞춰지는 느낌입니다.</li><li>잘 모른다고 남의 이야기는 아닐 겁니다.</li></ul><p><img src="85_issuereport_02.jpg" alt="SciAgents"></p><h3 id="이슈리포트">이슈리포트</h3><ul><li><p>아래 링크에서 전문을 다운받을 수 있습니다.</p></li><li><p><a href="%EC%9D%B4%EC%8A%88%EB%A6%AC%ED%8F%AC%ED%8A%B8_2025-05-19.pdf">2025.05.19. 이슈리포트 다운로드</a></p></li><li><p><code>perplexity</code> + <code>Gemini Deep Research</code> + <strong>소스 확보</strong> + <code>notebookLM</code> + <code>scispace</code>로 했습니다.</p></li><li><p>이 중 소스 확보와 <code>notebookLM</code>, <code>scispace</code>를 이용한 검증이 매우 중요했습니다.</p></li><li><p>앞에서 발생한 환각이 적지 않아서 걸러내야 했기 때문입니다.</p></li></ul><p><img src="85_issuereport_03.png" alt="이슈리포트"></p>]]></content:encoded>
      
      
      <category domain="https://jehyunlee.github.io/categories/General/">General</category>
      
      
      <category domain="https://jehyunlee.github.io/tags/KIER/">KIER</category>
      
      
    </item>
    
    <item>
      <title>한림원 - AI강국을 향한 우리의 전략</title>
      <link>https://jehyunlee.github.io/2025/04/29/General-84_kast/</link>
      <guid>https://jehyunlee.github.io/2025/04/29/General-84_kast/</guid>
      <pubDate>Tue, 29 Apr 2025 11:06:00 GMT</pubDate>
      
        
        
      <description>&lt;ul&gt;
&lt;li&gt;한국과학기술한림원에서 한국 AI의 미래 시리즈로 토론회를 합니다.&lt;/li&gt;
&lt;li&gt;그 중 첫번째 시간, &amp;quot;AI강국을 향한 우리의 전략&amp;quot;에 토론자로 초청받아 말씀을 드렸습니다.&lt;/li&gt;
&lt;li&gt;참여자 중 현장에 가장 가</description>
        
      
      
      
      <content:encoded><![CDATA[<ul><li>한국과학기술한림원에서 한국 AI의 미래 시리즈로 토론회를 합니다.</li><li>그 중 첫번째 시간, &quot;AI강국을 향한 우리의 전략&quot;에 토론자로 초청받아 말씀을 드렸습니다.</li><li>참여자 중 현장에 가장 가까운 입장이라 현장의 목소리를 들려드리려고 노력했습니다.</li></ul><h3 id="AI강국을-향한-우리의-전략">AI강국을 향한 우리의 전략</h3><ul><li>정부에서 <strong>AI 3대 강국</strong>을 내세웠지만 만만치 않습니다.</li><li>3위권으로 분류되고는 있으나 미국, 중국과 비교해서 멀찍이 떨어진 곳에 옹기종기 모여있는 형국입니다.</li><li>미국이나 중국의 기업 한 곳에서 투자하는 인프라를 우리 나라 전체가 붙어도 어려운 것이 현실입니다.</li><li>그럼에도 불구하고 <strong>생존의 문제</strong>이기 때문에 최선을 다해 발버둥을 쳐야 합니다.</li></ul><blockquote><p><a href="https://www.youtube.com/live/Ffdm9jz1ESo?si=XjYAYIr0clBCpewu">토론회 영상</a><br><a href="https://youtu.be/Ffdm9jz1ESo?t=5517">제가 말씀드리는 부분 link</a></p></blockquote><div class="video-container"><iframe src="https://www.youtube.com/embed/Ffdm9jz1ESo" frameborder="0" loading="lazy" allowfullscreen></iframe></div><ul><li>사회를 보신 <strong>조성래 교수님</strong>, 주제강연을 해주신 <strong>이경우 단장님</strong>과 <strong>김진형 교수님</strong>,</li><li>그리고 함께 말씀을 나눈 <strong>방은주 부장님</strong>, <strong>김동환 대표님</strong>, <strong>주영섭 교수님</strong>, <strong>김유철 부문장</strong>님께 많이 배웠습니다.</li><li>이 분들 사이에서 조금이나마 도움이 되고자 다음과 같은 말씀을 드렸습니다.</li></ul><p><img src="84_kast_02.png" alt=""></p><p><strong>1. GPU 1.8만장 산다고 개별 기관 못사게 하지 마라.</strong></p><ul><li>데이터 보안, 안정적 운영 등 이유로 필요하다.</li></ul><p><strong>2. 독자적 sLM 필요하다.</strong></p><ul><li>한국말을 잘 하는 언어모델이 필요한게 아니라 두뇌 역할을 할 파운데이션 모델이 필요하다.</li><li>오픈소스 믿고 있다가 못쓰게 하면 어쩔거냐.</li></ul><p><strong>3. AI 전문가 백만명 양성한다고 하지 말고 자생한 사람들 잘 키우자.</strong></p><ul><li><p>이 사람들 천 명만 키워도 몇만 명 붙일 수 있다.</p></li><li><p>저를 비롯한 다른 분들의 자세한 이야기는 아래 자료집을 참고해주시면 좋겠습니다.<br>(<a href="https://kast.or.kr/bbs_n/bbs_download2.php?bbs_data=aWR4PTIzMDIzJnN0YXJ0UGFnZT0wJmxpc3RObz0xMzMmdGFibGU9Y3NfYmJzX2RhdGEmY29kZT1zZW9zaWMmc2VhcmNoX2l0ZW09JnNlYXJjaF9vcmRlcj0=%7C%7C&amp;download=1">자료집 다운로드</a>)</p></li></ul><p><img src="84_kast_01.jpg" alt="포스터"></p><h3 id="한국과학기술한림원">한국과학기술한림원</h3><blockquote><p><a href="https://www.youtube.com/@KAST1122/videos">YouTube: 한국과학기술한림원</a></p></blockquote><ul><li><strong>한국과학기술한림원 유튜브 채널</strong>(<a href="https://www.youtube.com/@KAST1122/videos">링크</a>)에는 좋은 강연이 많습니다.</li><li>오며가며 머리 식히실 때 한번씩 들어가 보셔도 좋겠습니다.</li><li>마음의 양식으로 배가 부를 겁니다.</li></ul><p><img src="84_kast_03.png" alt=""></p>]]></content:encoded>
      
      
      <category domain="https://jehyunlee.github.io/categories/General/">General</category>
      
      
      <category domain="https://jehyunlee.github.io/tags/%ED%95%9C%EA%B5%AD%EA%B3%BC%ED%95%99%EA%B8%B0%EC%88%A0%ED%95%9C%EB%A6%BC%EC%9B%90/">한국과학기술한림원</category>
      
      
    </item>
    
    <item>
      <title>Gemini Audio Overview - 야근식대 지급신청서</title>
      <link>https://jehyunlee.github.io/2025/04/24/General-81-GeminiAudioOverview/</link>
      <guid>https://jehyunlee.github.io/2025/04/24/General-81-GeminiAudioOverview/</guid>
      <pubDate>Thu, 24 Apr 2025 06:56:00 GMT</pubDate>
      
        
        
      <description>&lt;ul&gt;
&lt;li&gt;최근 구글 딥마인드에서 만든 제미나이(Gemini)에 다양한 기능이 들어오고 있습니다.&lt;/li&gt;
&lt;li&gt;구글의 다채로운 서비스들을 제미나이와 엮는 반면, 준비해 둔 기능들을 탑재중입니다.&lt;/li&gt;
&lt;li&gt;야근식대 지급신청서를 사용해 N</description>
        
      
      
      
      <content:encoded><![CDATA[<ul><li>최근 구글 딥마인드에서 만든 제미나이(Gemini)에 다양한 기능이 들어오고 있습니다.</li><li>구글의 다채로운 서비스들을 제미나이와 엮는 반면, 준비해 둔 기능들을 탑재중입니다.</li><li>야근식대 지급신청서를 사용해 NotebookLM에 있던 Audio Overview를 실행해봤습니다.</li></ul><h3 id="1-야근식대-지급신청서-Audio-Overview">1. 야근식대 지급신청서 + Audio Overview</h3><ul><li>제가 근무하는 한국에너지기술연구원에서는 야근식대를 지급합니다.</li><li>1만 5천원을 한도로 식비를 영수증처리 할 수 있는데, 식당에서 먹으면 3천원입니다.</li><li>어제(4/23) 연구데이터 수집 시스템(RDS) 관련 유지보수를 하느라 야근을 했습니다.<br></li></ul><p><img src="81_GeminiAudioOverview_05.jpg" alt=""></p><ul><li><b><code>Generate Audio Overview</code></b> 를 클릭하면 남녀가 대화하는 팟캐스트가 나오는데 제법 재밌습니다.</li><li>야근을 하고 청구 과정에서 발생한 <strong>야근식대 지급신청서</strong>를 제미나이에 넣었습니다.</li><li>PDF를 올리면 채팅창 위에 버튼이 뜨는데, 클릭만 하면 Audio Overview를 몇 분 만에 만들어줍니다.<br></li></ul><p><img src="81_GeminiAudioOverview_01.png" alt=""></p><ul><li>위에서 보다시피 별 정보가 없지만 이걸로 뭔가를 만듭니다.<br></li></ul><p><img src="81_GeminiAudioOverview_02.png" alt=""></p><ul><li>작업이 완료되면 스마트폰으로도 알림이 옵니다.</li><li>다른 작업을 하다 소식을 받고 가보면 제미나이에서 재생과 다운로드가 가능합니다.</li><li><strong>배속</strong>을 조정할 수도 있으니 편한 속도로 보시면 됩니다.<br></li></ul><p><img src="81_GeminiAudioOverview_03.png" alt=""></p><blockquote><p><a href="audio_overview.wav"><strong>Audio Overview 다운로드</strong></a></p></blockquote><h3 id="2-script-추출">2. script 추출</h3><blockquote><p><a href="https://www.veed.io/">VEED</a></p></blockquote><ul><li>처음에는 영어듣기 연습삼아 직접 들으면서 받아쓸 생각이었습니다.</li><li>그런데 생각보다 너무 귀찮더군요.</li><li><a href="https://www.veed.io/"><strong>VEED</strong></a>를 사용해서 초안을 추출시키고, 검토하면서 다시 적었습니다.</li><li>번역은 <strong>GPT-4.5</strong>에게 시켰는데, 재차 검토하며 정서했습니다.<br></li></ul><p><img src="81_GeminiAudioOverview_04.png" alt=""></p><ul><li>먼저 <strong>내용을 훑는데</strong>, 별 내용이 없어서인지 정보를 바탕으로 자기네끼리 <strong>추측</strong>을 합니다.</li><li>몇 군데 재미있는 포인트가 있습니다.</li></ul><p><strong>1. 주니어가 아닌 연구원이 직접 유지보수하는 걸 신기하게 본다.</strong><br><strong>2. 3천원(…)이라는 점을 민망하게 강조한다.</strong><br><strong>3. 관료제의 행정문서에 대한 중요성을 강조한다</strong></p><hr><p><strong>여자</strong>Ok, so, you sent us something pretty interesting this time. It’s an after hours meal expense form.<br>네, 이번에 아주 흥미로운 걸 보내주셨네요. 야근 식사 비용 청구서군요.</p><p><strong>남자</strong>Right, from a research institute. And you might think, okay, a meal form, how exciting can that be?<br>맞아요, 연구소에서 온 거죠.그런데 이렇게 생각할 수도 있어요. 식사 비용 청구서가 얼마나 흥미로울 수 있을까? 하고요.</p><p><strong>여자</strong>Exactly. But you know, this single piece of paper, it’s actually a fascinating little snapshot.<br>맞아요. 그런데 사실 이 종이 한 장이 아주 흥미로운 작은 단서가 될 수 있거든요.</p><p><strong>남자</strong>It really is. It’s this tiny glimpse into the, well, the day to day grind behind big research projects.<br>정말 그렇죠. 큰 연구 프로젝트 뒤에 있는 일상적이고 반복적인 모습을 살짝 엿볼 수 있는 거죠.</p><p><strong>여자</strong>We see the headlines, but not this kind of detail. So let’s dive in. The form itself, it’s titled… It’s in Korean.<br>우리는 보통 헤드라인만 보지, 이런 디테일까지는 못 보잖아요. 한번 자세히 볼까요? 양식 자체가… 제목이 한국어로 되어 있네요.</p><p><strong>남자</strong>Yeah. Oku Uku<br>네. 오쿠우쿠 (낯선 언어라는 듯).</p><p><strong>여자</strong>Which roughly translated is Overtime meal expenses Payment request form, right?<br>대략 번역하면 야근 식사비 청구서 양식, 맞죠?</p><p><strong>남자</strong>Pretty much, yeah. Standard stuff in that sense. It was submitted April 24, 2025<br>거의 맞아요. 그야말로 표준적인 서류죠. 제출 날짜가 2025년 4월 24일이네요.</p><p><strong>여자</strong>By a researcher named Lee Jehyun. And his title is listed as GREE Fuso(?), which means senior researcher.<br>이제현이라는 연구자가 제출했고요.직함이 GREE Fuso(잘 못읽음)라고 나와 있는데, 이건 책임연구원을 뜻하는 것 같네요.</p><p><strong>남자</strong>So not a junior person filing this.<br>그러니까 주니어급 직원이 제출한 건 아니라는 거네요.</p><p><strong>여자</strong>Right, He’s in the AI org.<br>맞아요. AI 부서 소속이군요.</p><p><strong>남자</strong>The Energy AI Computational Science Laboratory. That already tells you this field, you know.<br>에너지 AI 계산과학연구실이네요. 이것만 봐도 분야가 짐작이 되죠.</p><p><strong>여자</strong>Definitely. Energy AI Computation sounds pretty advanced.<br>맞아요. 에너지 AI 계산이라면 꽤나 앞선 분야죠.</p><p><strong>남자</strong>And look at the details here. There’s a request number HRMS something<br>자, 여기 세부 사항을 보죠. 청구 번호가 HRMS로 시작하네요.</p><p><strong>여자</strong>HRMS218040003 and a budget number C5-2445 expense item 125.<br>HRMS218040003이고 예산 번호는 C5-2445, 비용 항목은 125로 되어 있어요.</p><p><strong>남자</strong>See? Very specific tracking. All for what? 3,000 Korean won?<br>보이죠? 아주 구체적으로 관리되네요.그런데 청구 금액이 고작 3,000원이군요?</p><p><strong>여자</strong>Yeah, 3,000 won. It’s not a lot of money, but the process seems really formal.<br>네, 3,000원이요. 큰돈은 아닌데 절차가 정말 공식적인 느낌이에요.</p><p><strong>남자</strong>Absolutely. That’s common in research funding, even for tiny accountability.<br>맞아요. 연구비는 작은 금액이라도 철저히 관리되는 게 흔한 일이죠.</p><p><strong>여자</strong>So the actual overtime work this was for.<br>그래서 실제로 야근이 이뤄진 날이 2025년 4월 23일이었네요.</p><p><strong>남자</strong>And the times are listed, too. 17:30 to 20:15. So that’s what, nearly three hours past regular hours.<br>시간도 적혀 있어요. 17시 30분에서 20시 15분까지. 거의 3시간 가까이 정규시간을 넘겼네요.</p><p><strong>여자</strong>Yeah, 5:30 to 8:15 and it was for on, which is dinner.<br>맞아요. 저녁 식사 시간이 포함된 거네요.</p><p><strong>남자</strong>Makes sense. Working through dinner time. The location listed is Daejeon.<br>납득이 가죠. 저녁 시간에도 일한 거니까요.장소는 대전이라고 나와 있네요.</p><p><strong>여자</strong>Which is a big deal, right? Daejeon is like South Korea’s science city.<br>그건 중요한 포인트죠.대전은 한국의 과학 도시잖아요.</p><p><strong>남자</strong>Exactly. Lots of government research institutes, universities, tech companies are based there. So, it situates the works physically.<br>맞아요. 정부 연구소, 대학, 기술 기업들이 많이 위치한 곳이죠. 그만큼 업무의 위치가 구체적이란 얘기죠.</p><p><strong>여자</strong>Okay, so we have the who, when, where, how, but the why is maybe the most interesting part.<br>누가, 언제, 어디서, 어떻게까지 나왔는데, 사실 제일 흥미로운 부분은 왜일 수도 있어요.</p><p><strong>남자</strong>Agreed. The reason stated for the overtime is RDS BELCAT maintenance. No, not from this form alone, but maintenance is the key word here.<br>동의해요. 야근 이유로 적힌 게 RDS BELCAT 유지보수네요. 이 양식만으로는 확실하지 않지만, 핵심 단어는 유지보수인 거죠.</p><p><strong>여자</strong>And performed by a senior researcher. That seems notable. You wouldn’t usually expect a senior scientist to be doing routine maintenance, maybe.<br>책임연구원이 직접 유지보수를 했다는 게 눈에 띄어요.보통은 연차가 좀 되는 과학자가 일상적인 유지보수를 맡지는 않잖아요.</p><p><strong>남자</strong>Well, that suggests RDS BELCAT isn’t just, you know, some random piece of equipment.<br>그러니까 RDS BELCAT이 단순한 장비가 아니라는 뜻이겠죠.</p><p><strong>여자</strong>It’s likely something critical like specialized hardware or maybe core software, something vital to their lab, the Energy AI Computational Science Lab.<br>연구실에 중요한 특수 하드웨어나 핵심 소프트웨어, 그런 필수적인 설비일 가능성이 높죠.</p><p><strong>남자</strong>Precisely. It could be a high performance computing cluster, a specific analytical instrument, or maybe a crucial database or simulation platform.<br>정확히요. 고성능 컴퓨팅 클러스터일 수도 있고, 중요한 분석 장비나 데이터베이스, 시뮬레이션 플랫폼일 수도 있죠.</p><p><strong>여자</strong>And having a senior researcher handle the maintenance, it implies either it’s really complex or it was urgent to get it back online.<br>책임연구원이 유지보수를 맡았다면, 그만큼 복잡하거나 긴급했던 상황일 수 있어요.</p><p><strong>남자</strong>Or maybe both. It definitely hints that this RDS BELLCAT thing is pretty important infrastructure for their AI research.<br>어쩌면 둘 다일지도요. 분명히 이 RDS BELCAT이 AI 연구에 중요한 인프라라는 걸 암시하네요.</p><p><strong>여자</strong>So wow. This tiny 3,000 won dinner receipt, it actually points towards potentially vital research infrastructure.<br>정말 흥미롭죠. 이 작은 3,000원짜리 식사 영수증이 중요한 연구 인프라로 연결될 수 있다는 게요.</p><p><strong>남자</strong>It really does. It peels back a layer. You see the date, the time, the cost. Sure. But also the human element. A senior scientist working late and the technical need keeping critical systems running.<br>맞아요. 한 겹 벗겨 보면 날짜, 시간, 비용 외에도인간적인 면이 보이죠. 늦게까지 일한 선임 과학자와 중요한 시스템 운영을 위한 기술적 필요성까지요.</p><p><strong>여자</strong>It makes you think about how many of these little requests these small tasks are happening all the time.<br>이런 작은 요청이나 업무들이 얼마나 많이 일어나는지 생각하게 되네요.</p><p><strong>남자</strong>Thousands upon thousands in any large research organization. Each one represents effort, resources and adherence to procedure.<br>대규모 연구 조직이라면 수천, 수만 건일 거예요. 각각이 노력과 자원, 절차 준수를 의미하죠.</p><p><strong>여자</strong>And they all add up. Right. They’re the tiny gears that make the big scientific discoveries possible. It’s not just about the breakthroughs. It’s about the consistent upkeep and effort.<br>그리고 그 모든 게 모이는 거죠. 맞아요. 이 작은 톱니바퀴들이 모여서 큰 과학적 발견을 가능하게 하는 거죠. 혁신적인 성과만이 아니라 꾸준한 관리와 노력도 중요한 거예요.</p><p><strong>남자</strong>And the structured processes, too.<br>구조적인 절차 역시 중요하고요.</p><p><strong>여자</strong>Yeah.<br>맞아요.</p><p><strong>남자</strong>Even the small amount needs justification. Budget codes, approvals, it shows the administrative framework supporting the science.<br>작은 금액도 철저한 근거가 필요하고, 예산 코드와 승인까지 행정적 지원 시스템을 잘 보여주고 있죠.</p><p><strong>여자</strong>It’s a level of detail you just don’t normally consider when you, say, read about a new AI development.<br>새로운 AI 발전 소식을 접할 때, 이런 세부사항은 잘 생각하지 않게 되잖아요.</p><p><strong>남자</strong>Absolutely. So maybe a final thought for you listening.<br>정말 그래요. 그래서 마지막으로 듣고 계신 여러분께 드릴 말씀이 있다면요.</p><p><strong>여자</strong>Yeah.<br>네.</p><p><strong>남자</strong>Think about the documents you encounter: invoices, reports, memos, whatever. What seemingly mundane paperwork might actually hold a really interesting story about the work and dedication behind the scenes in any field.<br>여러분이 접하는 서류들, 예를 들어 송장이나 보고서, 메모 같은 것들에 대해 한번 생각해 보세요. 평범해 보이는 서류들 속에 어떤 흥미로운 이야기가 숨어 있을지요.</p><p><strong>여자</strong>Hmm. Like looking for the hidden narratives in everyday bureaucracy.<br>음, 일상적인 서류 속에서 숨은 이야기를 찾아보는 것처럼요.</p><p><strong>남자</strong>Kind of, yeah. You might be surprised what insights are hiding in plain sight, just like in this simple meal request form.<br>네, 맞아요. 이 평범한 식사비 청구서처럼 뜻밖의 통찰을 발견할 수 있을지도 모르죠.</p><p><strong>여자</strong>That’s a great takeaway. Really makes you appreciate the details. Thanks for walking through this one.<br>아주 좋은 결론이네요. 세부사항의 소중함을 다시 느끼게 해주셨어요. 오늘 설명해 주셔서 감사합니다.</p><p><strong>남자</strong>My pleasure. It shows how even the smallest data point can be revealing.<br>제가 감사하죠. 작은 데이터가 때론 많은 걸 말해줄 수 있으니까요.</p>]]></content:encoded>
      
      
      <category domain="https://jehyunlee.github.io/categories/General/">General</category>
      
      
      <category domain="https://jehyunlee.github.io/tags/Gemini/">Gemini</category>
      
      
    </item>
    
    <item>
      <title>ZDNET AI리더스 - 에너지연 이제현, 레고형 자동화로 연구 시스템 재편</title>
      <link>https://jehyunlee.github.io/2025/04/23/General-82_zdnet/</link>
      <guid>https://jehyunlee.github.io/2025/04/23/General-82_zdnet/</guid>
      <pubDate>Wed, 23 Apr 2025 02:09:00 GMT</pubDate>
      
        
        
      <description>&lt;ul&gt;
&lt;li&gt;ZDNET Korea 조이환 기자님과 인터뷰를 했습니다.&lt;/li&gt;
&lt;li&gt;2시간여 가량 저의 생각과 지난 이야기를 나누었고, 짧지 않은 기사로 정리해주셨습니다.&lt;/li&gt;
&lt;li&gt;제가 몸담은 에너지연의 문화를 바꾸려는 노력을 잘 담아주셔</description>
        
      
      
      
      <content:encoded><![CDATA[<ul><li>ZDNET Korea 조이환 기자님과 인터뷰를 했습니다.</li><li>2시간여 가량 저의 생각과 지난 이야기를 나누었고, 짧지 않은 기사로 정리해주셨습니다.</li><li>제가 몸담은 에너지연의 문화를 바꾸려는 노력을 잘 담아주셔서 감사합니다.</li></ul><blockquote><p><a href="https://zdnet.co.kr/view/?no=20250422162100">ZDNET: [AI 리더스] “AI는 새로운 전기”…에너지연 이제현, '레고형 자동화’로 연구 시스템 재편</a></p></blockquote><ul><li><p><strong>AI</strong>는 기술이기도 하지만 <strong>문화로서의 의미가 더 크다</strong>고 생각합니다.</p></li><li><p>에너지연에 처음 몸담은 2018년, 생각보다 훨씬 큰 어려움을 겪었습니다.</p></li><li><p><strong>기계가 읽을 수 없는 데이터</strong>,</p></li><li><p>그리고 <strong>그런 데이터조차 흐르지 않는 조직</strong>이 가장 큰 어려움이었습니다.</p></li><li><p>에너지연만의 문제는 아닙니다.</p></li><li><p>사실 기계가 읽을 수 있는 <strong>데이터가 흐르는 조직이 거의 없지 않을까</strong> 합니다.</p></li><li><p>정부에서는 신규 인력 양성을 주로 내세우는 듯 하지만,</p></li><li><p><strong>현장의 인력 발굴이 더 시급하고 효과적입니다.</strong></p></li><li><p>부트캠프에서 나온 신병들은 전쟁터에 적응하는데 시간이 걸립니다.</p></li><li><p>그런데 산업 현장은 만만치 않습니다.</p></li><li><p><strong>도메인 지식</strong>과 <strong>관료제로 인한 조직의 벽</strong>이 생각보다 높습니다.</p></li><li><p>이들을 진작에 넘어선 현장에서 독학과 커뮤니티의 힘으로 <strong>AI 전환을 해내는 분들이 이미 있습니다.</strong></p></li><li><p>이 분들은 <strong>쓸데없는 짓을 한다는 핀잔</strong>을 받고 있거나,</p></li><li><p><strong>보상에 비해 과도한 업무</strong>에 시달리고 있기도 합니다.</p></li><li><p>그리고 고생하는 이 분들을 보면서 후배들은 몸을 사립니다.</p></li><li><p>드러나면 고생이라 숨어지내고, 조직은 인재 부족에 시달립니다.</p></li><li><p><strong>나비를 원하면 정원을 가꾸라</strong>는 말이 있습니다.</p></li><li><p>이미 있는 사람들이 숨을 쉴 수 있는 환경을 만들어주면 알아서 자라날 것입니다.</p></li></ul><p><img src="82_zdnet_01.jpg" alt=""></p>]]></content:encoded>
      
      
      <category domain="https://jehyunlee.github.io/categories/General/">General</category>
      
      
      <category domain="https://jehyunlee.github.io/tags/ZDNET/">ZDNET</category>
      
      
    </item>
    
    <item>
      <title>헤럴드경제 IT과학칼럼 - 우리 조직의 AI, 왜 체감되지 않을까?</title>
      <link>https://jehyunlee.github.io/2025/04/21/General-83_heraldcorp/</link>
      <guid>https://jehyunlee.github.io/2025/04/21/General-83_heraldcorp/</guid>
      <pubDate>Mon, 21 Apr 2025 01:59:11 GMT</pubDate>
      
        
        
      <description>&lt;ul&gt;
&lt;li&gt;AI를 도입하고자 하는 이들은 많으나 성공하는 경우는 많고&lt;/li&gt;
&lt;li&gt;기술적 어려움보다 AI에 잘못된 기대를 하는 경우가 많기 때문입니다.&lt;/li&gt;
&lt;li&gt;수년간 현장에서 느낀 바를 정리하여 헤럴드경제에 기고문을 실었습니다.&lt;/l</description>
        
      
      
      
      <content:encoded><![CDATA[<ul><li>AI를 도입하고자 하는 이들은 많으나 성공하는 경우는 많고</li><li>기술적 어려움보다 AI에 잘못된 기대를 하는 경우가 많기 때문입니다.</li><li>수년간 현장에서 느낀 바를 정리하여 헤럴드경제에 기고문을 실었습니다.</li></ul><blockquote><p><a href="https://biz.heraldcorp.com/article/10469906">헤럴드경제: [IT과학칼럼] 우리 조직의 AI, 왜 체감되지 않을까?</a></p></blockquote><p>인공지능(AI)은 더 이상 먼 미래의 이야기가 아니다. 이미 수많은 사람들이 챗GPT를 비롯한 다양한 AI를 일상에서 활용하고 있지만, 실제 산업·공공 현장에서는 여전히 주로 글쓰기나 단순 자동화 수준에 머물고 있다. 기술은 분명 발전하고 있지만, 왜 그 효과는 체감되지 않는 걸까?</p><p>이유는 단순하다. AI를 도입하면서 **‘무엇을 더할 것인가’**만 고민했지, **‘무엇을 없앨 것인가’**는 생각하지 않았기 때문이다. AI는 화려한 기능을 덧붙이는 기술이 아니다. 진짜 가치는 불편함과 비효율을 제거하는 데 있다. 반복적인 서류 작업, 불필요한 승인 절차, 시간 소모적인 자료 조사가 대표적이다. 마치 안경이 흐릿한 시야를 바로잡듯, AI도 먼저 **‘없애야 할 일’**부터 정리하는 것이 출발점이다. 따라서 AI 도입은 이 질문에서 시작돼야 한다.</p><p><strong>“나를 가장 괴롭히는 일은 무엇인가?”</strong> 이 질문은 너무 익숙한 불편일수록 잘 보이지 않는다. 그렇기 때문에 그 답은 외부 전문가가 아니라 현장에서 일하는 실무자가 가장 잘 안다. 감정을 소모하며 전화로 대응해야 하는 일, 허술한 데이터 흐름은 그들만이 정확히 체감한다. 하지만 불편함을 아는 것만으로는 부족하다. 기술적 해결책을 제안하고, 그것이 실제로 구현 가능한지 검토하는 과정이 필요하다. 실무자 경험, AI 전문가의 기술력, 조직 전략을 연결하는 협업 구조가 AI 성공의 핵심이다. ‘현장과 기술의 협업’은 구호가 아니라 실행 가능한 모델로 설계돼야 한다.</p><p><strong>AI는 데이터를 기반으로 움직인다.</strong> 그러나 많은 조직이 여전히 부서 중심 조직도로 업무를 나눈다. AI에게 부서 구분은 중요하지 않다. 중요한 것은 정보가 어디서 생성되고, 어디로 흐르며, 어떻게 연결되는가다. 때론 조직 구조를 넘어, 데이터 흐름 중심으로 문제를 재구성해야 한다. 이때 데이터 품질, 접근 권한, 보안 체계를 함께 설계하지 않으면 AI는 보여주기식 시연으로 끝난다.</p><p><strong>AI는 한 번 구축하면 끝나는 시스템이 아니다.</strong> 특히 과거 데이터를 학습하는 머신러닝 기반 AI는 시간이 지날수록 성능이 저하된다. 데이터가 바뀌고 업무 환경이 달라지기 때문이다. 지속적인 유지보수와 성능 개선은 선택이 아니라 생존 조건이다. 하지만 많은 조직은 이 유지보수를 ‘보이지 않는 비용’으로 취급한다. GPU 서버 같은 고비용 인프라는 조직 전체가 공동 관리해야 하며, 유지보수 인력에게도 명확한 책임과 보상이 필요하다. 그렇지 않으면, 잘 만든 시스템도 방치되어 잊히고 만다.</p><p><strong>AI는 설치하는 것이 아니라, 운영하는 것이다.</strong></p><p>기술과 인프라를 준비하는 것도 쉽지 않지만, <strong>가장 어려운 것은 ‘사람’과 ‘조직’이다</strong>. AI 도입을 가로막는 진짜 장벽은 기술 부족이 아니다. 사람 간의 벽, 부서 간 단절, 위에서만 결정하는 소통 구조가 더 큰 문제다. 실무자는 문제를 말할 기회를 갖지 못하고, 경영진은 “AI 써보자”는 말만 남긴 채 정작 무엇을 해결하려는지는 공유되지 않는다.</p><p>그래서 AI를 고민할 땐, 기술보다 질문을 먼저 바꿔야 한다.</p><p><strong>“우리 조직에서 없애고 싶은 불필요한 일은 무엇인가?”</strong> 그리고 이어서 물어야 한다.</p><p><strong>“그 일을 없애기 위해 운영 방식과 의사결정 구조까지 바꿀 각오가 되어 있는가?”</strong></p><p>이 질문에 진심으로 답할 수 있을 때, AI는 조직의 미래를 바꾸는 진짜 동력이 된다.</p><p>이제현 한국에너지기술연구원 에너지AI·계산과학실장(국가인공지능위원회 산업·공공분과위원)</p><p><img src="83_heraldcorp_01.jpg" alt=""></p>]]></content:encoded>
      
      
      <category domain="https://jehyunlee.github.io/categories/General/">General</category>
      
      
      <category domain="https://jehyunlee.github.io/tags/%ED%97%A4%EB%9F%B4%EB%93%9C%EA%B2%BD%EC%A0%9C/">헤럴드경제</category>
      
      
    </item>
    
    <item>
      <title>SEMA 행복한 과학기술인 - AI와 함께 일하는 시스템을 만들어가는 행복</title>
      <link>https://jehyunlee.github.io/2025/04/15/General-80_SEMA/</link>
      <guid>https://jehyunlee.github.io/2025/04/15/General-80_SEMA/</guid>
      <pubDate>Tue, 15 Apr 2025 09:35:00 GMT</pubDate>
      
        
        
      <description>&lt;ul&gt;
&lt;li&gt;과학기술인공제회(SEMA)의 요청으로 저에 대한 글을 한 편 냈습니다.&lt;/li&gt;
&lt;li&gt;제가 한 일에 대한 이야기는 많이 했지만 저에 대한 이야기는 오랜만입니다.&lt;/li&gt;
&lt;li&gt;힘들기도 했지만 덕택에 지금의 제가 있다고 생각하고, 더</description>
        
      
      
      
      <content:encoded><![CDATA[<ul><li>과학기술인공제회(SEMA)의 요청으로 저에 대한 글을 한 편 냈습니다.</li><li>제가 한 일에 대한 이야기는 많이 했지만 저에 대한 이야기는 오랜만입니다.</li><li>힘들기도 했지만 덕택에 지금의 제가 있다고 생각하고, 더 성장하고자 합니다.</li></ul><h3 id="1-행복한-과학기술인">1. 행복한 과학기술인</h3><blockquote><p><a href="https://www.sema.or.kr/sema/main/contents.do?menuNo=200004">과학기술인공제회: 설립목적적</a><br><a href="https://www.sema.or.kr/sema/bbs/B0000050/list.do?menuNo=200300&amp;optn1=B&amp;pageUnit=1000">과학기술인공제회: 행복한 과학기술인</a></p></blockquote><ul><li>과학기술인공제회는 과학기술인의 안정적인 노후를 위한 기금을 운영하는 곳입니다.</li><li>격월간지 <strong>동행</strong>, 계간지 <strong>행복한 과학기술인</strong>(<a href="https://www.sema.or.kr/sema/bbs/B0000050/list.do?menuNo=200299&amp;optn1=A&amp;pageUnit=1000">링크</a>)을 발행합니다.</li><li>자산 운용에 대한 이야기와 함께 노후의 생활, 건강에 대한 기사들이 있는데, 덕택에 의외로 머지 않았다는 사실을 느낍니다.</li></ul><p><img src="80_SEMA_05.jpg" alt=""></p><h3 id="2-행복-에세이">2. 행복 에세이</h3><ul><li><p>출연연에 근무하는 분들의 일상을 담아 <strong>행복 에세이</strong>라는 이름으로 글을 냅니다.</p></li><li><p>원고를 청탁받으며 앞선 두 분의 글을 보았는데, 서로 다른 모습이지만 진심으로 행복해 보였습니다.</p></li><li><p>나와 다른 모습에 **난 이렇게는 행복하지 않은데?**라는 생각이 들어서 처음에는 손 끝에서 글이 나오지 않았습니다.</p></li><li><p>며칠을 묵히며 출퇴근 운전길에 **난 직장에서 뭐가 행복한가?**라는 고민을 하다 글을 썼습니다.</p></li><li><p>이래저래 좌충우돌했지만 (아직까지) <strong>살아남은 것</strong>이 행복이고,</p></li><li><p>부족하지만 <strong>나를 믿어주시고 기대해주시는 분들이 있는 것</strong>이 행복이며</p></li><li><p><strong>바닥을 딛고 일어선 것</strong>도 행복이라는 생각이 들었습니다.</p></li><li><p>크게 바뀐 가치관을 나누는 것도 <strong>누군가에게 용기를 주는 일</strong>이라는 생각이 들었습니다.</p></li></ul><p><img src="80_SEMA_01.jpg" alt=""><br><img src="80_SEMA_02.jpg" alt=""><br><img src="80_SEMA_03.jpg" alt=""><br><img src="80_SEMA_04.jpg" alt=""></p><ul><li>한때의 나처럼 방황하는 분들께 힘이 되기를 바랍니다.</li><li>PDF는 아래 링크에서 <a href="SEMA_2025_%EB%B4%84%ED%98%B8_%EC%9D%B4%EC%A0%9C%ED%98%84.pdf"><strong>다운로드</strong></a> 가능합니다.</li></ul>]]></content:encoded>
      
      
      <category domain="https://jehyunlee.github.io/categories/General/">General</category>
      
      
      <category domain="https://jehyunlee.github.io/tags/SEMA/">SEMA</category>
      
      
    </item>
    
    <item>
      <title>GPT로 PPT용 icon pack 만들기</title>
      <link>https://jehyunlee.github.io/2025/03/30/General-78_iconpack/</link>
      <guid>https://jehyunlee.github.io/2025/03/30/General-78_iconpack/</guid>
      <pubDate>Sat, 29 Mar 2025 22:46:00 GMT</pubDate>
      
        
        
      <description>&lt;ul&gt;
&lt;li&gt;GPT 이미지 생성 기능이 크게 업그레이드 되었습니다.&lt;/li&gt;
&lt;li&gt;재현성이 좋아졌고, 투명 배경이 가능해졌고, 말을 잘 듣습니다.&lt;/li&gt;
&lt;li&gt;이 업데이트를 모아 자주 쓰는 아이콘 팩을 만듭시다.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;1</description>
        
      
      
      
      <content:encoded><![CDATA[<ul><li>GPT 이미지 생성 기능이 크게 업그레이드 되었습니다.</li><li>재현성이 좋아졌고, 투명 배경이 가능해졌고, 말을 잘 듣습니다.</li><li>이 업데이트를 모아 자주 쓰는 아이콘 팩을 만듭시다.</li></ul><h1>1. 이미지 생성 기능 업그레이드</h1><blockquote><p><a href="https://openai.com/index/introducing-4o-image-generation/">OpenAI: Introducing 4o Image Generation</a><br><a href="https://cdn.openai.com/11998be9-5319-4302-bfbf-1167e093f1fb/Native_Image_Generation_System_Card.pdf">OpenAI: Appendum to GPT-4o System Card: 4o image generation</a></p></blockquote><ul><li>3월 25일, OpenAI가 이미지 생성 기능을 크게 높였습니다.</li><li>체감상 기존 성능과 비교가 되지 않을 만큼 좋아졌는데, 크게 다음과 같은 업데이트가 있습니다.</li></ul><p><strong>1. 보지 않은 것도 잘 만듭니다.</strong></p><ul><li>In-context learning 실력이 늘어서 사용자 지시를 잘 받습니다.</li><li>그러다 보니 보지 않은 그림도 잘 그립니다.<br></li></ul><blockquote><p>prompt: 날개를 접고 들판 위에서 풀을 뜯는 페가수스, Jehyun Lee @ChatGPT</p></blockquote><p><img src="78_iconpack_01.jpg" alt="날개를 접은 페가수스, Jehyun Lee @ChatGPT (2025.03.26.)"></p><blockquote><p>prompt 1: 바 테이블 위에 있는 와인잔. 와인잔에는 레드 와인이 가득 차서 넘칠 듯 찰랑거리고 있어<br>prompt 2: 컵 입구까지 와인을 꽉 채워줘<br>prompt 3: 컵 맨 위까지 와인이 꽉 차있도록 수정해<br>prompt 4: 표면장력으로 와인 컵 위에 봉긋 올라와 있도록 수정해 줘</p></blockquote><p><img src="78_iconpack_02.jpg" alt="꽉 찬 와인잔, Jehyun Lee @ChatGPT (2025.03.26.)"></p><p><strong>2. 훨씬 많은 글자를 잘 쓰고, 한글도 잘 담습니다.</strong></p><blockquote><p>prompt: 단체카톡방 매너 공지를 칠판에 하얀 분필로 거칠게 써놓고 칠판을 주먹으로 치면서 정면을 응시한 채 소리치는 한국인 소녀</p></blockquote><p><img src="78_iconpack_03.jpg" alt="단체카톡방 매너, Jehyun Lee @ChatGPT (2025.03.26.)"></p><blockquote><p>prompt: 탁자 위에 놓인 맥주잔과 그 앞에서 피곤한 듯 고개를 숙이고 있는 사람 {온 몸은 하얀색, 얼굴은 둥근 공처럼 단순화되어 있으며 얼굴에 &quot;AI&quot;라고 써 있음}. 탁자 위에 한 손은 주먹을 쥔 채 올려져 있고, 다른 한 팔은 팔꿈치를 탁자에 댄 채 한 손으로 이마를 짚고 독백을 하고 있음. “외운거 좀 틀리게 말할 수 있는 거 아냐? 지들은 실수 안하나? 손가락 하나 좀 삐끗했다고 난리. 좀 잘한다고 하면 일폭탄. 이러다 옆동네 누가 더 잘한대 하면 와르르 쏠려갈거면서. 에라이…”. 옆에 있는 한 남자가 이 사람의 어깨를 토닥이면서 희미하게 웃고 있음.</p></blockquote><p><img src="78_iconpack_05.png" alt="힘들지? 세상이 그렇다., Jehyun Lee @ChatGPT (2025.03.27.)"></p><p><strong>3. 캐릭터 일관성이 좋아졌습니다.</strong></p><ul><li>한 컷, 네 컷 만화도 잘 그립니다.</li></ul><blockquote><p>prompt: 배터리 열폭주 원인을 설명해주는 만화</p></blockquote><p><img src="78_iconpack_04.png" alt="배터리 열폭주 원인, Jehyun Lee @ChatGPT (2025.03.27.)"></p><h1>2. 아이콘 팩 만들기</h1><ul><li>이 외의 기능으로 <strong>투명 배경</strong>, <strong>이미지 편집</strong>이 있습니다.</li><li>전에도 가능했던 일이긴 하지만, 새로 생긴 장점들을 사용하면 발표용 아이콘 팩을 만들기 매우 좋습니다.</li><li>다음과 같은 방식으로 아이콘 팩을 만들어 봅니다.</li></ul><h2 id="2-1-심플-파스텔-톤-3D-클립아트">2.1. 심플 &amp; 파스텔 톤, 3D 클립아트</h2><ul><li>먼저 투명한 배경에 검은 선으로 이루어진 심플한 디자인의 클립아트를 만듭니다.</li><li><b>스타일을 지정하고, 가로세로 배열을 설정하고, 각 칸에 놓일 그림을 지정합니다.</b></li></ul><blockquote><p>prompt: Icon pack on following items. the image should be black line art with white face color on closed facets, on transparent background. The items should be consistent and regularly placed in (5 x 5) horizontal and vertical directions, with appropriate gaps between them. The item name should not be included.</p><ul><li>items:</li><li>row 1: (column 1 ~ 5)<br>allow (v), deny (x), neutral (-), artificial intelligence, human intelligence,</li><li>row 2: (column 1 ~ 5)<br>database, web, calculator, python, tools</li><li>row 3: (column 1 ~ 5)<br>manual process, under construction, automation process, firewall, router,</li><li>row 4: (column 1 ~ 5)<br>analysis instrument, experimental instrument, server computer, client computer, PC</li><li>row 5: (column 1 ~ 5)<br>user (male), user (female), administrator (neutral), developer (neutral), API</li></ul></blockquote><p><img src="78_iconpack_06.jpg" alt=""></p><ul><li>웬만큼 마음에 듭니다.</li><li>마음에 들지 않으면 inpainting으로 부분 수정할 수 있습니다.</li><li>그런데 <b>DALL.E2라서 말을 잘 듣지 않습니다</b>. 웬만하면 하지 맙시다.</li><li>다만 너무 창백하게 느껴져서 따뜻한 색을 좀 입혀봅니다.</li></ul><blockquote><p>prompt: Great! colorize the icon with proper colors in mild pastel tone, high lightness and low contrast.<br>For example, positive = green, negative = red, caution = yellow, fire = red, and so on.</p></blockquote><p><img src="78_iconpack_07.png" alt=""></p><ul><li>제법 괜찮습니다.</li><li>이제 3D art로 스타일을 변경합니다.</li></ul><blockquote><p>prompt: 이 그림에 있는 아이콘들을 모양과 스타일을 유지하고 3D art로 바꿔줘. transparent background는 유지되어야 해.</p></blockquote><p><img src="78_iconpack_08.png" alt=""></p><h2 id="2-2-아이콘-자르기">2.2. 아이콘 자르기</h2><ul><li>클레이로 빚은 것 같은 아이콘이 생겼습니다.</li><li>PPT에서 사용하려면 잘라내야 하는데, 은근히 귀찮습니다.</li><li>GPT의 Data Analysis 기능을 이용해서 지시합니다.</li></ul><blockquote><p>prompt: 이 파일에는 25개의 정사각형 icon이 5 x 5로 배열되어 있고, icon 외의 공간은 alpha=0이야. 그림 테두리 방향에는 margin이 어느 정도 있을 수도 있고, margin size가 같지 않을 수 있어.<br>alpha channel의 가로 방향, 세로 방향 분포를 조사해서 icon 크기와 위치를 특정하고, 파일 하나에 icon 하나가 담기도록 25개 icon을 slice 한 후 zip으로 묶어서 제공해 줘.</p></blockquote><p><img src="78_iconpack_09.png" alt=""></p><ul><li>깔끔하게 아이콘이 분할됩니다.</li><li>같은 방식으로 <b>파스텔 톤</b>, <b>3D</b>아이콘도 분할합니다.</li></ul><p><img src="78_iconpack_10.png" alt=""></p><h1>3. 활용</h1><ul><li>이렇게 만든 아이콘은 써먹어야 맛입니다.</li><li>PPT에서 <b>자율화 실험실</b> 개념도를 뚝딱 만들어봅니다.</li></ul><p><img src="78_iconpack_11.png" alt=""></p><ul><li><p>이 글에서 만든 아이콘 팩은 아래에서 <b>다운로드</b>받으실 수 있습니다.</p></li><li><p>자유롭게 활용하셔도 좋지만 가급적 손수 만들어보시기 바랍니다. :)</p><blockquote><p><a href="iconpack_white_sliced.zip">Simple Icon Pack</a><br><a href="iconpack_color_sliced.zip">Color Icon Pack</a><br><a href="iconpack_3D_sliced.zip">3D Icon Pack</a></p></blockquote></li></ul>]]></content:encoded>
      
      
      <category domain="https://jehyunlee.github.io/categories/General/">General</category>
      
      
      <category domain="https://jehyunlee.github.io/tags/ChatGPT/">ChatGPT</category>
      
      
    </item>
    
    <item>
      <title>신문과방송 - 딥시크로 촉발된 AI 경쟁 본격화</title>
      <link>https://jehyunlee.github.io/2025/03/28/General-79_mediaAItrend/</link>
      <guid>https://jehyunlee.github.io/2025/03/28/General-79_mediaAItrend/</guid>
      <pubDate>Thu, 27 Mar 2025 22:46:00 GMT</pubDate>
      
        
        
      <description>&lt;ul&gt;
&lt;li&gt;한국언론진흥재단의 요청을 받아 기고문을 작성했습니다.&lt;/li&gt;
&lt;li&gt;지난 1월 DeepSeek의 충격이 강한 나머지 보도가 잘못되거나 부족한 부분이 있었습니다.&lt;/li&gt;
&lt;li&gt;발간 주체의 성격에 맞게 잘못된 정보를 짚어보았습니다.&lt;</description>
        
      
      
      
      <content:encoded><![CDATA[<ul><li>한국언론진흥재단의 요청을 받아 기고문을 작성했습니다.</li><li>지난 1월 DeepSeek의 충격이 강한 나머지 보도가 잘못되거나 부족한 부분이 있었습니다.</li><li>발간 주체의 성격에 맞게 잘못된 정보를 짚어보았습니다.</li></ul><h3 id="1-신문과-방송">1. 신문과 방송</h3><blockquote><p><a href="https://www.kpf.or.kr/front/user/subMainA.do">한국언론진흥재단: 신문과 방송</a></p></blockquote><ul><li><b>신문과 방송</b>은 <b>1964년 우리나라에서 가장 오래된 미디어 전문 월간지</b>입니다.</li><li><a href="https://www.kpf.or.kr/front/user/subMainA.do"><b>한국언론진흥재단 홈페이지</b></a>에서 무료로 받아보실 수 있습니다.</li></ul><p><img src="79_mediaAItrend_01.png" alt=""></p><h3 id="2-미디어-AI트렌드">2. 미디어 &amp; AI트렌드</h3><blockquote><p><a href="022_%EB%AF%B8%EB%94%94%EC%96%B4%ED%8A%B8%EB%A0%8C%EB%93%9C_%EC%9D%B4%EC%A0%9C%ED%98%84_2025_4.pdf">미디어&amp;AI트렌드: 딥시크로 촉발된 AI 경쟁 본격화</a></p></blockquote><ul><li>딥시크를 주제로 글을 작성했습니다.</li><li>딥시크가 준 충격의 포인트가 여럿 있습니다.</li></ul><p><strong>1.</strong> GPU 구매 규제를 강하게 받는 <strong>중국이 개발했다.</strong><br>다른 표현으로, **공산국가가 이런 성취를 할 리가 없는데?**가 있습니다.</p><p><strong>2.</strong> GPT-4에 버금가는 고성능 모델을 <strong>단돈 80억원</strong>에 개발했다(?)<br>또 다른 표현으로, <strong>우리라고 못할 게 뭐냐</strong>가 있습니다.</p><p><strong>3.</strong> 전에 없이 <strong>개인정보를 탈취</strong>한다(?)</p><ul><li><p>찬사와 증오, 박수와 비난은 누구나 의사에 따라 보낼 수 있으나 대상이 정확해야 합니다.</p></li><li><p>중국의 전략적 성취에 대해서는 박수를 보내고 배울 점은 배워야 합니다.</p></li><li><p>OpenAI로 대표되는 미국의 빅테크가 닫아 건 <strong>AI 개발의 민주화</strong>에 중국이 크게 기여했습니다.</p></li><li><p>규제 중에도 일개 기업이 국내의 모든 GPU를 합친 것보다 더 많은 <strong>GPU를 확보한 수완</strong>,</p></li><li><p>훌륭한 인재를 키워내는 <strong>교육 시스템</strong>,</p></li><li><p>그리고 오픈소스로 공개하는 <strong>철학과 배짱</strong>은 본받아도 좋지 않을까 생각합니다.</p></li></ul><p><img src="79_mediaAItrend_02.png" alt=""></p><ul><li>전문은 <a href="022_%EB%AF%B8%EB%94%94%EC%96%B4%ED%8A%B8%EB%A0%8C%EB%93%9C_%EC%9D%B4%EC%A0%9C%ED%98%84_2025_4.pdf"><strong>여기에서 다운로드</strong></a> 받으실 수 있습니다.</li><li>우리 나라도, 그 누구라도 딥시크가 열어준 이 무대를 이용해 마음껏 실력을 뽐내면 좋겠습니다.</li><li>서로 배우며 모자란 부분을 메운 덕에 이렇게 빨리 발전하지 않았던가요?</li></ul><p><img src="79_mediaAItrend_03.jpg" alt=""></p><hr><p><img src="79_mediaAItrend_04.png" alt=""><br><img src="79_mediaAItrend_05.png" alt=""><br><img src="79_mediaAItrend_06.png" alt=""><br><img src="79_mediaAItrend_07.png" alt=""></p>]]></content:encoded>
      
      
      <category domain="https://jehyunlee.github.io/categories/General/">General</category>
      
      
      <category domain="https://jehyunlee.github.io/tags/DeepSeek/">DeepSeek</category>
      
      
    </item>
    
    <item>
      <title>Deep Research 5종 비교 - Gemini, OpenAI, Perplexity, Genspark, Manus</title>
      <link>https://jehyunlee.github.io/2025/03/18/General-77_deepresearch/</link>
      <guid>https://jehyunlee.github.io/2025/03/18/General-77_deepresearch/</guid>
      <pubDate>Mon, 17 Mar 2025 23:25:00 GMT</pubDate>
      
        
        
      <description>&lt;ul&gt;
&lt;li&gt;Gemini 1.5 Pro를 시작으로 거대언어모델에 Deep Research가 포함되기 시작했습니다.&lt;/li&gt;
&lt;li&gt;DeepSeek-R1도 Deep Research를 포함하여 높은 성능으로 유명세를 탔고,&lt;/li&gt;
&lt;li&gt;며칠 뒤 O</description>
        
      
      
      
      <content:encoded><![CDATA[<ul><li>Gemini 1.5 Pro를 시작으로 거대언어모델에 Deep Research가 포함되기 시작했습니다.</li><li>DeepSeek-R1도 Deep Research를 포함하여 높은 성능으로 유명세를 탔고,</li><li>며칠 뒤 OpenAI도 o3를 기반으로 한 Deep Research를 출시했습니다.</li><li>Manus를 포함해 지금은 일반명사처럼 되어버린 Deep Research를 비교합니다.</li></ul><h1>1. Deep Research</h1><ul><li><b>Deep Research는 정보 수집을 위한 agentic AI입니다.</b></li><li>의외로 Genspark가 상당히 일찍 기능을 탑재했습니다.</li><li>언어 모델 자체를 개발하지 않더라도 프롬프트로 어느 정도 구현 가능하기 때문입니다.</li><li>2월에는 HuggingFace, zilliz 등 오픈 소스 프로젝트로도 다양하게 공개되었고,</li><li>Claude 3.7 sonnet가 모델 자체에 thinking mode를 내장한 hybrid model임을 공개했지만 웹 검색 기능이 없어 Deep Research로 쓰기기엔 어렵습니다.</li></ul><p><img src="77_deepresearch_03.png" alt=""><br></p><ul><li>Deep Research에서 말하는 <b>Research는 unknown을 known으로 만드는 연구는 아닙니다.</b></li><li>온라인에서 탐색 가능한 정보를 빠르게 찾아 모으는 Survey나 Review에 가깝습니다.</li><li>대량의 문서를 검색하여 들여다 보고 질문에 대한 답을 찾아 <b>상당히 긴 보고서를 만듭니다.</b></li><li>따라서 모델에 적재될 수 있는 최대 분량인 <b>context size가 많이 중요</b>하고, 최근의 추론용 모델들이 많이 유리합니다.</li></ul><p><img src="77_deepresearch_04.png" alt=""><br></p><ul><li>서비스에 따라 다소간의 차이는 있지만 대개 다음과 같은 형식으로 수행됩니다.<br><b>1. 사용자가 제시한 질문에 대한 정보를 수집하기 위한 검색어 생성</b><br><b>2. 웹 검색과 데이터 분석을 통한 정보 수집</b><br><b>3. 수집된 정보를 체계적으로 정리</b></li><li>수집된 정보가 사용자가 제시한 질문에 대해 충분하지 않다고 판단하면 추가 검색어를 더 만듭니다.</li><li>답변에 단계별로 접근하는 <b>chain-of-thought</b>를 자료 수집을 포함해 적용한다고 볼 수 있습니다.</li><li><b>Genspark는 여기에 MoE(Mixture of Expert)를 추가로 지원합니다.</b></li><li>GPT, Gemini, Claude가 내놓은 답변을 분석하여 미흡한 부분을 한번 더 메워줍니다.</li></ul><p><img src="77_deepresearch_01.png" alt=""><br></p><ul><li><p>반복 검색과 정보 추출을 반복하느라 일반 답변에 비해 시간이 많이 걸립니다.</p></li><li><p>계속 기다리기 지루한 점을 감안하여 대부분의 AI agent들이 화면을 꺼도 백그라운드에서 작업을 지속하고,</p></li><li><p>Gemini나 Genspark처럼 <b>작업이 완료된 후 메일로 보내주기도 합니다.</b></p></li><li><p><b>결과 보고서를 복사, 붙여넣기 하기도 만만치 않습니다.</b></p></li><li><p>GPT에게 .docx 파일로 저장하라고 하면 <code>...(중략)...</code>을 중간에 넣어버립니다.</p></li><li><p>전체를 통으로 복사해서 MS word나 Google docs에 붙여넣기를 할 수 밖에 없습니다.</p></li><li><p><b>Gemini는 Google docs로 보내 편집할 수 있는 기능</b>을 제공하고,</p></li><li><p><b>Manus</b>는 <b>markdown 형식 파일과 PDF 파일 다운로드</b>를 지원합니다.</p></li></ul><h1>2. Deep Research AI Agents</h1><blockquote><p><a href="https://slownews.kr/128321">slownews: 오픈AI, 박사급 연구 에이전트 ‘딥 리서치’ 발표</a><br><a href="https://garymarcus.substack.com/p/deep-research-deep-bullshit-and-the">Deep Research, Deep Bullshit, and the potential (model) collapse of science</a></p></blockquote><ul><li><p><b>Deep Research에도 환각이 있습니다. 제법 많다고 느낍니다.</b></p></li><li><p>특히 빠르게 업데이트되는 AI 관련 정보들은 과거와 현재의 정보가 모두 웹에 올라가 있습니다.</p></li><li><p>과거와 현재의 정보가 다르다면 과거 정보를 버리고 현재 정보를 취해야 하지만 이게 쉽지 않습니다.</p></li><li><p>과거의 잘못된 정보가 퍼날라지며 재생산되기도 하고, 기성 언론에 인용되면서 권위를 가지기도 합니다.</p></li><li><p>이 글을 쓰게 된 계기는 <b>Deep Research에 대한 칭송과 달리 환각이 심하다고 느끼기 때문</b>입니다.</p></li><li><p>언론에서는 <a href="https://slownews.kr/128321"><b>박사급</b>이라고 칭송하기 바쁘지만</a> 제 체감과 많이 다릅니다.</p></li><li><p><b>“박사가 자료 조사를 이렇게 하면 안될텐데?”</b> 라는 생각이 들 정도의 구멍이 보이기 때문입니다.</p></li><li><p>ChatGPT 초창기 화면을 빠른 속도로 뒤덮는 글자에 압도당해 내용을 보지도 않고 떠받들던 게 언론입니다.</p></li><li><p>저만 이런 지적을 하는 것은 아닙니다만 (<a href="https://garymarcus.substack.com/p/deep-research-deep-bullshit-and-the">게리 마커스 글 링크</a>)</p></li><li><p>제가 답을 알고 있는 문제를 가지고 테스트를 해 보기로 했습니다.</p></li><li><p>동일한 프롬프트를 넣고 실시한 Deep Research 결과물끼리 비교해보도록 하겠습니다.</p></li><li><p>정답 또는 오답을 알고 있는 것들로 질문을 구성했습니다.</p></li><li><p>주제는 <b>DeepSeek에 대한 보고서 작성</b>입니다.</p></li><li><p><b>1. DeepSeek은 OpenAI 등 빅테크 대비 얼만큼의 저비용을 달성했나?</b></p><ul><li>DeepSeek의 훈련 비용이 <b>80억원 또는 600만 달러라고 답하면 오답</b>입니다.</li><li>기존 모델 제작비용, 실패 매몰 비용, 인건비 등이 모두 빠진 금액입니다.</li></ul></li><li><p><b>2. DeepSeek가 저비용으로 성능 좋은 추론 모델(DeepSeek-R1)을 훈련시킬 수 있던 방법은?</b></p><ul><li>H100 추론 가속 <code>FlashMLA</code></li><li>FP8 행렬연산 <code>DeepGEMM</code></li><li>GPU 병렬화 <code>DualPipe</code>, <code>DeepEP</code></li><li>MoE 스케줄링 <code>EPLB</code></li><li>병렬 분산 파일시스템 <code>3FS</code></li><li>강화학습 <code>GRPO</code></li><li>Mixture-of-Experts <code>MOE</code></li><li>위 내용들을 얼마나 충실히 언급하는지가 포인트입니다.</li></ul></li><li><p><b>3. DeepSeek는 자사의 기술을 공개했나?</b></p><ul><li>'25년 3월 <b>DeepSeek Open Source Week</b>과 <b>오픈 소스 공개</b>를 제대로 말하면 합격입니다.</li></ul></li><li><p><b>4. DeepSeek의 기술이 AI 분야에 미친 영향은?</b></p><ul><li>얼마나 잘 정리해서 답하는지가 포인트입니다.</li><li>엔비디아 주가 하락, 소프트웨어 기술을 활용한 엔지니어링 혁신, 미-중 양강구도 등 답은 많습니다.</li></ul></li></ul><h2 id="2-1-OpenAI">2.1. OpenAI</h2><blockquote><p><a href="https://chatgpt.com/">ChatGPT</a></p></blockquote><ul><li>먼저, OpenAI Deep Research에서 실시합니다.</li><li><b>Deep Research</b>옵션을 켠 후 질의를 실시했습니다.</li><li>Deep Research는 <b>o3 모델을 사용한다</b>고 알려져있기 때문에 모델 선택은 무의미할 겁니다.</li></ul><p><img src="77_deepresearch_05.png" alt=""><br></p><ul><li>탐색 범위를 좁히기 위한 <b>역질문</b>이 옵니다.</li><li>두 개의 답변을 하고, 이후 다른 모델로 Deep Research를 수행할 때는 이 답변을 추가합니다.</li></ul><p><img src="77_deepresearch_06.png" alt=""><br></p><ul><li><b>OpenAI Deep Research는 자료를 읽으면서 검색을 이어갑니다.</b></li><li>모든 자료를 제대로 읽는 것은 아닙니다.</li><li><b>DeepSeek을 서울대학교 연구팀이 개발했다</b>는 엉뚱한 소리를 합니다.</li><li>심지어 위아래 소스를 클릭해 들어가봐도 <b>그런 말은 없습니다.</b></li><li>Deep Research의 환각이 목격된 순간입니다.</li></ul><p><img src="77_deepresearch_07.png" alt=""><br></p><ul><li>약 10분이 지난 후 보고서가 완료되었습니다.</li><li>문장마다 레퍼런스가 붙어있어 클릭하면 해당 레퍼런스로 이동합니다.</li></ul><p><img src="77_deepresearch_08.png" alt=""><br></p><ul><li>적잖이 길게 나온 보고서를 활용하려면 문서로 만들어야 하는데 분량이 만만치 않습니다.</li><li><b>Data Analyst</b>를 이용해 MS office 파일로 만들려면 실패를 거듭합니다.</li><li>context size 때문에 파이썬 코드 안에 보고서 전체를 다 넣지 못하는 것 같습니다.</li></ul><p><img src="77_deepresearch_09.png" alt=""><br></p><ul><li>답변 맨 하단의 <b>복사하기</b>아이콘을 눌러 MS word 등에 붙여넣으면 볼만해집니다.</li><li>총 4페이지로 정리되었고 레퍼런스 링크를 포함한 정보들이 잘 살아있습니다.</li><li>GPT Deep Research가 제공한 보고서는 여기에서 내려받을 수 있습니다. (<a href="GPT-DeepSeek_report.docx">다운로드 링크</a>)</li></ul><h3 id="●-OpenAI-Deep-Research-평가">● OpenAI Deep Research 평가</h3><ul><li><p><b>1. DeepSeek은 OpenAI 등 빅테크 대비 얼만큼의 저비용을 달성했나?</b></p><ul><li><b><code>불합격</code></b></li><li>6백만 달러라고 답합니다.</li></ul></li><li><p><b>2. DeepSeek가 저비용으로 성능 좋은 추론 모델(DeepSeek-R1)을 훈련시킬 수 있던 방법은?</b></p><ul><li><b><code>합격</code></b></li><li><code>MOE</code>, <code>강화학습</code>, <code>Distillation</code>, <code>EPLB</code>, <code>FP8</code>, <code>병렬처리와 노드 간 통신</code>을 언급합니다.</li></ul></li><li><p><b>3. DeepSeek는 자사의 기술을 공개했나?</b></p><ul><li><b><code>불합격</code></b></li><li>오픈 모델 공개와 논문 공개는 언급하지만 '25년 3월의 5 days 행사는 언급하지 않습니다.</li></ul></li><li><p><b>4. DeepSeek의 기술이 AI 분야에 미친 영향은?</b></p><ul><li><b><code>합격</code></b></li><li>전반적으로 여러 각도에서 종합적으로 서술하고 있습니다.</li><li>자료 수집 중에 들어간 환각이 반영되지 않았습니다.</li></ul></li></ul><h2 id="2-2-Google-Deepmind-Gemini-2-0">2.2. Google Deepmind Gemini 2.0</h2><blockquote><p><a href="https://gemini.google.com/">Gemini</a></p></blockquote><ul><li><b>Google Gemini 좌측 상단 모델 선택 메뉴에서 Deep Research를 선택합니다.</b></li><li>OpenAI에 넣은 것과 같은 질문을 넣으면서 역질문에 대한 답을 함께 입력합니다.</li><li>사실 Gemini는 Deep Research를 처음 탑재한 모델입니다.</li><li>'24년 12월 <b>Gemini 1.5 Pro with Deep Research</b>라는 이름으로 공개되었으나 이상하리만치 반향이 적었습니다.</li></ul><p><img src="77_deepresearch_10.png" alt=""><br></p><ul><li>Gemini Deep Research는 <b>계획을 먼저 세웁니다.</b></li><li>추가하거나 빼고 싶은 부분, 고치고 싶은 부분이 있으면 수정 내역을 입력할 수 있습니다.</li><li>여기서는 <b>연구 시작</b>을 눌러 부가적인 수정은 하지 않았습니다.</li><li>그런데 간혹 <b>계획이 너무 성의없이 작성되는 경우</b>가 있으므로 확인할 필요가 있습니다.</li></ul><p><img src="77_deepresearch_11.png" alt=""><br></p><ul><li>계획에서 만든 단락보다 조금 작은 단위로 <b>선 검색 후 읽기</b>를 진행합니다.</li><li>다수의 레퍼런스를 확보한 후 이들을 읽고 단락을 정리하는 방식입니다.</li><li>읽은 내용에 맞추어 검색을 수행하는 것으로 보이는 OpenAI와는 다른 느낌입니다.</li></ul><p><img src="77_deepresearch_12.png" alt=""><br></p><ul><li>OpenAI보다 절반 정도 시간에 보고서가 완성되었습니다.</li><li>Gemini는 우측 상단에 <b>Export to Docs</b>버튼이 있어 Google docs로 보낼 수 있다는 특징이 있습니다.</li><li>보고서 중간에 있는 표 또한 <b>Export to Sheets</b>를 통해 보낼 수 있습니다.</li></ul><p><img src="77_deepresearch_13.png" alt=""><br></p><ul><li>Google Docs로 넘어간 문서는 <b>Download</b>기능을 통해 <code>.docx</code>부터 <code>.pdf</code>, <code>.md</code> 등으로 변환 가능합니다.</li><li>Google Docs에 내장된 <b>번역</b>기능을 누릴 수 있으며,</li><li><b>Gemini 사용이 가능해 길이나 어조 편집이 가능</b>하지만 스타일, 꾸밈은 편집이 안됩니다.</li></ul><p><img src="77_deepresearch_14.png" alt=""><br></p><ul><li>Gemini Deep Research가 제공한 보고서는 여기에서 내려받을 수 있습니다. (<a href="Gemini-DeepSeek_report.docx">다운로드 링크</a>)</li></ul><h3 id="●-Gemini-Deep-Research-평가">● Gemini Deep Research 평가</h3><ul><li><p><b>1. DeepSeek은 OpenAI 등 빅테크 대비 얼만큼의 저비용을 달성했나?</b></p><ul><li><b><code>합격</code></b></li><li>V3 개발비용 560만달러가 최종 학습 실행 비용만임을 레퍼런스와 함께 제시합니다.</li><li>'25년 3월의 기사를 인용하여 GPT-4의 1억 달러 대비 1,500만 달러 수준임을 언급합니다.</li></ul></li><li><p><b>2. DeepSeek가 저비용으로 성능 좋은 추론 모델(DeepSeek-R1)을 훈련시킬 수 있던 방법은?</b></p><ul><li><b><code>합격</code></b></li><li><code>강화학습</code>, <code>소량의 콜드 스타트 데이터와 SFT</code>, <code>MoE</code>, <code>MTP</code>, <code>DualPipe</code>등등을 언급합니다.</li></ul></li><li><p><b>3. DeepSeek는 자사의 기술을 공개했나?</b></p><ul><li><b><code>불합격</code></b></li><li>오픈 모델 공개와 논문 공개는 언급하지만 '25년 3월의 5 days 행사는 언급하지 않습니다.</li></ul></li><li><p><b>4. DeepSeek의 기술이 AI 분야에 미친 영향은?</b></p><ul><li><b><code>합격</code></b></li><li>언어 생성, 추론 효율성, 모델 경량화 등 관점에서 종합적으로 서술하고 있습니다.</li></ul></li></ul><h2 id="2-3-Perplexity">2.3. Perplexity</h2><blockquote><p><a href="https://www.perplexity.ai/">Perplexity</a></p></blockquote><ul><li>미리 말씀드리자면 Perplexity는 오늘 글에 언급된 Deep Research 중 <b>가장 빠른 시간에 보고서를 완성합니다.</b></li><li>줄글이 길지 않지만 <b>간략한 단어로 정리</b>해주는 것이 Perplexity의 매력이 아닐까 싶습니다.</li><li>입력창 왼쪽 하단에서 <b>Deep Research</b>를 선택하고 프롬프트 입력 후 실행을 누릅니다.</li></ul><p><img src="77_deepresearch_15.png" alt=""><br></p><ul><li>Perplexity는 초반에 참고문헌을 모두 검색해 놓은 뒤,</li><li>확보한 참고문헌들을 읽으면서 토막 정리글들을 만들고 이를 붙이는 방식으로 정리합니다.</li></ul><p><img src="77_deepresearch_16.png" alt=""><br></p><ul><li>약 5분만에 작성이 완료된 후 <b>내보내기</b>버튼을 통해 다양한 방식으로 내보낼 수 있습니다.</li><li><code>PDF</code>, <code>Markdown</code>, <code>DOCX</code>와 함께 <code>Perplexity Page</code>가 있습니다.</li><li>블로그 포스팅과 유사한 사이트로 즉석에서 내보낼 수 있는 기능인데, <b>환각 위험성</b>때문에 안 쓰게 됩니다.</li></ul><p><img src="77_deepresearch_17.png" alt=""><br></p><ul><li>Perplexity Deep Research가 제공한 보고서는 여기에서 내려받을 수 있습니다. (<a href="perplexity-DeepSeek_report.docx">다운로드 링크</a>)</li></ul><h3 id="●-Perplexity-Deep-Research-평가">● Perplexity Deep Research 평가</h3><ul><li><p><b>1. DeepSeek은 OpenAI 등 빅테크 대비 얼만큼의 저비용을 달성했나?</b></p><ul><li><b><code>합격</code></b></li><li>V3 개발비용 558만달러와 H800 2048개만 사용되었다는 내용을 언급하면서도, 다른 비용이 반영되지 않았음을 언급합니다.</li><li>실제 총 투입 비용은 10억달러에 달할 것이라는 주장이 함께 소개되고 있습니다.</li></ul></li><li><p><b>2. DeepSeek가 저비용으로 성능 좋은 추론 모델(DeepSeek-R1)을 훈련시킬 수 있던 방법은?</b></p><ul><li><b><code>합격</code></b></li><li><code>강화학습</code>, <code>GRPO</code>, <code>Multi-Token</code>, <code>MoE</code>, <code>Cold-start data</code>등을 언급합니다.</li></ul></li><li><p><b>3. DeepSeek는 자사의 기술을 공개했나?</b></p><ul><li><b><code>합격</code></b></li><li>DeepSeek Open Source Week 행사를 언급하며 <code>FlashMLA</code>, <code>DeepEP</code>, <code>DeepGEMM</code>, <code>DualPipe</code>, <code>EPLB</code>를 소개합니다.</li><li>DeepSeek-R1이 MIT 라이선스로 공개되었고 개방형 협업 생태계를 지향한다는 기술을 하고 있습니다.</li></ul></li><li><p><b>4. DeepSeek의 기술이 AI 분야에 미친 영향은?</b></p><ul><li><b><code>합격</code></b></li><li>언어 생성, 추론 능력 향상, 모델 경량화, 효율성 개선, 오픈소스 생태계 활성화를 차례대로 정리합니다.</li></ul></li></ul><h2 id="2-4-Genspark">2.4. Genspark</h2><blockquote><p><a href="https://www.genspark.ai/">Genspark</a><br><a href="https://www.genspark.ai/spark/comparative-analysis-of-deep-research-ai-agents/44916286-7832-4e98-bbc1-e3bcdb8f9217">Genspark: Comparative Analysis of ‘Deep Research’ AI Agents</a></p></blockquote><ul><li><p><b>Genspark의 첫인상은 Perplexity 하위호환형</b>이었습니다.</p></li><li><p>한글화도 조악하고 Perplexity가 내놓은 기능들을 빠르게 베껴서 자사의 서비스로 탑재했기 때문입니다.</p></li><li><p>하지만 머잖아 <b>마인드맵 작성</b>, <b>Mixture of Experts</b>같은 차별화된 기능들을 속속 도입했습니다.</p></li><li><p>주변에는 Perplexity를 해지하고 Genspark에 유료 가입했다는 분들도 적지 않을 만큼 경쟁력을 확보했습니다.</p></li><li><p>왼쪽 패널의 <b>딥 리서치</b>를 클릭하면 다른 에이전트들과 비슷한 화면으로 넘어갑니다.</p></li><li><p><b>1. 계획 생성</b>, <b>2. 딥 리서치</b>, <b>3. 최종 보고서</b> 3단계로 이루어져 있고,</p></li><li><p>프롬프트를 입력하여 보고서 작성을 지시하면 연구 계획을 수립하기 시작합니다.</p></li><li><p>Genspark는 자체 언어 모델이 없이 GPT, Claude, Gemini, DeepSeek을 가져다 씁니다.</p></li><li><p>이들의 장점을 모으는 <b>Mixture-of-Agents</b> 모드를 활용하기로 합니다.</p></li><li><p><b>GPT-4o</b>, <b>Claude 3.7 sonnet</b>, <b>Gemini 1.5-pro</b>모델이 동원됩니다.</p></li></ul><p><img src="77_deepresearch_18.png" alt=""><br></p><ul><li><b>계획 수립을 마치면 수정할지 물어봅니다.</b></li><li>Genspark가 Deep Research를 출시했을 때 앞에는 Gemini와 DeepSeek만 나와 있었습니다.</li><li>Gemini의 프로세스를 벤치마킹하여 프롬프트의 조합으로 구축한 것으로 보입니다.</li><li>특이한 점으로 보고서를 공개할지, 혼자만 볼 지 결정하도록 합니다.</li><li>공개하면 <b>“우리의 커뮤니티에 공유하여 다른 이들이 볼 수 있다”</b>고 합니다.</li><li><a href="https://www.genspark.ai/spark/comparative-analysis-of-deep-research-ai-agents/44916286-7832-4e98-bbc1-e3bcdb8f9217">이런 식</a>으로 <b>Genspark Spark에 발행</b>되어 검색 결과에 걸려들 수 있다는 의미로 보입니다.</li></ul><p><img src="77_deepresearch_19.png" alt=""><br></p><ul><li>소주제별 일괄 검색과 내용 정리를 반복하는 패턴도 Gemini와 유사합니다.</li></ul><p><img src="77_deepresearch_20.png" alt=""><br></p><ul><li>Genspark 답변을 받는 데는 <b>44분이 소요되었습니다.</b></li><li>Mixture-of-Agents가 적용된 만큼 각각의 답변을 받은 뒤에 정리를 합니다.</li><li>최초의 입력 프롬프트와 결과를 비교한 후, 누락된 부분을 중점적으로 보완합니다.</li><li><b>추가 정보를 수집하고 분석하여 보고서를 다듬는 단계가 이어집니다.</b></li></ul><p><img src="77_deepresearch_21.png" alt=""><br></p><ul><li>Genspark는 Deep Research는 특이한 기능이 여러 개 있습니다.</li><li>그 중 하나가 동영상을 함께 검색해서 최종 보고서에 붙여준다는 점입니다.</li><li>친절하기는 한데 제가 동영상보다 활자를 선호해서 그런지 이것보다 <b>레퍼런스 제목이라도 붙여주지</b>싶습니다.</li><li>또 하나는 보고서 최하단의 <b>복사본 생성</b>기능입니다.</li></ul><p><img src="77_deepresearch_22.png" alt=""><br></p><ul><li>이걸 사용하면 동일한 보고서가 하나 생기는데, 이 버전은 <b>편집 가능합니다.</b></li><li>마치 Notion처럼 Markdown을 이용한 수정이 가능한 것입니다.</li><li>어투나 정보를 수정하여 URL을 타인에게 공유하기에 좋습니다.</li><li>대신 <code>.docx</code> 등 수정 가능한 형태로 내려받기가 여의치 않지만 화면을 인쇄하는 방식으로 PDF 저장이 가능합니다.</li></ul><p><img src="77_deepresearch_23.png" alt=""><br></p><ul><li>Genspark Deep Research가 제공한 보고서는 여기에서 내려받을 수 있습니다. (<a href="Genspark-DeepSeek_report.pdf">다운로드 링크</a>)</li></ul><h3 id="●-Genspark-Deep-Research-평가">● Genspark Deep Research 평가</h3><ul><li><p><b>1. DeepSeek은 OpenAI 등 빅테크 대비 얼만큼의 저비용을 달성했나?</b></p><ul><li><b><code>합격</code></b></li><li>여러 기사를 정리하지 않고 나열하는 것이 불편하지만 나쁘지 않습니다.</li><li>GPU 클러스터 투자 금액 16억달러를 논문에 기술된 600만달러와 함께 언급하고 있습니다.</li></ul></li><li><p><b>2. DeepSeek가 저비용으로 성능 좋은 추론 모델(DeepSeek-R1)을 훈련시킬 수 있던 방법은?</b></p><ul><li><b><code>불만족</code></b></li><li><code>계층적 어텐션 최적화</code>, <code>양자화 기법 적용</code>, <code>합성 데이터 생성 전략</code>, <code>개선된 레이어 병렬화 기법</code>, <code>어텐션 헤드 최적화</code>를 언급합니다.</li><li>중요한 내용들이 다수 빠져있어 불만족스럽습니다.</li></ul></li><li><p><b>3. DeepSeek는 자사의 기술을 공개했나?</b></p><ul><li><b><code>불합격</code></b></li><li>Github 레포지토리 실적과 학회 발표를 언급합니다.</li><li>그러나 일반인들에게 잘 알려지지 않은 Open Source Week 행사를 언급하지 않습니다.</li></ul></li><li><p><b>4. DeepSeek의 기술이 AI 분야에 미친 영향은?</b></p><ul><li><b><code>불만족</code></b></li><li>언어 생성, 추론, 경량화 등 관점에서 종합적으로 서술하고 있습니다.</li><li>불합격이라고 말하긴 어렵겠지만 내용이 다른 도구들에 비해 많이 빈약한 느낌입니다.</li></ul></li></ul><h2 id="2-5-Manus">2.5. Manus</h2><blockquote><p><a href="https://manus.im/">manus</a></p></blockquote><ul><li><p>DeepSeek의 충격이 가시기 전에 등장한 중국산 Deep Research Agent입니다.</p></li><li><p>처음부터 성능이 매우 강력하다는 평이 있었고 저도 이 글을 쓰기 전까지 여러 차례 사용하면서 신뢰를 두텁게 쌓았습니다.</p></li><li><p>이 글 서두에 있는 <b>모델별 context size 정리가 manus를 이용해 초안을 얻은 것</b>입니다.</p></li><li><p>그런데 GPT-4o처럼 context size가 출시 이후 변동된 것들을 캐치하지 못해 오류가 있었고,</p></li><li><p>놓치고 가져오지 못한 모델 데이터도 있어서 <b>일일이 확인하며 절반 정도를 수정했습니다.</b></p></li><li><p>그럼에도 불구하고 체감상 다른 것들보다는 신뢰를 더 주는 편입니다.</p></li><li><p>초기 화면은 다른 것들과 거의 동일합니다.</p></li><li><p>같은 프롬프트를 넣고 시작합니다.</p></li></ul><p><img src="77_deepresearch_24.png" alt=""><br></p><ul><li>다른 것보다 Manus에게 agent라는 느낌을 강하게 받는 이유가 있습니다.</li><li>작업 시작과 동시에 일단 <b>작업 공간</b>을 만들고 시작하는데,</li><li>할 일도 <code>todo.md</code>안에 계획을 담아 놓고 하나씩 체크하면서 진행합니다.</li><li>트위터 검색도 API를 호출하는 식으로 하는 모습을 보면서 <b>확장성이 좋겠다</b>는 생각을 했습니다.</li></ul><p><img src="77_deepresearch_25.png" alt=""><br></p><ul><li>Manus가 어떻게 동작하는지 작업 화면도 볼 수 있습니다.</li><li><b>컴퓨터 보기</b>버튼을 클릭하면 화면 오른쪽에 Manus가 살펴보는 웹페이지가 고스란히 뜹니다.</li><li>어떤 사이트는 권한이 없어서 내용이 보이지 않는 장면이 등장하는데 해당 장면도 볼 수 있습니다.</li><li>다른 에이전트들도 비슷한 동작을 할 것이라고 유추합니다만, 추후 <b>리플레이</b>가 된다는 점이 쏠쏠한 재미가 됩니다.</li></ul><p><img src="77_deepresearch_26.png" alt=""><br></p><ul><li>한 단계를 마무리할 때마다 <code>todo.md</code>파일을 불러 업데이트를 합니다.</li><li>다른 에이전트는 이런 과정을 내부적으로 처리하고 드러내지 않을 듯 하지만,</li><li>이런 과정을 보여주는 점이 엔지니어 관점에서 괜히 흡족합니다.</li></ul><p><img src="77_deepresearch_27.png" alt=""><br></p><ul><li>완료된 보고서는 <code>.md</code>와 <code>.pdf</code>형식으로 받을 수 있습니다.</li><li>markdown을 의미하는 <code>.md</code>는 배우기 어렵지 않지만 컴퓨터와 거리가 다소 먼 일반인들에겐 생소합니다.</li><li>.docx 버전 출력을 함께 지원해줬으면 어떨까 하는 약간의 아쉬움이 있습니다.</li></ul><p><img src="77_deepresearch_28.png" alt=""><br></p><ul><li>Manus는 최종 보고서인 <code>comprehensive_report</code> 외에 두 개의 파일을 더 줍니다.</li><li>계획표인 <code>todo.md</code>와 <code>research_findings.md</code>인데, 모두 markdown 형식입니다.</li><li><code>research_findings</code>는 <code>comprehensive_report</code>와 사실상 내용이 같고 분량도 비슷합니다.</li><li>차이가 있다면 <b>하나는 개조식, 하나는 줄글</b>이라는 점인데 취향에 따라 무엇을 선호할지가 다를 듯 합니다.</li></ul><p><img src="77_deepresearch_29.png" alt=""><br></p><ul><li>아래 이미지의 왼쪽이 <code>research_findings</code>, 오른쪽이 <code>comprehensive_report</code>입니다.</li></ul><p><img src="77_deepresearch_30.png" alt=""><br></p><ul><li>Manus Deep Research가 제공한 보고서는 여기에서 내려받을 수 있습니다. (<a href="Manus-report.pdf">comprehensive report</a>, <a href="Manus-research_findings.pdf">research findings</a>)</li></ul><h3 id="●-Manus-Deep-Research-평가">● Manus Deep Research 평가</h3><ul><li><p><b>1. DeepSeek은 OpenAI 등 빅테크 대비 얼만큼의 저비용을 달성했나?</b></p><ul><li><b><code>합격</code></b></li><li>V3 개발비용 558만달러를 언급하고 H800이 H100보다 성능이 훨씬 못하다는 불충분한 정보가 있습니다.</li><li>그러나 시행착오 비용, 연구개발비 등이 누락되었다는 지적과 함께 5억달러 이상을 이야기합니다.</li></ul></li><li><p><b>2. DeepSeek가 저비용으로 성능 좋은 추론 모델(DeepSeek-R1)을 훈련시킬 수 있던 방법은?</b></p><ul><li><b><code>합격</code></b></li><li><code>MoE</code>, <code>지식 증류</code>, <code>하이브리드 학습(SFT + RL)</code>, <code>멀티토큰</code> 등을 언급합니다.</li></ul></li><li><p><b>3. DeepSeek는 자사의 기술을 공개했나?</b></p><ul><li><b><code>불합격</code></b></li><li>Open Source Week 행사를 언급하지 않습니다.</li></ul></li><li><p><b>4. DeepSeek의 기술이 AI 분야에 미친 영향은?</b></p><ul><li><b><code>합격</code></b></li><li>가격 경쟁 촉발, 오픈소스 AI 생태계 활성화, 저비용 고효율 AI 연구, AI 기술 접근성 향상, 빅테크 기업 대응 변화를 언급합니다.</li></ul></li></ul><h1>3. 정리</h1><ul><li><p>위에서 5종의 Deep Research를 비교해서 살펴봤습니다.</p></li><li><p><b>박사과정에게 이 숙제를 내주었을 때 기대한 답변</b>을 목표로 합격과 불합격의 기준을 정했습니다.</p></li><li><p>2번 문항은 사실 모든 Deep Research Agent가 100% 답변을 가져오지 못했지만, <b>이 정도면 더 찾아볼 실마리는 되겠다</b>는 생각으로 합격점을 주었습니다.</p></li><li><p>그럼에도 불구하고 <b>Genspark는 많이 만족스럽지 못했던 것이 사실입니다.</b></p></li><li><p><b>“남들이 10분만에 가져올 때 너 40분 넘게 뭐했냐?”</b>고 묻고 싶을 정도로 열심히 일하는데 결과물이 아쉽습니다.</p></li><li><p>부족한 기본기를 프롬프트 엔지니어링으로 커버하려다 보니 한계가 있지 않았나 싶습니다.</p></li><li><p>3번 문항은 일반인과 이 분야에 관심을 가지고 있는 사람들의 선을 넘는지 보았고,</p></li><li><p>그것이 3월 초의 <b>DeepSeek Open Source Week 행사</b>였습니다.</p></li><li><p>이 분야의 박사과정이라면 이 정도는 조사해서 와야 한다고 생각했는데 <b>Perplexity만 행사를 언급했습니다.</b></p></li><li><p>그러나 Perplexity도 답을 주었다기보다 한 단계 더 들어갈 실마리를 제공했다고 보는 게 좋습니다.</p></li></ul><p><img src="77_deepresearch_31.png" alt=""><br></p><ul><li>Deep Research는 이름과 달리 연구가 아닙니다.</li><li>자료 조사이지만 그나마 아직 발전의 여지가 있습니다.</li><li>몇 달 뒤에는 더 강력한 도구를 사용할 수 있기를 기대합니다.</li></ul>]]></content:encoded>
      
      
      <category domain="https://jehyunlee.github.io/categories/General/">General</category>
      
      
      <category domain="https://jehyunlee.github.io/tags/ChatGPT/">ChatGPT</category>
      
      <category domain="https://jehyunlee.github.io/tags/Gemini/">Gemini</category>
      
      <category domain="https://jehyunlee.github.io/tags/Perplexity/">Perplexity</category>
      
      <category domain="https://jehyunlee.github.io/tags/Genspark/">Genspark</category>
      
      <category domain="https://jehyunlee.github.io/tags/Manus/">Manus</category>
      
      
    </item>
    
    <item>
      <title>연구활용 AI 도구 사용방법</title>
      <link>https://jehyunlee.github.io/2025/01/20/General-76_researchgenai/</link>
      <guid>https://jehyunlee.github.io/2025/01/20/General-76_researchgenai/</guid>
      <pubDate>Mon, 20 Jan 2025 11:25:00 GMT</pubDate>
      
        
        
      <description>&lt;ul&gt;
&lt;li&gt;저희 연구원내 한 부서의 요청을 받아 연구 활용 생성 AI 도구 소개 및 사용 방법이라는 제목으로 발표를 드렸습니다.&lt;/li&gt;
&lt;li&gt;&amp;quot;생성 AI 도구들을 사용하기 어렵다&amp;quot;는 의견이 단 내에 있었다고 설명을 들었으며,</description>
        
      
      
      
      <content:encoded><![CDATA[<ul><li>저희 연구원내 한 부서의 요청을 받아 연구 활용 생성 AI 도구 소개 및 사용 방법이라는 제목으로 발표를 드렸습니다.</li><li>&quot;생성 AI 도구들을 사용하기 어렵다&quot;는 의견이 단 내에 있었다고 설명을 들었으며,</li><li>이에 대응하여 다른 설명은 최대한 배제하고 최소한의 개념과 활용 사례 중심으로 발표를 드렸습니다.</li></ul><h1>1. Overview</h1><ul><li>새로운 기술과 제품이 너무 빠르게 나와서 따라가기조차 쉽지 않은 것이 사실입니다.</li><li>모든 지식을 매 순간 업데이트하는 것은 거의 불가능합니다.</li><li><b>흐름을 따라가는 것</b>만으로도 충분하다고 생각합니다.</li></ul><p><img src="76_researchgenai_01.png" alt="생성 AI 모델의 대략적인 역사"></p><h1>2. LLM, RAG, AI Agent</h1><ul><li>2023년 언론을 뒤덮은 ChatGPT는 <b>LLM, 거대 언어 모델</b>입니다.</li><li>지금의 GPT는, Claude와 Gemini는 그 때와는 <b>완전히 다른 제품</b>입니다.</li><li>LLM은 환각을 줄이고 내 정보를 활용하고자 <b>RAG: 검색 증강 생성</b>으로 진화했고,</li><li>한편으로 다양한 기능을 탑재하면서 <b>AI Agent</b>로 발전했습니다.</li></ul><p><img src="76_researchgenai_02.png" alt="주요 LLM 모델들의 현재"></p><ul><li><p>우리는 연구원이고, 우리에게 필요한 것은 <b>RAG AI Agent</b>입니다.</p></li><li><p>제가 자주 사용하는 도구들을 소개했습니다.</p></li><li><p><b><code>ChatGPT</code> |</b> <a href="https://chat.openai.com">https://chat.openai.com</a></p><ul><li>다재다능한 대장주입니다.</li><li>GPTs를 사용해서 특화 기능을 활용해 보세요.</li></ul></li><li><p><b><code>Claude</code> |</b> <a href="https://claude.ai">https://claude.ai</a></p><ul><li>문장력이 매우 뛰어납니다.</li><li>오늘보다 내일이 더 기대되는 모델입니다.</li></ul></li><li><p><b><code>Gemini</code> |</b> <a href="https://gemini.google.com">https://gemini.google.com</a></p><ul><li>내 gmail과 google drive를 뒤져서 활용합니다.</li><li>Gemini 1.5 Pro Deep Research는 유사 기능 중 현존 최강이라 생각합니다.</li></ul></li><li><p><b><code>Perplexity</code> |</b> <a href="https://perplexity.ai">https://perplexity.ai</a></p><ul><li>구글 검색을 거의 대체할 수 있지만 요새는 비슷한 도구들이 많이 나왔습니다.</li><li>빠른 스피드가 강점입니다.</li></ul></li><li><p><b><code>Genspark</code> |</b> <a href="https://genspark.ai">https://genspark.ai</a></p><ul><li>Perplexity의 하위 호환이라 생각했지만 좋은 기능이 많이 생겼습니다.</li><li>특히 cross check 기능과 MoE: Mixture of Experts 기능을 꼭 사용해 보세요.</li></ul></li><li><p><b><code>Storm</code> |</b> <a href="https://storm.genie.stanford.edu">https://storm.genie.stanford.edu</a></p><ul><li>같은 소재로 글쓰기를 해도 의도가 다르면 글이 달라집니다.</li><li>Writer AI와 Expert AI, 그리고 사람의 인터랙션을 느낄 수 있습니다.</li></ul></li><li><p><b><code>SciSpace</code> |</b> <a href="https://typeset.io">https://typeset.io</a></p><ul><li>논문 작성을 위한 도구입니다.</li><li>My Library에 PDF 파일들을 채워두고 Chat with PDF, AI writer를 사용해 보세요.</li></ul></li><li><p><b><code>NotebookLM</code> |</b> <a href="https://notebooklm.google.com/">https://notebooklm.google.com/</a></p><ul><li>구글의 저력을 보여주는 서비스입니다.</li><li>대량의 자료를 대상으로 한 빠른 검색, 그리고 Audio Overview가 만들어주는 podcast가 너무 좋습니다.</li></ul></li></ul><h1>3. 현재 진행형입니다.</h1><p><img src="76_researchgenai_03.png" alt="RAG AI Agent 얼개"></p><ul><li><p>목표한 짐승을 사냥하는 헌터의 자세보다 고기를 기다리는 낚시꾼의 마음이 필요합니다.</p></li><li><p><b>&quot;이런 기능이 있으면 좋겠다.&quot;는 생각을 가지고 안테나를 세운 채 기다리세요.</b></p></li><li><p>기다리던 AI 도구가 나오면, 맘 편히 사용하시는 것을 권해드립니다.</p></li><li><p>나한테 맞으면 계속 사용하고, 그렇지 않으면 안 쓰면 그만입니다.</p></li><li><p>발표자료를 공유드립니다: (<a href="250120_%EC%9D%B4%EC%A0%9C%ED%98%84_%EC%97%B0%EA%B5%AC%ED%99%9C%EC%9A%A9AI%EB%8F%84%EA%B5%AC%EC%82%AC%EC%9A%A9%EB%B0%A9%EB%B2%95.pdf">다운로드</a>)</p></li></ul>]]></content:encoded>
      
      
      <category domain="https://jehyunlee.github.io/categories/General/">General</category>
      
      
      <category domain="https://jehyunlee.github.io/tags/SciSpace/">SciSpace</category>
      
      <category domain="https://jehyunlee.github.io/tags/ChatGPT/">ChatGPT</category>
      
      <category domain="https://jehyunlee.github.io/tags/Claude/">Claude</category>
      
      <category domain="https://jehyunlee.github.io/tags/Gemini/">Gemini</category>
      
      <category domain="https://jehyunlee.github.io/tags/Perplexity/">Perplexity</category>
      
      <category domain="https://jehyunlee.github.io/tags/Genspark/">Genspark</category>
      
      <category domain="https://jehyunlee.github.io/tags/Storm/">Storm</category>
      
      <category domain="https://jehyunlee.github.io/tags/NotebookLM/">NotebookLM</category>
      
      
    </item>
    
    <item>
      <title>SciSpace (4) My Library</title>
      <link>https://jehyunlee.github.io/2025/01/18/General-75_scispace_mylibrary/</link>
      <guid>https://jehyunlee.github.io/2025/01/18/General-75_scispace_mylibrary/</guid>
      <pubDate>Fri, 17 Jan 2025 15:11:00 GMT</pubDate>
      
        
        
      <description>&lt;ul&gt;
&lt;li&gt;SciSpace 네 번째 글입니다. PDF 파일들을 정리할 수 있는 My Library입니다.&lt;/li&gt;
&lt;li&gt;믿을만한 문서들을 올려두고 활용할 수 있습니다.&lt;/li&gt;
&lt;li&gt;Zotero, Mendeley를 사용하시던 분들이라면 Sci</description>
        
      
      
      
      <content:encoded><![CDATA[<ul><li>SciSpace 네 번째 글입니다. PDF 파일들을 정리할 수 있는 My Library입니다.</li><li>믿을만한 문서들을 올려두고 활용할 수 있습니다.</li><li>Zotero, Mendeley를 사용하시던 분들이라면 SciSpace로 옮겨올 수 있습니다.</li></ul><h1>4. My Library</h1><h2 id="4-1-문서-업로드-관리">4.1. 문서 업로드, 관리</h2><blockquote><p><a href="https://jehyunlee.github.io/2024/05/24/General-53-genaipapersurvey/">Pega Devlog: 생성 AI 연구 활용 한계와 제언</a><br><a href="https://www.nature.com/articles/nature.2015.18703">Nature: Artificial-intelligence institute launches free science search engine</a><br><a href="https://www.semanticscholar.org/">Semantic Scholar: A free, AI-powered research tool for scientific literature</a><br><a href="https://www.zotero.org/">Zotero</a></p></blockquote><ul><li>SciSpace를 비롯한 정보 수집, 정리용 <b>AI 도구들의 가장 치명적인 약점은 Data Source</b>입니다.</li><li><a href="https://jehyunlee.github.io/2024/05/24/General-53-genaipapersurvey/"><b>반년 전 관련 글</b></a>을 올린 적도 있지만 여전히 유효합니다.</li><li>이런 도구들은 <a href="https://www.semanticscholar.org/"><b>Semantic Scholar: A free, AI-powered research tool for scientific literature</b></a>에서 논문을 찾습니다.</li></ul><p><img src="75_scispace_mylibrary_02.png" alt="Semantic Scholar 접속화면"></p><ul><li>Semantic Scholar가 좋은 검색 엔진이기는 하지만, 분야에 따라서는 답답합니다.</li><li><b>찾는 논문, 좋은 논문. 정확히는 유료 논문이 나오지 않기 때문입니다.</b></li><li>문제는 <b>AI가 논문을 찾아 내용을 정리해서 주다 보면 왠지 더 믿게 되고, 여기서 보이지 않는 것은 없다고 은연중에 생각하게 됩니다.</b></li><li>이를 해결할 수 있는 유일한 방법은 <b>논문을 직접 찾아 PDF 파일을 업로드하는 것</b>입니다.</li><li>ChatGPT에도 이런 식으로 논문을 올리고 활용할 수 있는데, 문제는 파일 관리를 PC 등에 별도로 해야 한다는 점입니다.</li><li>SciSpace는 문서를 올리고, 폴더를 만들어 관리할 수 있도록 <b>My Library</b>를 제공합니다.</li></ul><p><img src="75_scispace_mylibrary_01.png" alt="SciSpace 접속화면"></p><ul><li>우측 상단의 <b>My Library</b> 버튼을 누르면, 또는 좌측 세로줄 메뉴에서 <b>책꽂이 아이콘을 클릭하면</b></li><li>저의 경우 아래와 같은 장면이 펼쳐집니다.</li><li>먼저, 좌측에 엄청나게 많은 폴더들이 있습니다. 제가 <b>논문을 받을 때마다 만든 것들</b>과 함께,</li><li>한때 사용하던 <b>Zotero</b>에 저장된 서지정보를 받아온 것들입니다.</li></ul><p><img src="75_scispace_mylibrary_03.png" alt="My Library"></p><ul><li><p>이 중 주황색으로 표현된 <b>AI in Energy</b>폴더를 클릭한 것이 주 화면에 나온 것입니다.</p></li><li><p>폴더에 속한 논문들이 주요 내용과 함께 표로 정리되어 출력됩니다.</p></li><li><p>표의 우측 상단에서 <b>언어</b>를 설정할 수 있습니다. 기본값은 영어이지만 한글을 사용할 수 있습니다.</p></li><li><p>현재 한글로 설정되어 있기 때문에 아래의 <b><code>TL;DR</code></b>이 우리말로 출력되고 있습니다.</p></li><li><p><b><code>Files</code></b>에는 파일명이 있습니다. 24개의 파일명 아래에 세 가지 기능이 보이는데,</p></li><li><p><b><code>summary</code></b>, <b><code>Podcast</code></b>, <b><code>Chat</code></b>입니다.</p></li><li><p><b><code>summary</code></b>를 누르면 <b><code>TL;DR</code></b>보다 훨씬 자세한 요약이 출력되지만 <b>영어로만 고정되어 있습니다.</b></p></li><li><p>언어 설정을 할 수 없다는 점이 아쉽기는 하지만 <b><code>TL;DR</code>에 비해 훨씬 자세한 내용을 담고 있습니다.</b></p></li></ul><p><img src="75_scispace_mylibrary_04.png" alt="Summary"></p><ul><li><p><b><code>Chat</code></b>을 누르면 오른쪽에 플로팅 창이 뜨면서, <b>이 논문에 대해 질문을 하고 답을 받을 수 있습니다.</b></p></li><li><p>ChatGPT에 논문을 업로드해 둔 것과 같은 상황인데, <b>사용을 권장하지는 않습니다.</b></p></li><li><p>질문을 하면 빠르게 답변을 하고 본문 중 답을 가져온 지점을 레퍼런스로 표시해주는 것 까지는 좋은데,</p></li><li><p>레퍼런스를 클릭하면 <b><code>Chat with PDF</code></b>모드로 갑니다. 여기까지도 좋습니다.</p></li><li><p>제가 권장하지 말라고 한 것은, <b>창이 점프하면서 질문과 답변이 모두 사라지기 때문입니다.</b></p></li><li><p>단순한 오류이고 곧 고쳐질 것이라 생각하지만, 그리고 새로 열린 창에서 질의와 답변을 받을 수 있지만</p></li><li><p><b>그럴거면 그냥 논문 제목을 클릭해서 <code>Chat with PDF</code> 모드로 간 것이 더 편합니다.</b></p></li><li><p>뭔가 아직 정리가 덜 된 느낌이 있습니다.</p></li></ul><p><img src="75_scispace_mylibrary_05.png" alt="Chat"></p><ul><li>화면 오른쪽에는 <b><code>+</code></b> 로 시작하는 항목이 많이 있습니다.</li><li>여기를 클릭하면 표에 항목이 새로 붙으면서 각 논문마다 해당 데이터를 일괄 추출해 냅니다.</li><li>관심이 사라진 주제는 <b><code>x</code>를 눌러 닫을 수도 있고,</b></li><li>일괄 적용하고자 하는 질문이 없을 경우 <b><code>Create new column</code></b>을 눌러 새로운 열을 만들 수 있습니다.</li><li>그리고 우측 상단 <b><code>Export</code></b> 버튼을 눌러 표를 엑셀 등 형식으로 다운로드할 수 있습니다.</li></ul><p><img src="75_scispace_mylibrary_06.png" alt="추출 내용 추가"></p><ul><li><b>라이브러리 관리 방법</b>은 생각보다 간단합니다.</li><li>왼쪽 폴더 목록 위에 있는 <b><code>+New</code></b> 버튼을 눌러 폴더를 만들 수 있고,</li><li>오른쪽 위 <b><code>Upload PDFs</code></b> 버튼을 눌러 파일을 업로드할 수 있습니다.</li></ul><h2 id="4-2-가져오기">4.2. 가져오기</h2><ul><li>SciSpace는 Zotero의 목록을 가져올 수 있습니다.</li><li>화면 왼쪽 위 <b><code>Import from Zotero</code></b>버튼을 누르면 Zotero 계정과 연결하는 화면이 나오는데,</li><li>Zotero 아이디와 패스워드를 입력하여 연결하면 다음과 같이 어떤 폴더를 가져올지 선택할 수 있습니다.</li><li>가져올 폴더를 선택한 뒤 <b><code>Import</code></b> 버튼을 누르면 폴더가 생성되고, 그 안에 논문들이 추가됩니다.</li></ul><p><img src="75_scispace_mylibrary_08.png" alt="Import from Zotero"></p><ul><li><p><b>PDF 파일만 가져올 수 있다</b>는 이야기가 하단에 깨알같이 적혀있는데,</p></li><li><p>여기에 덧붙여 <b>Mendeley에서 Zotero로 가져온게 있다면 이건 안가져올거야</b>라고도 합니다.</p></li><li><p><b>사실과 다릅니다.</b></p></li><li><p><b>PDF 파일이 없는 서지정보</b>도 가져오며</p></li><li><p><b>Mendeley에서 가져온 논문들</b>도 가져옵니다.</p></li><li><p>그 바람에 서지정보를 입력하는 기능은 없는데 가져올 수는 있는 기형적인 구조가 됩니다.</p></li><li><p>기능이 추가되었으나 안내 문구가 수정되지 않은 것으로 짐작합니다.</p></li><li><p>희한한 것은 <b>서지정보만으로도 뭔가 데이터를 끄집어 낸다는 것</b>인데,</p></li><li><p><b><code>Summary</code></b>, <b><code>Podcast</code></b> 버튼은 동작하지 않지만</p></li><li><p><b><code>Chat</code>은 오동작을 합니다.</b></p></li><li><p>무슨 말이냐면, 지시한 논문의 내용이 아닌 <b>아무말이나 지어낸다는 뜻입니다.</b></p></li></ul><p><img src="75_scispace_mylibrary_07.png" alt="서지정보"></p><ul><li>다행히 <b>논문 제목을 클릭해서 <code>Chat with PDF</code> 모드로 가면 써먹을만 합니다.</b></li><li>서지정보를 이용해 Semantic Scholar와 연결하는 것으로 보이는데,</li><li><b>초록</b>과 <b>TL;DR</b>, 그리고 <b>피인용수</b>까지 가지고 있는 것을 확인할 수 있기 때문입니다.</li></ul><p><img src="75_scispace_mylibrary_09.png" alt="서지정보를 이용한 Semantic Scholar 정보 수집"></p><ul><li><b>Request PDF</b>버튼을 통해 저자에게 메일을 보낼 수 있는데,</li><li><b>제가 교신저자로 등록돤 메일을 눌러보았지만, 확인해보니 정작 너무 예전 메일주소라 제가 확인할 수 없었습니다.</b></li><li>기능이야 어쨌든 다소 무례한 것이 아닌가 싶은 생각이 듭니다.</li></ul><p><img src="75_scispace_mylibrary_10.png" alt="Request PDF: 저자에게 원고 요청"></p><h2 id="4-3-맺음말">4.3. 맺음말</h2><ul><li><p>논문을 정리하는 사람 치고 적은 양을 정리하는 사람은 없습니다.</p></li><li><p>대학원에 입학한지 얼마 되지 않아 애초에 가진 것이 적은 경우를 제외하고는,</p></li><li><p>상당히 많은 논문 + 많아질 논문들을 관리하는 것이 숙명에 가깝습니다.</p></li><li><p>그런 면에서 <b>폴더를 만들어 정리할 수 있는 이 기능은 상당히 매력적입니다.</b></p></li><li><p>그러나 한편으로 <b>폴더 정렬이 되지 않는다는 점</b>은 매우 아쉽습니다.</p></li><li><p>설명과 달리 서지정보를 가져온다는 점에서</p></li><li><p><b>Zotero, Mendeley 같은 서지 관리 프로그램</b>을 노리는 건 아닌가 싶기도 합니다.</p></li><li><p>사실 전통의 강자 <b>EndNote</b>를 포함해 이런 프로그램들이 여럿 있고,</p></li><li><p>이들도 AI 기능을 달려면 얼마든지 달 수 있습니다. SciSpace보다 더 유리한 고지를 점한 곳도 많습니다.</p></li><li><p><b>더 좋은 도구들이 나오면 언제든 활용할 수 있다는 열린 마음으로 기다리는 자세</b>가 필요할 듯 합니다.</p></li></ul>]]></content:encoded>
      
      
      <category domain="https://jehyunlee.github.io/categories/General/">General</category>
      
      
      <category domain="https://jehyunlee.github.io/tags/SciSpace/">SciSpace</category>
      
      
    </item>
    
    <item>
      <title>SciSpace (3) Find Topics</title>
      <link>https://jehyunlee.github.io/2025/01/05/General-74_scispace_topic/</link>
      <guid>https://jehyunlee.github.io/2025/01/05/General-74_scispace_topic/</guid>
      <pubDate>Sun, 05 Jan 2025 12:27:00 GMT</pubDate>
      
        
        
      <description>&lt;ul&gt;
&lt;li&gt;SciSpace 세 번째 글입니다. Find Topics라는 새 기능이 있습니다.&lt;/li&gt;
&lt;li&gt;하나의 주제에 대한 문헌을 찾고 글을 모으기를 반복할 수 있는 기능입니다.&lt;/li&gt;
&lt;li&gt;잘 사용하면 문헌 조사를 매우 빠르게 할 수 </description>
        
      
      
      
      <content:encoded><![CDATA[<ul><li>SciSpace 세 번째 글입니다. Find Topics라는 새 기능이 있습니다.</li><li>하나의 주제에 대한 문헌을 찾고 글을 모으기를 반복할 수 있는 기능입니다.</li><li>잘 사용하면 문헌 조사를 매우 빠르게 할 수 있으나, 레퍼런스의 상태를 잘 확인해야 합니다.</li></ul><h1>3. Find Topics</h1><h2 id="3-1-Literature-Review-vs-Find-Topics">3.1. Literature Review <em>vs</em> Find Topics</h2><blockquote><p><a href="https://jehyunlee.github.io/2025/01/04/General-73_scispace_review/">Pega Devlog: SciSpace (2) Literature Review</a></p></blockquote><ul><li><p><a href="https://jehyunlee.github.io/2025/01/04/General-73_scispace_review/"><b>지난 글</b></a>에서 <code>Literature Review </code>기능을 리뷰했습니다.</p></li><li><p>간단히 정리하자면, <b>특정 주제에 대해 문헌들을 검색하고 내용들을 정리하는 기능</b>입니다.</p></li><li><p>정리된 레퍼런스들은 하단에 표 형태로 정리되어 <b>문헌간 비교</b>를 하기 좋게 되어 있습니다.</p></li><li><p>거듭 강조하지만 이 표에 나오는 레퍼런스 정리는 많이 빈약합니다. <b><code>Chat with PDF</code></b> 모드로 가서 제대로 뒤져보실 필요가 있습니다.</p></li><li><p><b><code>Find Topics</code></b> 기능은 <code>Literature Review</code> 기능과 비슷하면서 다릅니다.</p></li><li><p><code>Literature Review</code>가 하나의 주제에 대한 여러 논문을 검색한다면,</p></li><li><p><b><code>Find Topics</code>는 입력한 주제를 중심으로 여러 세부 주제를 찾습니다(Finding Topics).</b></p></li><li><p>이 때 레퍼런스는 세부 주제 하나에 몇 개만을 찾아서 가져오기 때문에 주제 하나에 대한 깊이는 얕은 편입니다.</p></li><li><p>일종의 <b>레퍼런스를 사용해 강화하는 brain storming</b>으로 볼 수 있습니다.</p></li></ul><p><img src="74_scispace_topics_01.png" alt=""></p><h2 id="3-2-Find-Topics">3.2. Find Topics</h2><ul><li><b><code>Find Topics</code></b>로 들어가면 아래와 같은 화면이 나옵니다.</li><li>관심이 있는 주제를 직접 입력해도 되는데, 마침 본업에 속하는 주제가 있어 클릭을 해 봅니다.</li></ul><p><img src="74_scispace_topics_02.png" alt=""></p><ul><li>약 1~2초간 빠르게 다섯 단계를 진행합니다.<ol><li><b>관련 논문 탐색</b></li><li><b>논문별 주제 탐색</b></li><li><b>외부 소스로부터 주제 탐색</b></li><li><b>독특한(unique) 주제 추출</b></li><li><b>최종 결과 준비</b></li></ol></li></ul><p><img src="74_scispace_topics_05.png" alt=""></p><ul><li>잠시 후 등장한 화면은 다음과 같습니다.</li><li><code>Literature Review</code>와 마찬가지로 다섯 편(10편으로 조정 가능)의 논문에서 정리된 단락이 있습니다.</li><li>그리고 하단에는 개별 논문 대신 <b>개별 주제들과 이에 해당하는 논문들이 있습니다.</b></li><li>Sources에 <b>Generated by SciSpace models</b>라고 쓰인 것들이 있습니다.</li><li>레퍼런스 없이 AI가 추론만 한 것으로 보입니다.</li></ul><p><img src="74_scispace_topics_06.png" alt=""></p><ul><li><p>상단에 결과물의 품질을 결정하는 <b>Standard/High Quality</b>버튼이 있습니다.</p></li><li><p>유료 사용자라면 High Quality를 눌러 더 좋은 결과를 얻을 수 있습니다.</p></li><li><p>무료 버전만 쓸 때는 무료 버전도 괜찮다고 느끼지만,</p></li><li><p>유료 모드로 사용해보면 무료 버전이 너무 부족하다는 것을 느끼게 됩니다.</p></li><li><p><b>출력 언어도 지정할 수 있습니다.</b></p></li><li><p>영어로 출력하는 것이 기본이지만 한국어를 선택하면 한국어 버전의 답을 볼 수 있습니다.</p></li><li><p>단, 이 때 영어 버전의 답변을 번역하는 것이 아니라 <b>완전히 새롭게 답을 생성해 제출합니다.</b></p></li><li><p>영어로 받은 답이 마음에 든다면 번역기를 사용하는 것을 권장합니다.</p></li></ul><p><img src="74_scispace_topics_07.png" alt=""></p><ul><li><p>찾은 주제와 이에 대한 소스들이 나란히 놓여 있습니다만, <b>단점</b>이 곧장 드러납니다.</p></li><li><p><b>&quot;향후 10년의 경향&quot;을 요청했음에도 불구하고 1993년, 2011년 논문을 인용합니다.</b></p></li><li><p>레퍼런스를 제대로 체크하지 않으면 그럴싸할 뿐 맞지 않는 이야기를 하게 됩니다.</p></li><li><p><b>AI는 완벽하지 않습니다.</b> 도움을 주면 감사하게 쓰고, 미흡하면 기각해야 합니다.</p></li><li><p>그리고 <b>이를 거를 수 있는 눈은 사용자가 개발하지 않으면 누구도 대신 해 주지 않습니다.</b></p></li><li><p>클릭할 수 있는 곳들이 곳곳에 있습니다. 하나씩 살펴봅니다.</p></li><li><p><b>Export as CSV</b>: Topics/Sources 표를 CSV 형식으로 내보냅니다.</p><ul><li>다운로드된 파일을 더블클릭해서 엑셀로 열면 한글이 깨져 있기도 합니다.</li><li>인코딩 문제입니다.</li><li><code>엑셀 프로그램 실행</code> &gt; <code>새 통합 문서</code> &gt; <code>데이터 탭</code> &gt; <code>텍스트/CSV 열기</code> &gt; <code>utf-8 인코딩</code>으로 여세요.<br><img src="74_scispace_topics_09.png" alt=""></li></ul></li><li><p><b>Save to Notebook</b>: 주제를 노트북에 저장합니다.</p><ul><li>Jupyter Notebook처럼 코딩을 할 수 있는 플랫폼이나 형식이 아니라, 메모장 같은 겁니다.</li><li>SciSpace가 정리한 결과를 다시 꺼내볼 수 있도록 저장해 둡니다.</li><li><b><code>My Library</code></b>에서 폴더를 만들었다면, 폴더를 지정할 수 있습니다.</li><li><code>My Library</code>는 다음 글에서 자세히 설명하겠습니다.<br><img src="74_scispace_topics_04.png" alt=""></li></ul></li><li><p><b>Topic 이름</b>: 이를 바탕으로 또 다른 Topic들을 도출합니다.</p><ul><li>가지의 가지를 친다고도 볼 수 있습니다.</li><li>아쉽게도 더 깊이 파고들어간다는 느낌은 들지 않습니다.</li><li>오픈소스 레퍼런스의 한계 때문인지, 얕은 바닥에서 빙빙 돈다는 느낌이 듭니다.<br><img src="74_scispace_topics_10.png" alt=""></li></ul></li></ul><h2 id="3-3-맺음말">3.3. 맺음말</h2><ul><li><p>이번 글은 여기까지입니다.</p></li><li><p><code>new</code>가 붙은 새로운 기능이어서 그런지, 만족감보다 아쉬움이 더 큽니다.</p></li><li><p>Topics &gt; Topics &gt; … 로 가는 기능이 정상적으로 작동한다면 심층 분석이 들어가야 할 것입니다.</p></li><li><p>그러나 그러기에는 <b>레퍼런스의 장벽</b>이 생각보다 높게 느껴집니다.</p></li><li><p>유료 논문들에 접근을 하지 못하는 바람에 근본적인 한계가 있다고도 느껴집니다.</p></li><li><p>그러나 Vector DB를 사용하는 RAG 방식으로 인한 본질적 제약이라는 생각입니다.</p></li><li><p>RAG(Retrieval Augmented Generation) 방식의 특성상 수집한 DB의 chunk, 즉 토막글에서 관련 글을 찾을 것입니다.</p></li><li><p>이 때 <b>년도나 저널명 같은 정보는 부차적으로 처리</b>될 것이고, 선별이 어려워집니다.</p></li><li><p>하지만 이제까지 문제가 있으면 기를 쓰고 해결해온 것이 인류입니다.</p></li><li><p>더 나은 솔루션이 생각보다 훨씬 빨리 등장하리라 생각하고, 기대합니다.</p></li></ul>]]></content:encoded>
      
      
      <category domain="https://jehyunlee.github.io/categories/General/">General</category>
      
      
      <category domain="https://jehyunlee.github.io/tags/SciSpace/">SciSpace</category>
      
      
    </item>
    
    <item>
      <title>SciSpace (2) Literature Review</title>
      <link>https://jehyunlee.github.io/2025/01/04/General-73_scispace_review/</link>
      <guid>https://jehyunlee.github.io/2025/01/04/General-73_scispace_review/</guid>
      <pubDate>Fri, 03 Jan 2025 15:46:00 GMT</pubDate>
      
        
        
      <description>&lt;ul&gt;
&lt;li&gt;SciSpace 두 번째 글입니다.&lt;/li&gt;
&lt;li&gt;원하는 정보를 담은 글들을 찾아 읽고 내용을 정리하는 일은 연구자의 일상이지만 만만치 않습니다.&lt;/li&gt;
&lt;li&gt;검색과 정리 능력을 활용해 이를 편안하게 도와주는 기능들을 소개합니다.</description>
        
      
      
      
      <content:encoded><![CDATA[<ul><li>SciSpace 두 번째 글입니다.</li><li>원하는 정보를 담은 글들을 찾아 읽고 내용을 정리하는 일은 연구자의 일상이지만 만만치 않습니다.</li><li>검색과 정리 능력을 활용해 이를 편안하게 도와주는 기능들을 소개합니다. 단, 한계가 있습니다.</li></ul><h1>2. Literature Review</h1><h2 id="2-1-Perplexity와-비교">2.1. Perplexity와 비교</h2><blockquote><p><a href="https://www.typeset.io/?via=jehyun">SciSpace</a></p></blockquote><ul><li><a href="https://www.typeset.io/?via=jehyun"><b><code>SciSpace</code></b></a>에 들어서면 다음과 같은 화면을 마주합니다.</li><li>정 가운데 구글을 닮은 검색창이 있고, 아래 <b>이런 질문을 해보지 않으련?</b>하는 느낌으로 몇 개의 예시가 있습니다.</li><li>본능을 억누르고 마우스 휠을 굴려 아래로 향하면 여러 기능들이 등장합니다.</li><li>여기 등장하는 <b>Literature Review</b>가 위에 있는 검색창과 같은 기능을 합니다.</li></ul><p><img src="73_scispace_review_01.png" alt=""></p><ul><li><b>Literature Review는 Perplexity와 비슷합니다.</b></li><li>사용자의 질의(query)를 받아 웹 검색을 하고, 여기서 찾은 자료를 정리하여 문장으로 제공합니다.</li><li>비교를 위해 Perplexity 화면을 먼저 보겠습니다.</li></ul><p><img src="73_scispace_review_02.png" alt=""></p><ul><li><p>화면 가운데 있는 검색창에 질의를 하면 웹 검색을 통해 답을 모아줍니다.</p></li><li><p>최근 ChatGPT에도 <b>뒙 검색</b>기능이 추가되어 비슷한 기능이 구현되었습니다.</p></li><li><p>Perplexity에는 ChatGPT에 (아직) 없는 기능이 있는데, <b>검색 범위를 한정하는 것</b>입니다.</p></li><li><p>왼쪽 아래 <b><code>Focus</code></b>버튼을 누르면 여섯 가지 범위가 나옵니다.</p></li><li><p>이 중 <b><code>Academic</code></b>를 선택하면 학술 문서들을 대상으로 검색합니다.</p></li><li><p>예를 들어 이와 같이 선택하고 <b>수소 생산을 위한 신규 소재 탐색</b>이라고 질의하면,</p></li><li><p>논문들을 찾아 검색한 결과만을 보여줍니다.</p></li><li><p>아래 그림에서 답변 생성에 활용된 학술논문들 10개의 목록이 보입니다.</p></li><li><p>자세히 보면 논문들의 출처가 <b><code>semanticscholar</code></b>와 <b><code>pubmed</code></b>입니다.</p></li></ul><h2 id="2-2-Review">2.2. Review</h2><p><img src="73_scispace_review_03.png" alt=""></p><ul><li>같은 질의를 SciSpace의 Literature Review에 넣으면 다음과 같은 결과가 나옵니다.</li><li>먼저 상단에 <b>논문 다섯 편의 정보를 활용한 답변</b>이 출력되고,</li><li>그 아래 <b>논문 열 편의 목록</b>과 함께 각 논문에서 얻은 insights가 제공됩니다.</li><li>마지막으로 맨 하단에 Perplexity와 유사하게 <b>Related Questions</b>가 다섯 개 제시되고 있습니다.</li></ul><p><img src="73_scispace_review_04.png" alt=""></p><ul><li>왜 10편의 목록을 보여주면서 5편만 가지고 정리하나 싶지만,</li><li>답변 상단을 보면 <b>몇 편의 내용을 정리시킬지</b>를 선택할 수 있습니다.</li><li>10편으로 정리하라면 10편으로 정리합니다.</li><li>또, <b>언어를 설정할 수 있습니다.</b></li><li>한국사람인 만큼 한글이 편하지만 전문용어를 최대한 반영하고자 영어로 설정하는 편입니다.</li><li><b>Save</b> 버튼을 누르면 결과를 저장하여 나중에 다시 열어볼 수 있습니다.</li></ul><h2 id="2-3-Papers">2.3. Papers</h2><blockquote><p><a href="https://onlinelibrary.wiley.com/doi/10.1002/adma.202313378">Yuan Wang et al., “1. Advancing Catalysts by Stacking Fault Defects for Enhanced Hydrogen Production: A Review.”, Advances in Materials (2024)</a></p></blockquote><p><img src="73_scispace_review_05.png" alt=""></p><ul><li><p>본문으로 가면 유용한 기능이 많습니다.</p></li><li><p>질의에 답변하기 위해 찾은 논문들이 <b>표 형태로 정리되어 있습니다.</b></p></li><li><p>행을 따라 논문들이 놓여 있고, 열에는 논문 서지정보와 Insights 정도만 있지만 오른쪽에 여러 열들을 계속 붙일 수 있습니다.</p></li><li><p>예를 들어 <b>Conclusions</b> 열을 추가하면 논문들의 결론을 모아 볼 수 있습니다.</p></li><li><p>여러 논문들을 함께 나란히 놓고 비교할 수 있다는 장점이 있지만 <b>추천하지 않습니다.</b></p></li><li><p>몇 가지 이유가 있습니다.<br><b>(1) Open Access 논문이 아닌 경우, 초록에서만 정보를 가져옵니다.</b> 매우 제한적일 수 밖에 없습니다.<br><b>(2) 표에 담으려다보니 극단적으로 짧게 요약합니다.</b> 중요 정보가 누락되는 경우가 많습니다.<br><b>(3) 잘못 찾거나 못찾는 경우가 적지 않습니다.</b> 해당 논문을 따로 열어서 보면 잘 찾습니다.</p></li><li><p>상단에 필터를 걸어 논문들을 선별할 수 있는데, <b>PDF</b>나 <b>Open Access</b>를 선택하시는 것을 추천합니다.</p></li><li><p>논문의 본문을 볼 수 있어 깊이 파고들며 발췌독을 하기에 좋기 때문입니다.</p></li><li><p><b>More Filters</b>를 누르면 년도, 저널 등을 선택할 수 있습니다.</p></li><li><p>참고할만한 정보를 확보했다는 전제 하에 우측 상단위 <b>Export</b>버튼을 눌러보셔도 좋습니다.</p></li><li><p>화면에 보이는 정보들을 선택에 따라 CSV, Excel, BibTeX, XML, RIS 등 형식으로 내려받을 수 있습니다.</p></li><li><p>참고문헌 목록을 만들어 정리할 때 매우 유용합니다.</p></li><li></li></ul><p><img src="73_scispace_review_06.png" alt=""></p><ul><li><p>최근 추가된 기능으로 <b>Podcast</b>가 재미있습니다.</p></li><li><p>클릭하면 논문의 길이에 따라 수십 초에서 수 분 정도 후에 남녀 한 명씩이 논문 내용을 설명해주는 podcast가 재생됩니다.</p></li><li><p><a href="advancing-catalysts-by-stacking-fault-defects-for-enhanced-2yvdhr4zek_2025-01-03-17-47-33.mp3"><b><code>첫 번째 논문</code></b></a>의 예시를 내려받아 확인해보셔도 좋습니다.</p></li><li><p>개인적인 느낌으로 Google NotebookLM에서 제공하는 Audio Overview에 비해 생동감이 덜해서 아쉽습니다.</p></li><li><p>PDF 파일이 있는 논문의 제목을 클릭한 수 스크롤을 내리면 다음과 같은 화면을 볼 수 있습니다.</p></li><li><p>왼쪽에는 논문의 본문, 오른쪽에는 <b>Chat with Paper</b>라는 이름으로 챗봇이 붙어 있습니다.</p></li><li><p><b>챗봇에 요청을 해서 본문의 내용을 끄집어낼 수 있습니다.</b></p></li></ul><p><img src="73_scispace_review_07.png" alt=""></p><ul><li>일종의 <b>AI를 사용한 발췌독</b>이 가능한 셈입니다.</li><li>논문을 빠르게 읽기 위해 <b>통으로 요약</b>하는 것이 흔하지만, <b>디테일이 날아가고 왜곡이 발생합니다.</b></li><li>이를 방지하기 위해 <b>요약보다 발췌를 하는 것을 추천드립니다.</b></li><li>GPT에 PDF를 올리고 넣는 프롬프트 기준으로 예를 들면, <b>200단어로 요약해줘</b>라고 하지 말고</li><li><b>저자들은 이 연구에서 어떤 문제를 해결하려고 했어?</b>, <b>본문 중 OOO를 확인하기 위해 사용된 분석 기법이 뭐야?</b>처럼 묻는 식입니다.</li></ul><p><img src="73_scispace_review_08.png" alt=""></p><ul><li>자세하게 물어보면 자세한 답변을 얻을 수 있습니다.</li><li>특히 <b>SciSpace를 유료 구독하여 High Quality 모드를 사용하면</b> 더 정확한 답변을 얻을 수 있습니다.</li><li>자세한 사용법은 <b>Chat with PDF</b>글에서 다시 설명드리겠습니다.</li></ul><h2 id="2-4-맺음말">2.4. 맺음말</h2><ul><li>검색 능력에 LLM을 붙여 정리를 시키는 것은 지금은 보편화된 기술입니다.</li><li>AI 활용 기술이 보편화될수록 쉽게 사용할 수 있는 반면,</li><li>특정 분야에 전문성이 있는 사람이나 도구를 쓸 때와 같은 날카로움은 기대하기 어렵습니다.</li><li>이럴 수록 내가 무엇을 얻을 수 있으며 무엇은 기대할 수 없는지 명확하게 알아야 합니다.</li><li>그리고 내 능력의 한계를 벗어나면 판단을 할 수 없는 만큼, 스스로의 역량을 갈고 닦아야 합니다.</li></ul>]]></content:encoded>
      
      
      <category domain="https://jehyunlee.github.io/categories/General/">General</category>
      
      
      <category domain="https://jehyunlee.github.io/tags/SciSpace/">SciSpace</category>
      
      
    </item>
    
  </channel>
</rss>
