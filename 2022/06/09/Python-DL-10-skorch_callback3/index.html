<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8" />

    

    
    <title>skorch callbacks (3) ML Pipeline | Pega Devlog</title>
    
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <meta name="keywords" content="pytorch,sklearn,pipeline,neural network,callback,visualization" />
    
    <meta name="description" content="PyTorch는 현재 가장 인기있는 딥러닝 라이브러리 중 하나입니다. 학습 세부 사항을 지정하기 위해 Callback으로 다양한 기능을 지원합니다. skorch는 PyTorch를 scikit-learn과 함께 사용할 수 있게 해 줍니다. skorch도 PyTorch callback을 이용할 수 있습니다.   글이 길어 세 개로 나눕니다.  세 번째로, s">
<meta property="og:type" content="article">
<meta property="og:title" content="skorch callbacks (3) ML Pipeline">
<meta property="og:url" content="https://jehyunlee.github.io/2022/06/09/Python-DL-10-skorch_callback3/index.html">
<meta property="og:site_name" content="Pega Devlog">
<meta property="og:description" content="PyTorch는 현재 가장 인기있는 딥러닝 라이브러리 중 하나입니다. 학습 세부 사항을 지정하기 위해 Callback으로 다양한 기능을 지원합니다. skorch는 PyTorch를 scikit-learn과 함께 사용할 수 있게 해 줍니다. skorch도 PyTorch callback을 이용할 수 있습니다.   글이 길어 세 개로 나눕니다.  세 번째로, s">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://jehyunlee.github.io/thumbnails/Python-DL/10_sc_00.png">
<meta property="article:published_time" content="2022-06-09T10:50:00.000Z">
<meta property="article:modified_time" content="2022-06-10T00:21:14.988Z">
<meta property="article:author" content="Jehyun Lee">
<meta property="article:tag" content="pytorch">
<meta property="article:tag" content="sklearn">
<meta property="article:tag" content="pipeline">
<meta property="article:tag" content="neural network">
<meta property="article:tag" content="callback">
<meta property="article:tag" content="visualization">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://jehyunlee.github.io/thumbnails/Python-DL/10_sc_00.png">
    
	<link rel="canonical" href="https://jehyunlee.github.io/2022/06/09/python-dl-10-skorch_callback3/"/>

    
        <link rel="alternate" href="https://jehyunlee.github.io/rss2.xml" title="Pega Devlog" type="application/atom+xml" />
    

    
        <link rel="icon" href="/images/favicon-32x32.png" />
    

    
<link rel="stylesheet" href="/libs/font-awesome/css/font-awesome.min.css">

    
<link rel="stylesheet" href="/libs/titillium-web/styles.css">

    
<link rel="stylesheet" href="/libs/source-code-pro/styles.css">


    
<link rel="stylesheet" href="/css/style.css">


    
<script src="/libs/jquery/3.5.0/jquery.min.js"></script>

    
    
        
<link rel="stylesheet" href="/libs/lightgallery/css/lightgallery.min.css">

    
    
        
<link rel="stylesheet" href="/libs/justified-gallery/justifiedGallery.min.css">

    
    
        <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-155262264-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-155262264-1');
</script>
<!-- End Google Analytics -->

    
    
    


<meta name="generator" content="Hexo 5.4.0"></head>

<body>
    <div id="wrap">
        <header id="header">
    <div id="header-outer" class="outer">
        <div class="container">
            <div class="container-inner">
                <div id="header-title">
                    <h1 class="logo-wrap">
                        <a href="/" class="logo"></a>
                    </h1>
                    
                </div>
                <div id="header-inner" class="nav-container">
                    <a id="main-nav-toggle" class="nav-icon fa fa-bars"></a>
                    <div class="nav-container-inner">
                        <ul id="main-nav">
                            
                                <li class="main-nav-list-item" >
                                    <a class="main-nav-list-link" href="/">Home</a>
                                </li>
                            
                                    <ul class="main-nav-list"><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/GIS/">GIS</a><ul class="main-nav-list-child"><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/GIS/Python/">Python</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/GIS/QGIS/">QGIS</a></li></ul></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/ImageJ/">ImageJ</a><ul class="main-nav-list-child"><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/ImageJ/Cookbook/">Cookbook</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/ImageJ/Tutorial/">Tutorial</a></li></ul></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Python/">Python</a><ul class="main-nav-list-child"><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Python/Data-Science/">Data Science</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Python/Deep-Learning/">Deep Learning</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Python/General/">General</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Python/Physics/">Physics</a></li></ul></li></ul>
                                
                                <li class="main-nav-list-item" >
                                    <a class="main-nav-list-link" href="/about/index.html">About</a>
                                </li>
                            
                        </ul>
                        <nav id="sub-nav">
                            <div id="search-form-wrap">

    <form class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="Search" />
        <button type="submit" class="search-form-submit"></button>
    </form>
    <div class="ins-search">
    <div class="ins-search-mask"></div>
    <div class="ins-search-container">
        <div class="ins-input-wrapper">
            <input type="text" class="ins-search-input" placeholder="Type something..." />
            <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: 'Posts',
            PAGES: 'Pages',
            CATEGORIES: 'Categories',
            TAGS: 'Tags',
            UNTITLED: '(Untitled)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>

<script src="/js/insight.js"></script>


</div>
                        </nav>
                    </div>
                </div>
            </div>
        </div>
    </div>
</header>

        <div class="container">
            <div class="main-body container-inner">
                <div class="main-body-inner">
                    <section id="main">
                        <div class="main-body-header">
    <h1 class="header">
    
    <a class="page-title-link" href="/categories/Python/">Python</a><i class="icon fa fa-angle-right"></i><a class="page-title-link" href="/categories/Python/Deep-Learning/">Deep Learning</a>
    </h1>
</div>

                        <div class="main-body-content">
                            <article id="post-Python-DL-10-skorch_callback3" class="article article-single article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
            <header class="article-header">
                
    
        <h1 class="article-title" itemprop="name">
        skorch callbacks (3) ML Pipeline
        </h1>
    

            </header>
        
        
            <div class="article-meta">
                
  <div class="article-date">
    <i class="fa fa-calendar"></i>
    <a href="/2022/06/09/Python-DL-10-skorch_callback3/" class="article-date">
       <time datetime="2022-06-09T10:50:00.000Z" itemprop="datePublished">2022-06-09</time>
    </a>
  </div>


<div class="article-date">
  <i class="fa fa-calendar-plus-o"></i>
  <a href="/2022/06/09/Python-DL-10-skorch_callback3/" class="article-date">
     <time datetime="2022-06-10T00:21:14.988Z" itemprop="dateModified">2022-06-10</time>
  </a>
</div>


                

                
    <div class="article-tag">
        <i class="fa fa-tag"></i>
        <a class="tag-link-link" href="/tags/callback/" rel="tag">callback</a>, <a class="tag-link-link" href="/tags/neural-network/" rel="tag">neural network</a>, <a class="tag-link-link" href="/tags/pipeline/" rel="tag">pipeline</a>, <a class="tag-link-link" href="/tags/pytorch/" rel="tag">pytorch</a>, <a class="tag-link-link" href="/tags/sklearn/" rel="tag">sklearn</a>, <a class="tag-link-link" href="/tags/visualization/" rel="tag">visualization</a>
    </div>

                

                

            </div>
        
        
        <div class="article-entry" itemprop="articleBody">
            

            

            

            <ul>
<li>PyTorch는 현재 가장 인기있는 딥러닝 라이브러리 중 하나입니다.</li>
<li>학습 세부 사항을 지정하기 위해 Callback으로 다양한 기능을 지원합니다.</li>
<li>skorch는 PyTorch를 scikit-learn과 함께 사용할 수 있게 해 줍니다.</li>
<li>skorch도 PyTorch callback을 이용할 수 있습니다.</li>
</ul>
<ul>
<li>글이 길어 세 개로 나눕니다. </li>
<li>세 번째로, skorch를 사용해 ML Pipeline을 완성합니다.</li>
<li>여러 callback으로 자세한 설정을 반영합니다.</li>
</ul>
<h2 id="3-2-skorch"><a href="#3-2-skorch" class="headerlink" title="3.2. skorch"></a>3.2. skorch</h2><blockquote>
<p><a target="_blank" rel="noopener" href="https://bit.ly/3xxznt8">Colab code: skorch callbacks</a></p>
</blockquote>
<ul>
<li>Colab에는 skorch가 기본으로 설치되어 있지 않습니다.</li>
<li>간단한 명령으로 skorch를 설치합니다.<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!pip install skorch</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="3-2-1-preprocessor-포함-pipeline-작성"><a href="#3-2-1-preprocessor-포함-pipeline-작성" class="headerlink" title="3.2.1. preprocessor 포함 pipeline 작성"></a>3.2.1. preprocessor 포함 pipeline 작성</h3><blockquote>
<p><a target="_blank" rel="noopener" href="https://skorch.readthedocs.io/en/stable/regressor.html">skorch: skorch.regressor</a></p>
</blockquote>
<ul>
<li>앞에서 만든 <code>get_preprocessor()</code>함수는 전처리 Pipeline을 출력합니다.</li>
<li>Pipeline을 연장해 이 뒤에 PyTorch로 만든 Neural Network를 덧붙입니다.</li>
<li>우리가 푸는 문제는 펭귄의 체중을 구하는 <b>regression</b>문제입니다.</li>
</ul>
<ul>
<li><p>skorch에서 제공하는 <a target="_blank" rel="noopener" href="https://skorch.readthedocs.io/en/stable/regressor.html"><code>NeuralNetRegressor()</code></a>로 PyTorch 신경망을 감쌉니다.</p>
</li>
<li><p>loss function, optimizer 등은 <code>NeuralNetRegressor()</code> 안에 <code>criterion</code>, <code>optimizer</code> 등의 매개변수를 사용해 입력합니다.</p>
</li>
<li><p><code>optimizer=optim.Adam</code>으로 Adam을 선택했습니다. </p>
</li>
<li><p>PyTorch에서 Adam()안에 들어가던 매개변수 <code>lr=1e-3</code>은 <code>optimizer__lr=1e-3</code>으로 바뀌어 들어갑니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> skorch</span><br><span class="line"><span class="keyword">from</span> skorch <span class="keyword">import</span> NeuralNetRegressor</span><br><span class="line"></span><br><span class="line"><span class="comment"># skorch로 PyTorch neural network wrapping</span></span><br><span class="line">net_sk = NeuralNetRegressor(Net(), device=device, verbose=<span class="number">1</span>,</span><br><span class="line">                            criterion=RMSELoss,         <span class="comment"># loss function</span></span><br><span class="line">                            optimizer=optim.Adam,       <span class="comment"># optimizer</span></span><br><span class="line">                            optimizer__lr=<span class="number">1e-3</span>)         <span class="comment"># learning rate of the optimizer</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># training</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">300</span>):</span><br><span class="line">    net_sk.fit(X_train_np, y_train.astype(np.float32).values.reshape(-<span class="number">1</span>, <span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<ul>
<li>실행 결과 : 너무 길어서 out of memory 오류가 뜰 수 있습니다. 침착하게 새로 고침을 누르시면 됩니다.</li>
<li>또는, <code>verbose=1</code>을 <code>verbose=0</code>으로 바꾸시는 것도 방법입니다.<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">... (생략) ...</span><br><span class="line"></span><br><span class="line">     10      271.1115      286.5908  0.0111</span><br><span class="line">Re-initializing module.</span><br><span class="line">Re-initializing criterion.</span><br><span class="line">Re-initializing optimizer.</span><br><span class="line">  epoch    train_loss    valid_loss     dur</span><br><span class="line">-------  ------------  ------------  ------</span><br><span class="line">      1      272.8632      286.3571  0.0108</span><br><span class="line">      2      267.6815      285.4084  0.0145</span><br><span class="line">      3      268.0632      284.9375  0.0106</span><br><span class="line">      4      269.3830      284.9683  0.0104</span><br><span class="line">      5      270.7425      285.2378  0.0101</span><br><span class="line">      6      271.6377      285.5552  0.0108</span><br><span class="line">      7      271.9493      285.8589  0.0082</span><br><span class="line">      8      271.8214      286.1465  0.0080</span><br><span class="line">      9      271.4798      286.4032  0.0087</span><br><span class="line">     10      271.1165      286.5942  0.0088</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p><b>학습이 <code>model.fit()</code>으로 잘 됩니다.</b></p>
</li>
<li><p><code>NeuralNetRegressor</code>로 한 번 감싼 것 만으로 scikit-learn API를 사용할 수 있게 되었습니다.</p>
</li>
<li><p><code>X_train</code>대신 전처리를 거친 <code>X_train_np</code>를 입력했습니다.</p>
</li>
<li><p><code>y_train</code>대신으로는 <code>y_train.astype(np.float32).values.reshape(-1, 1)</code>이 들어갔습니다.</p>
</li>
<li><p>pandas Series에서 데이터 타입을 바꾸고, 값을 추출해서, shape을 바꾼 것입니다.</p>
</li>
</ul>
<ul>
<li><b>예측도 <code>model.forward()</code> 대신 <code>model.predict()</code>로 진행합니다.</b><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># prediction</span></span><br><span class="line">y_pred_train = net_sk.predict(X_train_np)</span><br><span class="line">y_pred_val = net_sk.predict(X_val_np)</span><br><span class="line">y_pred_test = net_sk.predict(X_test_np)</span><br><span class="line"></span><br><span class="line"><span class="comment"># parity plot</span></span><br><span class="line">plot_parity3(net_sk, Xs=[X_train_np, X_val_np, X_test_np])</span><br></pre></td></tr></table></figure>
<img src="10_sc_01.png"><br></li>
</ul>
<h3 id="3-2-2-ML-pipeline-작성"><a href="#3-2-2-ML-pipeline-작성" class="headerlink" title="3.2.2. ML pipeline 작성"></a>3.2.2. ML pipeline 작성</h3><blockquote>
<p><a target="_blank" rel="noopener" href="https://skorch.readthedocs.io/en/stable/callbacks.html?highlight=input_dim#skorch.callbacks.InputShapeSetter">skorch: skorch.callbacks.InputShapeSetter</a><br><a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/60005715/how-to-pass-input-dim-from-fit-method-to-skorch-wrapper/60170023#60170023">stackoverflow: How to pass input dim from fit method to skorch wrapper?</a></p>
</blockquote>
<ul>
<li>앞에서 만든 전처리기, <code>get_preprocessor()</code>를 포함하는 Pipeline을 만듭니다.</li>
<li><code>method</code>매개변수로 neural network 뿐 아니라 linear regression, random forest를 선택할 수 있게 합니다.</li>
</ul>
<ul>
<li><p>Neural network는 이들 방법들과 달리 <b>input dimension이 중요</b>합니다. </p>
</li>
<li><p>신경망 구조를 만들 때 필요한 변수이기 때문입니다.</p>
</li>
<li><p>callback은 여러 설정을 지정할 수 있는 방법입니다. <code>InputShapeSetter</code>를 callback에 기본값으로 박아 넣습니다.</p>
</li>
<li><p>그 외에 여러 keyword arguments를 입력할 수 있도록 <b><code>**kwargs</code></b>를 <code>NeuralNetRegressor()</code>에 추가합니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># machine learning models</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestRegressor</span><br><span class="line"></span><br><span class="line"><span class="comment"># embedding pytorch model in scikit-learn Pipeline</span></span><br><span class="line"><span class="keyword">from</span> skorch <span class="keyword">import</span> NeuralNetRegressor</span><br><span class="line"><span class="keyword">from</span> skorch.helper <span class="keyword">import</span> predefined_split</span><br><span class="line"><span class="keyword">from</span> skorch.callbacks <span class="keyword">import</span> Callback</span><br><span class="line"></span><br><span class="line"><span class="comment"># dynamic input size of the PyTorch module</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">InputShapeSetter</span>(<span class="params">Callback</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">on_train_begin</span>(<span class="params">self, net, X, y</span>):</span></span><br><span class="line">        net.set_params(module__ninput=X.shape[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_model</span>(<span class="params">method=<span class="string">&quot;lr&quot;</span>, device=device, cols_cat=cols_cat, cols_num=cols_num, degree=<span class="number">1</span>, </span></span></span><br><span class="line"><span class="params"><span class="function">              callbacks=[InputShapeSetter(<span class="params"></span>)], **kwargs</span>):</span></span><br><span class="line">    <span class="keyword">if</span> method == <span class="string">&quot;lr&quot;</span>:</span><br><span class="line">        ml = LinearRegression(fit_intercept=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">elif</span> method == <span class="string">&quot;rf&quot;</span>:</span><br><span class="line">        ml = RandomForestRegressor(random_state=rng)</span><br><span class="line">    <span class="keyword">elif</span> method == <span class="string">&quot;nn&quot;</span>:</span><br><span class="line">        ml = NeuralNetRegressor(Net(), device=device, callbacks=callbacks, **kwargs)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;# &#x27;method&#x27; should be in [&#x27;lr&#x27;, &#x27;rf&#x27;, &#x27;nn&#x27;].&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">    </span><br><span class="line">    preprocessor = get_preprocessor(cols_cat=cols_cat, cols_num=cols_num, degree=degree)</span><br><span class="line">    model = Pipeline([(<span class="string">&quot;preprocessor&quot;</span>, preprocessor), </span><br><span class="line">                      (<span class="string">&quot;ml&quot;</span>, ml)])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure></li>
<li><p>이제 <code>get_model()</code>을 사용해 ML pipeline을 제작할 수 있습니다.</p>
</li>
<li><p><code>method</code>에 입력하는 값에 따라 선형 회귀, 앙상블 트리, 신경망을 선택할 수 있습니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model = get_model(<span class="string">&quot;nn&quot;</span>, max_epochs=epochs, verbose=<span class="number">1</span>, criterion=RMSELoss, optimizer=optim.Adam, optimizer__lr = <span class="number">1e-3</span>)</span><br><span class="line">model</span><br></pre></td></tr></table></figure>
<p><img src="10_sc_02.png"><br></p>
</li>
<li><p>웬만한 매개변수는 모두 <code>NeuralNetRegressor()</code>에 들어갑니다.</p>
</li>
<li><p>어떤 매개변수들이 있는지 출력해서 확인합니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model[<span class="string">&quot;ml&quot;</span>].get_params()</span><br></pre></td></tr></table></figure>
<ul>
<li>실행 결과<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">&#x27;_kwargs&#x27;</span>: &#123;<span class="string">&#x27;optimizer__lr&#x27;</span>: 0.001&#125;,</span><br><span class="line"> <span class="string">&#x27;batch_size&#x27;</span>: 128,</span><br><span class="line"> <span class="string">&#x27;callbacks&#x27;</span>: [&lt;__main__.InputShapeSetter at 0x7f4911406cd0&gt;],</span><br><span class="line"> <span class="string">&#x27;callbacks__epoch_timer&#x27;</span>: &lt;skorch.callbacks.logging.EpochTimer at 0x7f49745b2a50&gt;,</span><br><span class="line"> <span class="string">&#x27;callbacks__print_log&#x27;</span>: &lt;skorch.callbacks.logging.PrintLog at 0x7f49701261d0&gt;,</span><br><span class="line"> <span class="string">&#x27;callbacks__print_log__floatfmt&#x27;</span>: <span class="string">&#x27;.4f&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;callbacks__print_log__keys_ignored&#x27;</span>: None,</span><br><span class="line"> <span class="string">&#x27;callbacks__print_log__sink&#x27;</span>: &lt;<span class="keyword">function</span> <span class="built_in">print</span>&gt;,</span><br><span class="line"> <span class="string">&#x27;callbacks__print_log__stralign&#x27;</span>: <span class="string">&#x27;right&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;callbacks__print_log__tablefmt&#x27;</span>: <span class="string">&#x27;simple&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;callbacks__train_loss&#x27;</span>: &lt;skorch.callbacks.scoring.PassthroughScoring at 0x7f49745b2450&gt;,</span><br><span class="line"> <span class="string">&#x27;callbacks__train_loss__lower_is_better&#x27;</span>: True,</span><br><span class="line"> <span class="string">&#x27;callbacks__train_loss__name&#x27;</span>: <span class="string">&#x27;train_loss&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;callbacks__train_loss__on_train&#x27;</span>: True,</span><br><span class="line"> <span class="string">&#x27;callbacks__valid_loss&#x27;</span>: &lt;skorch.callbacks.scoring.PassthroughScoring at 0x7f49745b24d0&gt;,</span><br><span class="line"> <span class="string">&#x27;callbacks__valid_loss__lower_is_better&#x27;</span>: True,</span><br><span class="line"> <span class="string">&#x27;callbacks__valid_loss__name&#x27;</span>: <span class="string">&#x27;valid_loss&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;callbacks__valid_loss__on_train&#x27;</span>: False,</span><br><span class="line"> <span class="string">&#x27;criterion&#x27;</span>: __main__.RMSELoss,</span><br><span class="line"> <span class="string">&#x27;dataset&#x27;</span>: skorch.dataset.Dataset,</span><br><span class="line"> <span class="string">&#x27;device&#x27;</span>: <span class="string">&#x27;cuda:0&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;iterator_train&#x27;</span>: torch.utils.data.dataloader.DataLoader,</span><br><span class="line"> <span class="string">&#x27;iterator_valid&#x27;</span>: torch.utils.data.dataloader.DataLoader,</span><br><span class="line"> <span class="string">&#x27;lr&#x27;</span>: 0.01,</span><br><span class="line"> <span class="string">&#x27;max_epochs&#x27;</span>: 1000,</span><br><span class="line"> <span class="string">&#x27;module&#x27;</span>: Net(</span><br><span class="line">   (layer0): Linear(in_features=12, out_features=16, bias=True)</span><br><span class="line">   (layer1): Linear(in_features=16, out_features=16, bias=True)</span><br><span class="line">   (layer2): Linear(in_features=16, out_features=12, bias=True)</span><br><span class="line">   (layer3): Linear(in_features=12, out_features=8, bias=True)</span><br><span class="line">   (layer4): Linear(in_features=8, out_features=1, bias=True)</span><br><span class="line">   (activation): ReLU()</span><br><span class="line"> ),</span><br><span class="line"> <span class="string">&#x27;optimizer&#x27;</span>: torch.optim.adam.Adam,</span><br><span class="line"> <span class="string">&#x27;optimizer__lr&#x27;</span>: 0.001,</span><br><span class="line"> <span class="string">&#x27;predict_nonlinearity&#x27;</span>: <span class="string">&#x27;auto&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;train_split&#x27;</span>: &lt;skorch.dataset.ValidSplit object at 0x7f497434cc90&gt;,</span><br><span class="line"> <span class="string">&#x27;verbose&#x27;</span>: 1,</span><br><span class="line"> <span class="string">&#x27;warm_start&#x27;</span>: False&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<h3 id="3-2-3-train-and-validate-self"><a href="#3-2-3-train-and-validate-self" class="headerlink" title="3.2.3. train and validate (self)"></a>3.2.3. train and validate (self)</h3><blockquote>
<p><a target="_blank" rel="noopener" href="https://skorch.readthedocs.io/en/stable/user/neuralnet.html#train-split">skorch: NeuralNet#train_split</a></p>
</blockquote>
<ul>
<li><p>X_train과 y_train만 사용해서 학습시킵니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.fit(X_train, y_train.values.reshape(-<span class="number">1</span>, <span class="number">1</span>).astype(np.float32))</span><br></pre></td></tr></table></figure>
<ul>
<li>실행 결과<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Re-initializing module because the following parameters were re-set: module__ninput.</span><br><span class="line">Re-initializing criterion.</span><br><span class="line">Re-initializing optimizer.</span><br><span class="line">  epoch    train_loss    valid_loss     dur</span><br><span class="line">-------  ------------  ------------  ------</span><br><span class="line">      1     4280.8532     4388.5093  0.0188</span><br><span class="line">      2     4280.8443     4388.4990  0.0129</span><br><span class="line">      </span><br><span class="line">      ... (생략) ...</span><br><span class="line">      </span><br><span class="line">    998      295.5285      270.6504  0.0106</span><br><span class="line">    999      295.5276      270.6288  0.0153</span><br><span class="line">   1000      295.5247      270.6030  0.0102</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>validation set을 입력하지 않았음에도 valid_loss가 출력됩니다.</p>
</li>
<li><p><b>train data의 20%를 validation set으로 따로 떼어 놓기 때문입니다.</b></p>
</li>
<li><p><a href="https://jehyunlee.github.io/2022/06/09/Python-DL-8-skorch_callback/">맨 처음</a> 전체 데이터의 60%만 train set으로 지정했습니다.</p>
</li>
<li><p>여기서 다시 80%만 학습에 투입되었으니 <b>총 48%.</b> 반도 안되는 데이터로 학습한 셈입니다.</p>
</li>
<li><p><code>train_split=None</code>을 입력하면 모든 데이터를 다 학습에 투입하지만 validation 결과가 출력되지 않습니다.</p>
</li>
</ul>
<ul>
<li><p><b>learning curve</b>는 신경망에서 <b><code>.history</code></b> 속성을 추출해 확인할 수 있습니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">history = model[<span class="string">&quot;ml&quot;</span>].history</span><br><span class="line">train_loss = history[:, <span class="string">&quot;train_loss&quot;</span>]</span><br><span class="line">valid_loss = history[:, <span class="string">&quot;valid_loss&quot;</span>]</span><br><span class="line"></span><br><span class="line">plot_epoch(train_loss, valid_loss)</span><br></pre></td></tr></table></figure>
<p><img src="10_sc_03.png"><br></p>
</li>
<li><p>학습도 정상적으로 이루어졌습니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_parity3(model)</span><br></pre></td></tr></table></figure>
<p><img src="10_sc_04.png"><br></p>
</li>
</ul>
<h3 id="3-2-4-train-and-validate-predefined-validation-set"><a href="#3-2-4-train-and-validate-predefined-validation-set" class="headerlink" title="3.2.4. train and validate (predefined validation set)"></a>3.2.4. train and validate (predefined validation set)</h3><blockquote>
<p><a target="_blank" rel="noopener" href="https://skorch.readthedocs.io/en/stable/dataset.html">skorch: skorch.dataset.Dataset</a></p>
</blockquote>
<ul>
<li><p>먼저 준비한 validation set을 사용하려면 <code>train_split</code>에 validation set을 입력합니다.</p>
</li>
<li><p>validation set은 skorch의 <a target="_blank" rel="noopener" href="https://skorch.readthedocs.io/en/stable/dataset.html"><code>Dataset</code></a>을 사용해 만듭니다.</p>
</li>
<li><p>내친 김에 y data도 ML Pipeline에 만들기 좋은 형태, 즉 float32, (-1, 1) shape으로 변경해서 모아놓습니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> skorch.dataset <span class="keyword">import</span> Dataset</span><br><span class="line"></span><br><span class="line"><span class="comment"># ys (numpy)</span></span><br><span class="line">y_train_np = y_train.values.reshape(-<span class="number">1</span>, <span class="number">1</span>).astype(np.float32)</span><br><span class="line">y_val_np = y_val.values.reshape(-<span class="number">1</span>, <span class="number">1</span>).astype(np.float32)</span><br><span class="line">y_test_np = y_test.values.reshape(-<span class="number">1</span>, <span class="number">1</span>).astype(np.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># predefined validation set</span></span><br><span class="line">preprocessor = get_preprocessor()</span><br><span class="line">X_val_pp = preprocessor.fit(X_train).transform(X_val)</span><br><span class="line">valid_dataset = Dataset(X_val_pp, y_val_np)</span><br><span class="line"></span><br><span class="line"><span class="comment"># model training</span></span><br><span class="line">model = get_model(<span class="string">&quot;nn&quot;</span>, max_epochs=epochs, verbose=<span class="number">1</span>, criterion=RMSELoss, optimizer=optim.Adam, optimizer__lr = <span class="number">1e-3</span>,</span><br><span class="line">                  <span class="comment"># predefined validataion set</span></span><br><span class="line">                  train_split=predefined_split(valid_dataset))  </span><br><span class="line">model.fit(X_train, y_train_np)</span><br></pre></td></tr></table></figure>
<ul>
<li>실행 결과<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Re-initializing module because the following parameters were re-set: module__ninput.</span><br><span class="line">Re-initializing criterion.</span><br><span class="line">Re-initializing optimizer.</span><br><span class="line">  epoch    train_loss    valid_loss     dur</span><br><span class="line">-------  ------------  ------------  ------</span><br><span class="line">      1     4302.1941     4250.5781  0.0160</span><br><span class="line">      2     4302.1851     4250.5693  0.0175</span><br><span class="line">      </span><br><span class="line">      ... (생략) ...</span><br><span class="line">      </span><br><span class="line">    998      280.9618      267.7317  0.0121</span><br><span class="line">    999      280.9596      267.7302  0.0122</span><br><span class="line">   1000      280.9580      267.7280  0.0141</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p><b>learning curve</b>를 확인합니다.</p>
</li>
<li><p>여기서 얻은 learning curve를 reference로 사용하겠습니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">history = model[<span class="string">&quot;ml&quot;</span>].history</span><br><span class="line">train_loss_0 = history[:, <span class="string">&quot;train_loss&quot;</span>]</span><br><span class="line">valid_loss_0 = history[:, <span class="string">&quot;valid_loss&quot;</span>]</span><br><span class="line"></span><br><span class="line">plot_epoch(train_loss_0, valid_loss_0)</span><br></pre></td></tr></table></figure>
<p><img src="10_sc_05.png"><br></p>
</li>
<li><p><b>parity plot</b>도 확인합니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_parity3(model)</span><br></pre></td></tr></table></figure>
<p><img src="10_sc_06.png"><br></p>
</li>
</ul>
<h3 id="3-2-5-learning-rate-scheduler"><a href="#3-2-5-learning-rate-scheduler" class="headerlink" title="3.2.5. learning rate scheduler"></a>3.2.5. learning rate scheduler</h3><blockquote>
<p><a target="_blank" rel="noopener" href="https://skorch.readthedocs.io/en/stable/user/callbacks.html#learning-rate-schedulers">skorch: Learning rate schedulers</a><br><a href="https://jehyunlee.github.io/2020/03/28/Python-DL-3-1cycle-learning-rate-policy/">Pega Devlog: Fast.ai의 fit_one_cycle 방법론 이해</a></p>
</blockquote>
<ul>
<li><p>callbacks에 learning late scheduler를 추가해 learning rate를 조정할 수 있습니다.</p>
</li>
<li><p>input dimension 조정을 위해 <b>callbacks 기본값으로 InputShapeSetter()가 들어가 있습니다.</b></p>
</li>
<li><p>이를 삭제하지 않도록 유의하면서 learning rate scheduler를 추가합니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> skorch.callbacks <span class="keyword">import</span> LRScheduler</span><br><span class="line"></span><br><span class="line">model = get_model(<span class="string">&quot;nn&quot;</span>, max_epochs=epochs, verbose=<span class="number">1</span>, criterion=RMSELoss, optimizer=optim.Adam, optimizer__lr = <span class="number">1e-3</span>,</span><br><span class="line">                  train_split=predefined_split(valid_dataset),               <span class="comment"># predefined validataion set                </span></span><br><span class="line">                  callbacks=[<span class="comment"># input dimension setter</span></span><br><span class="line">                             (<span class="string">&quot;input_shape_setter&quot;</span>, InputShapeSetter()),</span><br><span class="line">                             </span><br><span class="line">                             <span class="comment"># LR scheduler</span></span><br><span class="line">                             (<span class="string">&quot;lr_scheduler&quot;</span>, LRScheduler(policy=OneCycleLR, <span class="comment"># LR scheduler</span></span><br><span class="line">                                                         max_lr=<span class="number">0.1</span>,</span><br><span class="line">                                                         total_steps=epochs))])</span><br><span class="line">model.fit(X_train, y_train_np)</span><br></pre></td></tr></table></figure></li>
<li><p>학습 과정은 생략하고 learning curve를 비교해서 봅니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">ax = plot_epoch(train_loss_0, valid_loss_0)</span><br><span class="line">lines = ax.lines</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> lines:</span><br><span class="line">    line.set_alpha(<span class="number">0.3</span>)</span><br><span class="line"></span><br><span class="line">history = model[<span class="string">&quot;ml&quot;</span>].history</span><br><span class="line">train_loss = history[:, <span class="string">&quot;train_loss&quot;</span>]</span><br><span class="line">valid_loss = history[:, <span class="string">&quot;valid_loss&quot;</span>]</span><br><span class="line"></span><br><span class="line">ax = plot_epoch(train_loss, valid_loss, ax=ax)</span><br><span class="line">handles, labels = ax.get_legend_handles_labels()</span><br><span class="line">ax.legend(handles=handles, labels=labels, ncol=<span class="number">2</span>, title=<span class="string">&quot;base      OneCycleLR&quot;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="10_sc_07.png"><br></p>
</li>
<li><p>learning rate scheduler가 적용되어 학습 양상이 바뀌었습니다.</p>
</li>
<li><p>parity plot도 여전히 좋습니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_parity3(model)</span><br></pre></td></tr></table></figure>
<p><img src="10_sc_08.png"><br></p>
</li>
</ul>
<h3 id="3-2-6-early-stopping"><a href="#3-2-6-early-stopping" class="headerlink" title="3.2.6. early stopping"></a>3.2.6. early stopping</h3><blockquote>
<p><a target="_blank" rel="noopener" href="https://skorch.readthedocs.io/en/stable/callbacks.html#skorch.callbacks.EarlyStopping">skorch: skorch.callbacks.EarlyStopping</a></p>
</blockquote>
<ul>
<li><p>불필요하게 학습을 길게 하는 경향을 줄이고자 early stopping을 적용합니다.</p>
</li>
<li><p><code>EarlyStopping()</code>을 callbacks에 추가하고 기준을 설정합니다.</p>
</li>
<li><p>valid_loss가 20번 줄어들지 않으면 학습을 중단하도록 <code>monitor=&quot;valid_loss&quot;</code>, <code>patience=20</code>을 설정했습니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> skorch.callbacks <span class="keyword">import</span> EarlyStopping</span><br><span class="line"></span><br><span class="line">model = get_model(<span class="string">&quot;nn&quot;</span>, max_epochs=epochs, verbose=<span class="number">1</span>, criterion=RMSELoss, optimizer=optim.Adam, optimizer__lr = <span class="number">1e-3</span>,</span><br><span class="line">                  train_split=predefined_split(valid_dataset),               <span class="comment"># predefined validataion set        </span></span><br><span class="line"></span><br><span class="line">                  callbacks=[<span class="comment"># input dimension setter</span></span><br><span class="line">                             (<span class="string">&quot;input_shape_setter&quot;</span>, InputShapeSetter()),</span><br><span class="line"></span><br><span class="line">                             <span class="comment"># LR scheduler</span></span><br><span class="line">                             (<span class="string">&quot;lr_scheduler&quot;</span>, LRScheduler(policy=OneCycleLR, </span><br><span class="line">                                                         max_lr=<span class="number">0.1</span>,</span><br><span class="line">                                                         total_steps=epochs)),</span><br><span class="line"></span><br><span class="line">                             <span class="comment"># early stopping</span></span><br><span class="line">                             (<span class="string">&quot;early_stopping&quot;</span>, EarlyStopping(monitor=<span class="string">&quot;valid_loss&quot;</span>,</span><br><span class="line">                                                              patience=<span class="number">20</span>))])</span><br><span class="line">model.fit(X_train, y_train.values.reshape(-<span class="number">1</span>, <span class="number">1</span>).astype(np.float32))</span><br></pre></td></tr></table></figure></li>
<li><p><b>learning curve</b>를 비교합니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">ax = plot_epoch(train_loss_0, valid_loss_0)</span><br><span class="line">lines = ax.lines</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> lines:</span><br><span class="line">    line.set_alpha(<span class="number">0.3</span>)</span><br><span class="line"></span><br><span class="line">history = model[<span class="string">&quot;ml&quot;</span>].history</span><br><span class="line">train_loss = history[:, <span class="string">&quot;train_loss&quot;</span>]</span><br><span class="line">valid_loss = history[:, <span class="string">&quot;valid_loss&quot;</span>]</span><br><span class="line"></span><br><span class="line">ax = plot_epoch(train_loss, valid_loss, ax=ax)</span><br><span class="line">handles, labels = ax.get_legend_handles_labels()</span><br><span class="line">ax.legend(handles=handles, labels=labels, ncol=<span class="number">2</span>, title=<span class="string">&quot;base      LRS+ES&quot;</span>)</span><br><span class="line">ax.set_xlim(<span class="number">0</span>, <span class="number">200</span>)</span><br></pre></td></tr></table></figure>
<p><img src="10_sc_09.png"><br></p>
</li>
<li><p>parity plot도 정상적입니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_parity3(model)</span><br></pre></td></tr></table></figure>
<p><img src="10_sc_10.png"><br></p>
</li>
</ul>
<h3 id="3-2-7-Saving-and-Loading-manual"><a href="#3-2-7-Saving-and-Loading-manual" class="headerlink" title="3.2.7. Saving and Loading (manual)"></a>3.2.7. Saving and Loading (manual)</h3><blockquote>
<p><a target="_blank" rel="noopener" href="https://skorch.readthedocs.io/en/stable/user/save_load.html">skorch: Saving and Loading</a></p>
</blockquote>
<ul>
<li><p>모델 파라미터를 pickle 형식으로 저장하고 불러올 수 있습니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># save parameters</span></span><br><span class="line">model[<span class="string">&quot;ml&quot;</span>].save_params(f_params=<span class="string">&quot;nn_params.pkl&quot;</span>)</span><br></pre></td></tr></table></figure></li>
<li><p>모델을 새로 만들면 paramter를 불러오기 전에 초기화하는 과정이 필요합니다.</p>
</li>
<li><p><code>.initialize()</code>를 사용합니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># load parameters</span></span><br><span class="line">model_new = get_model(method=<span class="string">&quot;nn&quot;</span>)</span><br><span class="line">model_new[<span class="string">&quot;ml&quot;</span>].initialize()</span><br><span class="line">model_new[<span class="string">&quot;ml&quot;</span>].load_params(f_params=<span class="string">&quot;nn_params.pkl&quot;</span>)</span><br></pre></td></tr></table></figure></li>
<li><p>새로 만든 모델에 parameter를 불러와서 적용합니다.</p>
</li>
<li><p>단, 신경망 parameter에만 해당하는 상황이므로 preprocessor는 기존 모델의 preprocessor를 사용합니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># check reproducibility</span></span><br><span class="line">y_pred_train = model_new[<span class="string">&quot;ml&quot;</span>].predict(model[<span class="string">&quot;preprocessor&quot;</span>].transform(X_train))</span><br><span class="line">y_pred_val = model_new[<span class="string">&quot;ml&quot;</span>].predict(model[<span class="string">&quot;preprocessor&quot;</span>].transform(X_val))</span><br><span class="line">y_pred_test = model_new[<span class="string">&quot;ml&quot;</span>].predict(model[<span class="string">&quot;preprocessor&quot;</span>].transform(X_test))</span><br><span class="line"></span><br><span class="line">plot_parity3(model_new, preds=[y_pred_train, y_pred_val, y_pred_test])</span><br></pre></td></tr></table></figure>
<p><img src="10_sc_11.png"></p>
</li>
<li><p>학습을 하지 않았음에도 parity plot이 똑같이 재현되었습니다.</p>
</li>
</ul>
<h3 id="3-2-8-Saving-and-Loading-callbacks"><a href="#3-2-8-Saving-and-Loading-callbacks" class="headerlink" title="3.2.8. Saving and Loading (callbacks)"></a>3.2.8. Saving and Loading (callbacks)</h3><blockquote>
<p><a target="_blank" rel="noopener" href="https://skorch.readthedocs.io/en/stable/user/save_load.html#using-callbacks">skorch: Saving and Loading Using Callbacks</a></p>
</blockquote>
<ul>
<li><p>callback을 이용하면 valid loss가 줄어들때마다 저장할 수 있습니다.</p>
</li>
<li><p>valid loss가 줄어들때마다 저장하는 <code>cp</code>와 epoch마다 저장하는 <code>train_end_cp</code>를 동시에 지정합니다.</p>
</li>
<li><p><code>exp1</code> 폴더를 만들어 여기에 함께 저장하도록 합니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> skorch.callbacks <span class="keyword">import</span> Checkpoint, TrainEndCheckpoint</span><br><span class="line"></span><br><span class="line"><span class="comment"># save the model parameters, optimizer, and history</span></span><br><span class="line">cp = Checkpoint(dirname=<span class="string">&#x27;exp1&#x27;</span>)</span><br><span class="line">train_end_cp = TrainEndCheckpoint(dirname=<span class="string">&#x27;exp1&#x27;</span>)</span><br><span class="line"></span><br><span class="line">model = get_model(<span class="string">&quot;nn&quot;</span>, max_epochs=epochs, verbose=<span class="number">1</span>, criterion=RMSELoss, optimizer=optim.Adam, optimizer__lr = <span class="number">1e-3</span>,</span><br><span class="line">                  train_split=predefined_split(valid_dataset),                               </span><br><span class="line"></span><br><span class="line">                  callbacks=[<span class="comment"># input dimension setter</span></span><br><span class="line">                             (<span class="string">&quot;input_shape_setter&quot;</span>, InputShapeSetter()),</span><br><span class="line"></span><br><span class="line">                             <span class="comment"># LR scheduler</span></span><br><span class="line">                             (<span class="string">&quot;lr_scheduler&quot;</span>, LRScheduler(policy=OneCycleLR, </span><br><span class="line">                                                         max_lr=<span class="number">0.1</span>,</span><br><span class="line">                                                         total_steps=epochs)),</span><br><span class="line"></span><br><span class="line">                             <span class="comment"># early stopping</span></span><br><span class="line">                             (<span class="string">&quot;early_stopping&quot;</span>, EarlyStopping(monitor=<span class="string">&quot;valid_loss&quot;</span>,</span><br><span class="line">                                                              patience=<span class="number">20</span>)),</span><br><span class="line">                             </span><br><span class="line">                             <span class="comment"># Checkpoints</span></span><br><span class="line">                             (<span class="string">&quot;checkpoint&quot;</span>, cp),</span><br><span class="line">                             (<span class="string">&quot;train_end_checkpoint&quot;</span>, train_end_cp)</span><br><span class="line">                             ])</span><br><span class="line">model.fit(X_train, y_train.values.reshape(-<span class="number">1</span>, <span class="number">1</span>).astype(np.float32))</span><br></pre></td></tr></table></figure>
<ul>
<li>실행 결과<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">Re-initializing module because the following parameters were re-set: module__ninput.</span><br><span class="line">Re-initializing criterion.</span><br><span class="line">Re-initializing optimizer.</span><br><span class="line">  epoch    train_loss    valid_loss    cp      lr     dur</span><br><span class="line">-------  ------------  ------------  ----  ------  ------</span><br><span class="line">      1     4301.9097     4250.2661     +  0.0040  0.0093</span><br><span class="line">      2     4301.8677     4250.2241     +  0.0040  0.0107</span><br><span class="line">      3     4301.8278     4250.1914     +  0.0040  0.0083</span><br><span class="line">      4     4301.7949     4250.1577     +  0.0040  0.0111</span><br><span class="line">      </span><br><span class="line">      ... (생략) ...</span><br><span class="line">      </span><br><span class="line">     96      294.3520      265.7108        0.0260  0.0112</span><br><span class="line">     97      296.6454      273.7495        0.0264  0.0098</span><br><span class="line">     98      290.3884      289.0570        0.0268  0.0071</span><br><span class="line">     99      289.6089      273.4566        0.0273  0.0093</span><br><span class="line">    100      284.6194      261.8893     +  0.0277  0.0092</span><br><span class="line">    101      290.8631      262.4448        0.0281  0.0107</span><br><span class="line">    102      289.3116      275.2869        0.0286  0.0077</span><br><span class="line">    103      286.5198      278.3972        0.0290  0.0075</span><br><span class="line">    104      283.5437      266.5125        0.0295  0.0106</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p><b>cp</b>라는 열이 하나 생겼고, 여기 +가 붙은 곳들이 있습니다.</p>
</li>
<li><p>valid_loss가 기존 기록보다 작아진 지점입니다.</p>
</li>
</ul>
<ul>
<li><p>History도 파일에서 불러와서 그립니다.</p>
</li>
<li><p>부를 때는 <code>skorch.history.History</code>를 사용합니다.</p>
</li>
<li><p>세 개의 learning curve를 겹쳐 그리느라 코드가 다소 복잡해졌습니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> skorch.history <span class="keyword">import</span> History</span><br><span class="line"></span><br><span class="line"><span class="comment"># base plot</span></span><br><span class="line">ax = plot_epoch(train_loss_0, valid_loss_0)</span><br><span class="line">lines = ax.lines</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> lines:</span><br><span class="line">    line.set_alpha(<span class="number">0.3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># history</span></span><br><span class="line">history = History().from_file(<span class="string">&quot;./exp1/history.json&quot;</span>)</span><br><span class="line">train_loss = history[:, <span class="string">&quot;train_loss&quot;</span>]</span><br><span class="line">valid_loss = history[:, <span class="string">&quot;valid_loss&quot;</span>]</span><br><span class="line">ax = plot_epoch(train_loss, valid_loss, ax=ax)</span><br><span class="line"></span><br><span class="line"><span class="comment"># event_cp : cp == True</span></span><br><span class="line">epoch = history[:, <span class="string">&quot;epoch&quot;</span>]</span><br><span class="line">event_cp = history[:, <span class="string">&quot;event_cp&quot;</span>]</span><br><span class="line">df_cp = pd.DataFrame(&#123;<span class="string">&quot;epoch&quot;</span>:epoch, <span class="string">&quot;event_cp&quot;</span>:event_cp, <span class="string">&quot;train_loss&quot;</span>:train_loss, <span class="string">&quot;valid_loss&quot;</span>:valid_loss&#125;)</span><br><span class="line">df_cp = df_cp.loc[df_cp[<span class="string">&quot;event_cp&quot;</span>]==<span class="literal">True</span>]</span><br><span class="line"></span><br><span class="line">ax.scatter(df_cp[<span class="string">&quot;epoch&quot;</span>], df_cp[<span class="string">&quot;train_loss&quot;</span>], fc=c_train, alpha=<span class="number">0.5</span>, label=<span class="string">&quot;train_cp&quot;</span>)</span><br><span class="line">ax.scatter(df_cp[<span class="string">&quot;epoch&quot;</span>], df_cp[<span class="string">&quot;valid_loss&quot;</span>], fc=c_val, alpha=<span class="number">0.5</span>, label=<span class="string">&quot;valid_cp&quot;</span>)</span><br><span class="line"></span><br><span class="line">ax.legend(ncol=<span class="number">3</span>, title=<span class="string">&quot;base      LRS+ES           checkpoint&quot;</span>, loc=<span class="string">&quot;center right&quot;</span>)</span><br><span class="line">ax.set_xlim(<span class="number">0</span>, <span class="number">200</span>)</span><br></pre></td></tr></table></figure>
<p><img src="10_sc_12.png"><br></p>
</li>
<li><p>희미하게 그려진 것은 skorch에 validation set을 사용한 base line입니다.</p>
</li>
<li><p>그리고 Learning Rate Scheduler와 Early Stopping을 적용한 것을 LRS + ES로 표기했습니다.</p>
</li>
<li><p>앞에서와 같이 학습이 훨씬 빨리 끝났습니다.</p>
</li>
<li><p>그리고 이 중 <b>checkpoint가 적용된 것</b>을 scatter plot으로 표현했습니다.</p>
</li>
<li><p>one-cycle-fit의 영향으로 learning curve가 요동치는 와중에서도 train과 valid에서 단조 감소하는 모습만이 기록되었습니다.</p>
</li>
</ul>
<h3 id="3-2-9-valid-loss가-가장-적었던-checkpoint-불러서-learning-rate-낮추기"><a href="#3-2-9-valid-loss가-가장-적었던-checkpoint-불러서-learning-rate-낮추기" class="headerlink" title="3.2.9. valid loss가 가장 적었던 checkpoint 불러서 learning rate 낮추기"></a>3.2.9. valid loss가 가장 적었던 checkpoint 불러서 learning rate 낮추기</h3><blockquote>
<p><a target="_blank" rel="noopener" href="https://skorch.readthedocs.io/en/stable/callbacks.html#skorch.callbacks.LoadInitState">skorch: skorch.callbacks.LoadInitState</a></p>
</blockquote>
<ul>
<li><p>각 checkpoint에서는 history 외에도 criterion, optimizer, parameter 등의 상태를 저장합니다.</p>
</li>
<li><p>Colab 왼쪽의 폴더 모양을 클릭해 저장한 파일을 확인하면 볼 수 있습니다.<br><br><img src="10_sc_13.png"><br></p>
</li>
<li><p>학습이 과하게 진행되어 overfitting이 되면 지나가버린 과거의 최적점이 아쉽습니다.</p>
</li>
<li><p>지나친 최적점을 불러와서 훨씬 낮은 learning rate로 살살 학습시키면 더 좋을 것 같습니다.</p>
</li>
</ul>
<ul>
<li>이 때 skorch에서 제공하는 <a target="_blank" rel="noopener" href="https://skorch.readthedocs.io/en/stable/callbacks.html#skorch.callbacks.LoadInitState">LoadInitState</a>를 사용할 수 있습니다.</li>
<li><code>cp = Checkpoint()</code>로 저장된 위치를 지정하고,</li>
<li><code>load_state = LoadInitState(cp)</code>로 불러와 상태를 불러옵니다.</li>
<li>마지막으로 <code>callbacks</code>에 <code>cp</code>와 <code>load_state</code>를 추가합니다.<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> skorch.callbacks <span class="keyword">import</span> LoadInitState</span><br><span class="line"></span><br><span class="line">cp = Checkpoint(dirname=<span class="string">&#x27;exp1&#x27;</span>)</span><br><span class="line">load_state = LoadInitState(cp)</span><br><span class="line"></span><br><span class="line">model = get_model(<span class="string">&quot;nn&quot;</span>, max_epochs=epochs, verbose=<span class="number">1</span>, criterion=RMSELoss, optimizer=optim.Adam, </span><br><span class="line">                  <span class="comment"># learning rate 조정</span></span><br><span class="line">                  optimizer__lr = <span class="number">1e-5</span>,</span><br><span class="line"></span><br><span class="line">                  <span class="comment"># predefined validataion set</span></span><br><span class="line">                  train_split=predefined_split(valid_dataset),                               </span><br><span class="line"></span><br><span class="line">                  callbacks=[<span class="comment"># input dimension setter</span></span><br><span class="line">                             (<span class="string">&quot;input_shape_setter&quot;</span>, InputShapeSetter()),</span><br><span class="line"></span><br><span class="line">                             <span class="comment"># early stopping</span></span><br><span class="line">                             (<span class="string">&quot;early_stopping&quot;</span>, EarlyStopping(monitor=<span class="string">&quot;valid_loss&quot;</span>,</span><br><span class="line">                                                              patience=<span class="number">100</span>)),</span><br><span class="line">                             </span><br><span class="line">                             <span class="comment"># Checkpoints</span></span><br><span class="line">                             (<span class="string">&quot;checkpoint&quot;</span>, cp),</span><br><span class="line">                             (<span class="string">&quot;load_initial_state&quot;</span>, load_state)</span><br><span class="line">                             ])</span><br><span class="line">model.fit(X_train, y_train.values.reshape(-<span class="number">1</span>, <span class="number">1</span>).astype(np.float32))</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="3-2-10-Saving-and-Loading-model-itself"><a href="#3-2-10-Saving-and-Loading-model-itself" class="headerlink" title="3.2.10. Saving and Loading (model itself)"></a>3.2.10. Saving and Loading (model itself)</h3><ul>
<li><p>모델 전체를 저장할 때는 pickle을 권장하고 있습니다.</p>
</li>
<li><p><code>pickle.dump()</code>와 <code>pickle.load()</code>를 사용해 모델을 읽고 씁니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># saving</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;skorch_dl.pkl&#x27;</span>, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    pickle.dump(model, f)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># loading</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;skorch_dl.pkl&#x27;</span>, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    model_pkl = pickle.load(f)</span><br></pre></td></tr></table></figure></li>
<li><p>저장한 모델을 불러오면서 이름을 model_pkl로 바꿨습니다.</p>
</li>
<li><p>이 모델의 parity plot을 그려서 잘 저장되었고 불러졌는지 확인합니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># check reproducibility</span></span><br><span class="line">plot_parity3(model=model_pkl)</span><br></pre></td></tr></table></figure>
<p><img src="10_sc_14.png"><br></p>
</li>
<li><p>추가 학습 없이도 원래의 성능이 확인되었습니다.</p>
</li>
</ul>
<h1 id="4-정리-skorch-ML-pipeline"><a href="#4-정리-skorch-ML-pipeline" class="headerlink" title="4. 정리 : skorch ML pipeline"></a>4. 정리 : skorch ML pipeline</h1><ul>
<li>이제까지 세 편의 글에 걸쳐 데이터를 정리한 후,</li>
<li><b>scikit-learn preprocessor</b>를 만들고,</li>
<li><b>PyTorch neural network</b>를 구축한 후,</li>
<li><b>skorch로 이들을 엮은 뒤 callbacks로 여러 옵션을 뿌렸습니다.</b></li>
</ul>
<ul>
<li>최종적으로 사용한 코드가 여기 저기 흩뿌려져 있어 활용이 어려울 듯도 싶습니다.</li>
<li>skorch ML pipeline 코드를 아래에 정리합니다.</li>
<li><b>목적은 Ctrl+C/V</b>와 약간의 수정으로 사용하는 것입니다.</li>
</ul>
<h2 id="4-1-skorch-ML-pipeline"><a href="#4-1-skorch-ML-pipeline" class="headerlink" title="4.1. skorch ML pipeline"></a>4.1. skorch ML pipeline</h2><ul>
<li><p>scikit-learn preprocessor</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># preprocessors</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OneHotEncoder</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> FunctionTransformer</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> PolynomialFeatures</span><br><span class="line"></span><br><span class="line"><span class="comment"># pipeline</span></span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"><span class="keyword">from</span> sklearn.compose <span class="keyword">import</span> ColumnTransformer</span><br><span class="line"><span class="keyword">from</span> sklearn.base <span class="keyword">import</span> BaseEstimator, TransformerMixin</span><br><span class="line"></span><br><span class="line"><span class="comment"># Preprocessings for Categorical and Numerical features</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_concat</span>(<span class="params">cols_cat=cols_cat, cols_num=cols_num, degree=<span class="number">1</span></span>):</span></span><br><span class="line">    <span class="comment"># categorical features: one-hot encoding</span></span><br><span class="line">    cat_features = cols_cat</span><br><span class="line">    cat_transformer = OneHotEncoder(sparse=<span class="literal">False</span>, handle_unknown=<span class="string">&quot;ignore&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># numerical features: standard scaling &amp; polynomial features</span></span><br><span class="line">    num_features = cols_num</span><br><span class="line">    num_transformer = Pipeline(steps=[(<span class="string">&quot;polynomial&quot;</span>, PolynomialFeatures(degree=degree)),</span><br><span class="line">                                      (<span class="string">&quot;scaler&quot;</span>, StandardScaler())])</span><br><span class="line">    </span><br><span class="line">    numcat = ColumnTransformer(transformers=[(<span class="string">&quot;categorical&quot;</span>, cat_transformer, cat_features),</span><br><span class="line">                                          (<span class="string">&quot;numerical&quot;</span>, num_transformer, num_features)])</span><br><span class="line">    <span class="keyword">return</span> numcat</span><br><span class="line"></span><br><span class="line"><span class="comment"># Float64 to Float32 for PyTorch</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FloatTransformer</span>(<span class="params">BaseEstimator, TransformerMixin</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span>(<span class="params">self, X, y=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform</span>(<span class="params">self, X</span>):</span></span><br><span class="line">        <span class="keyword">return</span> np.array(X, dtype=np.float32)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># preprocessing Pipeline</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_preprocessor</span>(<span class="params">cols_cat=cols_cat, cols_num=cols_num, degree=<span class="number">1</span></span>):</span></span><br><span class="line">    concat = get_concat(cols_cat=cols_cat, cols_num=cols_num, degree=degree)</span><br><span class="line">    ft = FloatTransformer()</span><br><span class="line"></span><br><span class="line">    pipeline= Pipeline(steps=[(<span class="string">&quot;concat&quot;</span>, concat), </span><br><span class="line">                              (<span class="string">&quot;float64to32&quot;</span>, ft)])</span><br><span class="line">    <span class="keyword">return</span> pipeline</span><br></pre></td></tr></table></figure></li>
<li><p>PyTorch Neural Network</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># neural network: ninput(12)-16-16-12-8-1</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, ninput=<span class="number">12</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.layer0 = nn.Linear(ninput, <span class="number">16</span>)</span><br><span class="line">        self.layer1 = nn.Linear(<span class="number">16</span>, <span class="number">16</span>)</span><br><span class="line">        self.layer2 = nn.Linear(<span class="number">16</span>, <span class="number">12</span>)</span><br><span class="line">        self.layer3 = nn.Linear(<span class="number">12</span>, <span class="number">8</span>)</span><br><span class="line">        self.layer4 = nn.Linear(<span class="number">8</span>, <span class="number">1</span>)</span><br><span class="line">        self.activation = nn.ReLU()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.activation(self.layer0(x))</span><br><span class="line">        x = self.activation(self.layer1(x))</span><br><span class="line">        x = self.activation(self.layer2(x))</span><br><span class="line">        x = self.activation(self.layer3(x))</span><br><span class="line">        x = self.layer4(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># loss: RMSE</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RMSELoss</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, eps=<span class="number">1e-6</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.mse = nn.MSELoss()</span><br><span class="line">        self.eps = eps</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, true, pred</span>):</span></span><br><span class="line">        loss = torch.sqrt(self.mse(true, pred) + self.eps)</span><br><span class="line">        <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure></li>
<li><p>skorch ML Pipeline</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># machine learning models</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestRegressor</span><br><span class="line"></span><br><span class="line"><span class="comment"># embedding pytorch model in scikit-learn Pipeline</span></span><br><span class="line"><span class="keyword">from</span> skorch <span class="keyword">import</span> NeuralNetRegressor</span><br><span class="line"><span class="keyword">from</span> skorch.helper <span class="keyword">import</span> predefined_split</span><br><span class="line"></span><br><span class="line"><span class="comment"># callbacks</span></span><br><span class="line"><span class="keyword">from</span> skorch.callbacks <span class="keyword">import</span> Callback</span><br><span class="line"><span class="keyword">from</span> skorch.callbacks <span class="keyword">import</span> LRScheduler</span><br><span class="line"><span class="keyword">from</span> skorch.callbacks <span class="keyword">import</span> EarlyStopping</span><br><span class="line"><span class="keyword">from</span> skorch.callbacks <span class="keyword">import</span> Checkpoint, TrainEndCheckpoint</span><br><span class="line"></span><br><span class="line"><span class="comment"># dynamic input size of the PyTorch module</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">InputShapeSetter</span>(<span class="params">Callback</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">on_train_begin</span>(<span class="params">self, net, X, y</span>):</span></span><br><span class="line">        net.set_params(module__ninput=X.shape[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># save the model parameters, optimizer, and history</span></span><br><span class="line">cp = Checkpoint(dirname=<span class="string">&#x27;exp_test&#x27;</span>)</span><br><span class="line">train_end_cp = TrainEndCheckpoint(dirname=<span class="string">&#x27;exp_test&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># skorch ML pipeline</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_model</span>(<span class="params">method=<span class="string">&quot;lr&quot;</span>, device=device, cols_cat=cols_cat, cols_num=cols_num, degree=<span class="number">1</span>, </span></span></span><br><span class="line"><span class="params"><span class="function">              callbacks=[(<span class="params"><span class="string">&quot;input_shape_setter&quot;</span>, InputShapeSetter(<span class="params"></span>)</span>),</span></span></span><br><span class="line"><span class="params"><span class="function">                         (<span class="params"><span class="string">&quot;lr_scheduler&quot;</span>, LRScheduler(<span class="params">policy=OneCycleLR, max_lr=<span class="number">0.1</span>, total_steps=epochs</span>)</span>),</span></span></span><br><span class="line"><span class="params"><span class="function">                         (<span class="params"><span class="string">&quot;early_stopping&quot;</span>, EarlyStopping(<span class="params">monitor=<span class="string">&quot;valid_loss&quot;</span>, patience=<span class="number">20</span></span>)</span>),</span></span></span><br><span class="line"><span class="params"><span class="function">                         (<span class="params"><span class="string">&quot;checkpoint&quot;</span>, cp</span>), (<span class="params"><span class="string">&quot;train_end_checkpoint&quot;</span>, train_end_cp</span>)], </span></span></span><br><span class="line"><span class="params"><span class="function">              **kwargs</span>):</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> method == <span class="string">&quot;lr&quot;</span>:</span><br><span class="line">        ml = LinearRegression(fit_intercept=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">elif</span> method == <span class="string">&quot;rf&quot;</span>:</span><br><span class="line">        ml = RandomForestRegressor(random_state=rng)</span><br><span class="line">    <span class="keyword">elif</span> method == <span class="string">&quot;nn&quot;</span>:</span><br><span class="line">        ml = NeuralNetRegressor(Net(), device=device, callbacks=callbacks, **kwargs)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;# &#x27;method&#x27; should be in [&#x27;lr&#x27;, &#x27;rf&#x27;, &#x27;nn&#x27;].&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">    </span><br><span class="line">    preprocessor = get_preprocessor(cols_cat=cols_cat, cols_num=cols_num, degree=degree)</span><br><span class="line">    model = Pipeline([(<span class="string">&quot;preprocessor&quot;</span>, preprocessor), </span><br><span class="line">                      (<span class="string">&quot;ml&quot;</span>, ml)])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="4-2-Visualizations"><a href="#4-2-Visualizations" class="headerlink" title="4.2. Visualizations"></a>4.2. Visualizations</h2><ul>
<li><p>learning curve</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_epoch</span>(<span class="params">history=<span class="literal">None</span>, loss_trains=<span class="literal">None</span>, loss_vals=<span class="literal">None</span>, ax=<span class="literal">None</span></span>):</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">any</span>([history, loss_trains]) == <span class="literal">False</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;# one of &#x27;history&#x27; and &#x27;loss_trains&#x27; has to be used!&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> ax</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> ax == <span class="literal">None</span>:</span><br><span class="line">        fig, ax = plt.subplots(figsize=(<span class="number">10</span>, <span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> loss_trains == <span class="literal">None</span>:</span><br><span class="line">        loss_trains = history[:, <span class="string">&quot;train_loss&quot;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> history != <span class="literal">None</span> <span class="keyword">and</span> loss_vals == <span class="literal">None</span>:</span><br><span class="line">        loss_vals = history[:, <span class="string">&quot;valid_loss&quot;</span>]</span><br><span class="line"></span><br><span class="line">    ax.plot(<span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(loss_trains)+<span class="number">1</span>)), loss_trains, c=c_train, label=<span class="string">&quot;train&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> loss_vals != <span class="literal">None</span>:</span><br><span class="line">        ax.plot(<span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(loss_vals)+<span class="number">1</span>)), loss_vals, c=c_val, label=<span class="string">&quot;valid&quot;</span>)</span><br><span class="line">    ax.grid(axis=<span class="string">&quot;y&quot;</span>)</span><br><span class="line">    ax.set_xlabel(<span class="string">&quot;epochs&quot;</span>, fontdict=font_label)</span><br><span class="line">    ax.legend()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> ax</span><br></pre></td></tr></table></figure></li>
<li><p>parity plots</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_parity3</span>(<span class="params">model, target=[<span class="string">&quot;train&quot;</span>, <span class="string">&quot;val&quot;</span>, <span class="string">&quot;test&quot;</span>], figsize=(<span class="params"><span class="number">10</span>, <span class="number">4</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="function">                 Xs=<span class="literal">None</span>, trues=<span class="literal">None</span>, preds=<span class="literal">None</span>, colors=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> Xs:</span><br><span class="line">        Xs = [<span class="built_in">eval</span>(<span class="string">f&quot;X_<span class="subst">&#123;t&#125;</span>&quot;</span>) <span class="keyword">for</span> t <span class="keyword">in</span> target]</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> trues:</span><br><span class="line">        trues = [<span class="built_in">eval</span>(<span class="string">f&quot;y_<span class="subst">&#123;t&#125;</span>&quot;</span>) <span class="keyword">for</span> t <span class="keyword">in</span> target]</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> preds:</span><br><span class="line">        preds = [model.predict(X) <span class="keyword">for</span> X <span class="keyword">in</span> Xs]</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> colors:</span><br><span class="line">        colors = [<span class="built_in">eval</span>(<span class="string">f&quot;c_<span class="subst">&#123;t&#125;</span>&quot;</span>) <span class="keyword">for</span> t <span class="keyword">in</span> target]</span><br><span class="line"></span><br><span class="line">    fig, axs = plt.subplots(ncols=<span class="built_in">len</span>(target), figsize=figsize, constrained_layout=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">for</span> ax, true, pred, c, title <span class="keyword">in</span> <span class="built_in">zip</span>(axs, trues, preds, colors, titles):</span><br><span class="line">        plot_parity(true, pred, ax=ax, scatter_kws=&#123;<span class="string">&quot;fc&quot;</span>:c, <span class="string">&quot;ec&quot;</span>:c, <span class="string">&quot;alpha&quot;</span>:<span class="number">0.5</span>&#125;, title=title)</span><br><span class="line">        <span class="keyword">if</span> ax != axs[<span class="number">0</span>]:</span><br><span class="line">            ax.set_ylabel(<span class="string">&quot;&quot;</span>)</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="4-3-test-run"><a href="#4-3-test-run" class="headerlink" title="4.3. test run"></a>4.3. test run</h2><ul>
<li><p>정의한 함수들로 예제를 돌려봅니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># predefined validation set</span></span><br><span class="line">preprocessor = get_preprocessor()</span><br><span class="line">X_val_pp = preprocessor.fit(X_train).transform(X_val)</span><br><span class="line">valid_dataset = Dataset(X_val_pp, y_val_np)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ML pipeline preparation</span></span><br><span class="line">model_test = get_model(<span class="string">&quot;nn&quot;</span>, max_epochs=epochs, verbose=<span class="number">1</span>, criterion=RMSELoss, optimizer=optim.Adam, optimizer__lr = <span class="number">1e-3</span>,</span><br><span class="line">                       train_split=predefined_split(valid_dataset))</span><br><span class="line">model_test.fit(X_train, y_train_np)</span><br><span class="line"></span><br><span class="line"><span class="comment"># learning curve</span></span><br><span class="line">history = History().from_file(<span class="string">&quot;./exp_test/history.json&quot;</span>)</span><br><span class="line">ax = plot_epoch(history)</span><br><span class="line"></span><br><span class="line"><span class="comment"># parity plots</span></span><br><span class="line">plot_parity3(model_test)</span><br></pre></td></tr></table></figure>
<ul>
<li>실행 결과<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Re-initializing module because the following parameters were re-set: module__ninput.</span><br><span class="line">Re-initializing criterion.</span><br><span class="line">Re-initializing optimizer.</span><br><span class="line">  epoch    train_loss    valid_loss    cp      lr     dur</span><br><span class="line">-------  ------------  ------------  ----  ------  ------</span><br><span class="line">      1     4301.9874     4250.3501     +  0.0040  0.0135</span><br><span class="line">      2     4301.9537     4250.3169     +  0.0040  0.0139</span><br><span class="line">      </span><br><span class="line">      ... (생략) ...</span><br><span class="line">      </span><br><span class="line">    110      283.9839      270.7625        0.0322  0.0108</span><br><span class="line">    111      282.3574      270.6689        0.0326  0.0145</span><br><span class="line">Stopping since valid_loss has not improved <span class="keyword">in</span> the last 20 epochs.</span><br></pre></td></tr></table></figure>
<img src="10_sc_15.png"><br><img src="10_sc_16.png"></li>
</ul>
</li>
<li><p><b>잘 돌아갑니다. :)</b></p>
</li>
<li><p>전체를 실행해볼 수 있는 코드는 여기 있습니다: <a target="_blank" rel="noopener" href="https://bit.ly/3xxznt8">Notebook</a></p>
</li>
</ul>

        </div>
		
        <footer class="article-footer">
            



    <a data-url="https://jehyunlee.github.io/2022/06/09/Python-DL-10-skorch_callback3/" data-id="cl46woapp014w9wtqgw5b8p8z" class="article-share-link"><i class="fa fa-share"></i>Share</a>
<script>
    (function ($) {
        $('body').on('click', function() {
            $('.article-share-box.on').removeClass('on');
        }).on('click', '.article-share-link', function(e) {
            e.stopPropagation();

            var $this = $(this),
                url = $this.attr('data-url'),
                encodedUrl = encodeURIComponent(url),
                id = 'article-share-box-' + $this.attr('data-id'),
                offset = $this.offset(),
                box;

            if ($('#' + id).length) {
                box = $('#' + id);

                if (box.hasClass('on')){
                    box.removeClass('on');
                    return;
                }
            } else {
                var html = [
                    '<div id="' + id + '" class="article-share-box">',
                        '<input class="article-share-input" value="' + url + '">',
                        '<div class="article-share-links">',
                            '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="article-share-twitter" target="_blank" title="Twitter"></a>',
                            '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="article-share-facebook" target="_blank" title="Facebook"></a>',
                            '<a href="http://pinterest.com/pin/create/button/?url=' + encodedUrl + '" class="article-share-pinterest" target="_blank" title="Pinterest"></a>',
                            '<a href="https://plus.google.com/share?url=' + encodedUrl + '" class="article-share-google" target="_blank" title="Google+"></a>',
                        '</div>',
                    '</div>'
                ].join('');

              box = $(html);

              $('body').append(box);
            }

            $('.article-share-box.on').hide();

            box.css({
                top: offset.top + 25,
                left: offset.left
            }).addClass('on');

        }).on('click', '.article-share-box', function (e) {
            e.stopPropagation();
        }).on('click', '.article-share-box-input', function () {
            $(this).select();
        }).on('click', '.article-share-box-link', function (e) {
            e.preventDefault();
            e.stopPropagation();

            window.open(this.href, 'article-share-box-window-' + Date.now(), 'width=500,height=450');
        });
    })(jQuery);
</script>

        </footer>
    </div>
    <script type="application/ld+json">
    {
        "@context": "https://schema.org",
        "@type": "BlogPosting",
        "author": {
            "@type": "Person",
            "name": "Jehyun Lee"
        },
        "headline": "skorch callbacks (3) ML Pipeline",
        "image": "https://jehyunlee.github.io/thumbnails/Python-DL/10_sc_00.png",
        "keywords": "pytorch sklearn pipeline neural network callback visualization",
        "genre": "Python Deep Learning",
        "datePublished": "2022-06-09",
        "dateCreated": "2022-06-09",
        "dateModified": "2022-06-10",
        "url": "https://jehyunlee.github.io/2022/06/09/Python-DL-10-skorch_callback3/",
        "description": "
PyTorch는 현재 가장 인기있는 딥러닝 라이브러리 중 하나입니다.
학습 세부 사항을 지정하기 위해 Callback으로 다양한 기능을 지원합니다.
skorch는 PyTorch를 scikit-learn과 함께 사용할 수 있게 해 줍니다.
skorch도 PyTorch callback을 이용할 수 있습니다.


글이 길어 세 개로 나눕니다. 
세 번째로, s",
        "wordCount": 7531
    }
</script>

</article>



                        </div>
                    </section>
                    <aside id="sidebar">
    <a class="sidebar-toggle" title="Expand Sidebar"><i class="toggle icon"></i></a>
    <div class="sidebar-top">
        <p>follow:</p>
        <ul class="social-links">
            
                
                <li>
                    <a class="social-tooltip" title="github" href="https://github.com/jehyunlee" target="_blank" rel="noopener">
                        <i class="icon fa fa-github"></i>
                    </a>
                </li>
                
            
                
                <li>
                    <a class="social-tooltip" title="rss" href="https://jehyunlee.github.io/rss2.xml" target="_blank" rel="noopener">
                        <i class="icon fa fa-rss"></i>
                    </a>
                </li>
                
            
                
                <li>
                    <a class="social-tooltip" title="instagram" href="https://www.instagram.com/jehyunlee20/" target="_blank" rel="noopener">
                        <i class="icon fa fa-instagram"></i>
                    </a>
                </li>
                
            
        </ul>
    </div>
    
        
<nav id="article-nav">
    
    
        <a href="/2022/06/09/Python-DL-9-skorch_callback2/" id="article-nav-older" class="article-nav-link-wrap">
        <strong class="article-nav-caption">older</strong>
        <p class="article-nav-title">skorch callbacks (2) sklearn preprocesing + PyTorch neural network</p>
        <i class="icon fa fa-chevron-left" id="icon-chevron-left"></i>
        </a>
    
</nav>

    
    <div class="widgets-container">
        
            
                

            
                
    <div class="widget-wrap">
        <h3 class="widget-title">recents</h3>
        <div class="widget">
            <ul id="recent-post" class="">
                
                    <li>
                        
                        <div class="item-thumbnail">
                            <a href="/2022/06/09/Python-DL-10-skorch_callback3/" class="thumbnail">
    
    
        <span style="background-image:url(/thumbnails/Python-DL/10_sc_00.png)" alt="skorch callbacks (3) ML Pipeline" class="thumbnail-image"></span>
    
    
</a>

                        </div>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/Python/">Python</a><i class="icon fa fa-angle-right"></i><a class="article-category-link" href="/categories/Python/Deep-Learning/">Deep Learning</a></p>
                            <p class="item-title"><a href="/2022/06/09/Python-DL-10-skorch_callback3/" class="title">skorch callbacks (3) ML Pipeline</a></p>
                            <p class="item-date"><time datetime="2022-06-09T10:50:00.000Z" itemprop="datePublished">2022-06-09</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-thumbnail">
                            <a href="/2022/06/09/Python-DL-9-skorch_callback2/" class="thumbnail">
    
    
        <span style="background-image:url(/thumbnails/Python-DL/9_sc_00.png)" alt="skorch callbacks (2) sklearn preprocesing + PyTorch neural network" class="thumbnail-image"></span>
    
    
</a>

                        </div>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/Python/">Python</a><i class="icon fa fa-angle-right"></i><a class="article-category-link" href="/categories/Python/Deep-Learning/">Deep Learning</a></p>
                            <p class="item-title"><a href="/2022/06/09/Python-DL-9-skorch_callback2/" class="title">skorch callbacks (2) sklearn preprocesing + PyTorch neural network</a></p>
                            <p class="item-date"><time datetime="2022-06-09T01:28:00.000Z" itemprop="datePublished">2022-06-09</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-thumbnail">
                            <a href="/2022/06/09/Python-DL-8-skorch_callback/" class="thumbnail">
    
    
        <span style="background-image:url(/thumbnails/Python-DL/8_sc_00.png)" alt="skorch callbacks (1) dataset preparation" class="thumbnail-image"></span>
    
    
</a>

                        </div>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/Python/">Python</a><i class="icon fa fa-angle-right"></i><a class="article-category-link" href="/categories/Python/Deep-Learning/">Deep Learning</a></p>
                            <p class="item-title"><a href="/2022/06/09/Python-DL-8-skorch_callback/" class="title">skorch callbacks (1) dataset preparation</a></p>
                            <p class="item-date"><time datetime="2022-06-08T21:16:00.000Z" itemprop="datePublished">2022-06-09</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-thumbnail">
                            <a href="/2022/06/06/Python-DS-103-snsreglmplot/" class="thumbnail">
    
    
        <span style="background-image:url(/thumbnails/Python-DS/103_snsreglmplot_00.png)" alt="seaborn regplot vs lmplot" class="thumbnail-image"></span>
    
    
</a>

                        </div>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/Python/">Python</a><i class="icon fa fa-angle-right"></i><a class="article-category-link" href="/categories/Python/Data-Science/">Data Science</a></p>
                            <p class="item-title"><a href="/2022/06/06/Python-DS-103-snsreglmplot/" class="title">seaborn regplot vs lmplot</a></p>
                            <p class="item-date"><time datetime="2022-06-06T09:51:00.000Z" itemprop="datePublished">2022-06-06</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-thumbnail">
                            <a href="/2022/05/27/Python-DS-102-kdeplot/" class="thumbnail">
    
    
        <span style="background-image:url(/thumbnails/Python-DS/102_kdeplot_00.png)" alt="short discussions on KDE plot" class="thumbnail-image"></span>
    
    
</a>

                        </div>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/Python/">Python</a><i class="icon fa fa-angle-right"></i><a class="article-category-link" href="/categories/Python/Data-Science/">Data Science</a></p>
                            <p class="item-title"><a href="/2022/05/27/Python-DS-102-kdeplot/" class="title">short discussions on KDE plot</a></p>
                            <p class="item-date"><time datetime="2022-05-26T23:51:00.000Z" itemprop="datePublished">2022-05-27</time></p>
                        </div>
                    </li>
                
            </ul>
        </div>
    </div>

            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">categories</h3>
        <div class="widget">
            <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/GIS/">GIS</a><span class="category-list-count">7</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/GIS/Python/">Python</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/GIS/QGIS/">QGIS</a><span class="category-list-count">2</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/ImageJ/">ImageJ</a><span class="category-list-count">12</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/ImageJ/Cookbook/">Cookbook</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/ImageJ/Tutorial/">Tutorial</a><span class="category-list-count">8</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/">Python</a><span class="category-list-count">121</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Python/Data-Science/">Data Science</a><span class="category-list-count">101</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/Deep-Learning/">Deep Learning</a><span class="category-list-count">10</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/General/">General</a><span class="category-list-count">9</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/Physics/">Physics</a><span class="category-list-count">1</span></li></ul></li></ul>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-float">
        <h3 class="widget-title">tag cloud</h3>
        <div class="widget tagcloud">
            <a href="/tags/1-cycle-learning-rate-policy/" style="font-size: 10px;">1 cycle learning rate policy</a> <a href="/tags/3D/" style="font-size: 12.31px;">3D</a> <a href="/tags/AI-Factory/" style="font-size: 10px;">AI Factory</a> <a href="/tags/AI-Frenz/" style="font-size: 12.31px;">AI Frenz</a> <a href="/tags/AI-festival/" style="font-size: 10.77px;">AI festival</a> <a href="/tags/C-level/" style="font-size: 10px;">C-level</a> <a href="/tags/Daejeon-Learning-Day/" style="font-size: 10px;">Daejeon Learning Day</a> <a href="/tags/Daejeon-Science-Festival/" style="font-size: 10px;">Daejeon Science Festival</a> <a href="/tags/Data-Scienctist/" style="font-size: 10px;">Data Scienctist</a> <a href="/tags/FacetGrid/" style="font-size: 10px;">FacetGrid</a> <a href="/tags/GPU/" style="font-size: 10px;">GPU</a> <a href="/tags/Google-Colab/" style="font-size: 10px;">Google Colab</a> <a href="/tags/Google-Form/" style="font-size: 10px;">Google Form</a> <a href="/tags/HelloDD/" style="font-size: 10px;">HelloDD</a> <a href="/tags/KIER/" style="font-size: 11.54px;">KIER</a> <a href="/tags/NIA/" style="font-size: 10.77px;">NIA</a> <a href="/tags/NIA-data-story/" style="font-size: 10px;">NIA data story</a> <a href="/tags/Nicolas-P-Rougier/" style="font-size: 10.77px;">Nicolas P. Rougier</a> <a href="/tags/PCA/" style="font-size: 10px;">PCA</a> <a href="/tags/PairGrid/" style="font-size: 10px;">PairGrid</a> <a href="/tags/VOSviewer/" style="font-size: 10.77px;">VOSviewer</a> <a href="/tags/X-STEM/" style="font-size: 10px;">X-STEM</a> <a href="/tags/align/" style="font-size: 10px;">align</a> <a href="/tags/annotate/" style="font-size: 10px;">annotate</a> <a href="/tags/art/" style="font-size: 10px;">art</a> <a href="/tags/article/" style="font-size: 10px;">article</a> <a href="/tags/ascii/" style="font-size: 10px;">ascii</a> <a href="/tags/atom/" style="font-size: 10.77px;">atom</a> <a href="/tags/automation/" style="font-size: 10px;">automation</a> <a href="/tags/axes/" style="font-size: 10px;">axes</a> <a href="/tags/batch/" style="font-size: 10px;">batch</a> <a href="/tags/batch-normalization/" style="font-size: 10.77px;">batch normalization</a> <a href="/tags/bresenham/" style="font-size: 10px;">bresenham</a> <a href="/tags/calendar/" style="font-size: 10.77px;">calendar</a> <a href="/tags/calibration/" style="font-size: 10px;">calibration</a> <a href="/tags/callback/" style="font-size: 11.54px;">callback</a> <a href="/tags/capstyle/" style="font-size: 10px;">capstyle</a> <a href="/tags/channel-threshold/" style="font-size: 10px;">channel threshold</a> <a href="/tags/chrome-remote-desktop/" style="font-size: 10px;">chrome remote desktop</a> <a href="/tags/class/" style="font-size: 10px;">class</a> <a href="/tags/classification/" style="font-size: 10px;">classification</a> <a href="/tags/code-states/" style="font-size: 10px;">code states</a> <a href="/tags/cognitive-science/" style="font-size: 11.54px;">cognitive science</a> <a href="/tags/color/" style="font-size: 16.15px;">color</a> <a href="/tags/colorbar/" style="font-size: 13.08px;">colorbar</a> <a href="/tags/colormap/" style="font-size: 15.38px;">colormap</a> <a href="/tags/colorsys/" style="font-size: 10px;">colorsys</a> <a href="/tags/confidence-interval/" style="font-size: 10px;">confidence interval</a> <a href="/tags/convolution/" style="font-size: 10px;">convolution</a> <a href="/tags/cookbook/" style="font-size: 10.77px;">cookbook</a> <a href="/tags/csv/" style="font-size: 10px;">csv</a> <a href="/tags/data-augmentation/" style="font-size: 10px;">data augmentation</a> <a href="/tags/data-cleansing/" style="font-size: 10px;">data cleansing</a> <a href="/tags/data-generation/" style="font-size: 10.77px;">data generation</a> <a href="/tags/data-preprocessing/" style="font-size: 10px;">data preprocessing</a> <a href="/tags/data-sampling/" style="font-size: 10.77px;">data sampling</a> <a href="/tags/datetime/" style="font-size: 11.54px;">datetime</a> <a href="/tags/deep-learning/" style="font-size: 10px;">deep learning</a> <a href="/tags/displot/" style="font-size: 10px;">displot</a> <a href="/tags/docker/" style="font-size: 10px;">docker</a> <a href="/tags/environment/" style="font-size: 10px;">environment</a> <a href="/tags/error-bar/" style="font-size: 10px;">error bar</a> <a href="/tags/fast-ai/" style="font-size: 10px;">fast.ai</a> <a href="/tags/font/" style="font-size: 10.77px;">font</a> <a href="/tags/geopandas/" style="font-size: 10px;">geopandas</a> <a href="/tags/get-data/" style="font-size: 10px;">get_data</a> <a href="/tags/get-lines/" style="font-size: 10px;">get_lines</a> <a href="/tags/gibbs-sampling/" style="font-size: 10px;">gibbs sampling</a> <a href="/tags/gis/" style="font-size: 14.62px;">gis</a> <a href="/tags/github/" style="font-size: 10px;">github</a> <a href="/tags/gitignore/" style="font-size: 10px;">gitignore</a> <a href="/tags/google-map/" style="font-size: 10px;">google map</a> <a href="/tags/google-trends/" style="font-size: 10px;">google trends</a> <a href="/tags/grid/" style="font-size: 10px;">grid</a> <a href="/tags/gridspec/" style="font-size: 10px;">gridspec</a> <a href="/tags/image/" style="font-size: 12.31px;">image</a> <a href="/tags/image-file/" style="font-size: 10px;">image file</a> <a href="/tags/imagej/" style="font-size: 16.92px;">imagej</a> <a href="/tags/inspection/" style="font-size: 10px;">inspection</a> <a href="/tags/installation/" style="font-size: 10px;">installation</a> <a href="/tags/io/" style="font-size: 10px;">io</a> <a href="/tags/isomap/" style="font-size: 10px;">isomap</a> <a href="/tags/joinstyle/" style="font-size: 10px;">joinstyle</a> <a href="/tags/jointplot/" style="font-size: 10px;">jointplot</a> <a href="/tags/jupyter-lab/" style="font-size: 10.77px;">jupyter lab</a> <a href="/tags/jython/" style="font-size: 13.08px;">jython</a> <a href="/tags/kdeplot/" style="font-size: 10px;">kdeplot</a> <a href="/tags/keras/" style="font-size: 10.77px;">keras</a> <a href="/tags/keras-learing-day/" style="font-size: 10px;">keras learing day</a> <a href="/tags/learning-rate/" style="font-size: 10px;">learning rate</a> <a href="/tags/legend/" style="font-size: 11.54px;">legend</a> <a href="/tags/linear-algebra/" style="font-size: 10px;">linear algebra</a> <a href="/tags/linux/" style="font-size: 10px;">linux</a> <a href="/tags/lmplot/" style="font-size: 10px;">lmplot</a> <a href="/tags/locally-linear-embedding/" style="font-size: 10px;">locally linear embedding</a> <a href="/tags/machine-learning/" style="font-size: 13.85px;">machine learning</a> <a href="/tags/matplotlib/" style="font-size: 18.46px;">matplotlib</a> <a href="/tags/midnight-commander/" style="font-size: 10px;">midnight commander</a> <a href="/tags/minimum-oriented-rectangle/" style="font-size: 10px;">minimum oriented rectangle</a> <a href="/tags/multidimensional-scaling/" style="font-size: 10px;">multidimensional scaling</a> <a href="/tags/natural-language/" style="font-size: 10.77px;">natural language</a> <a href="/tags/naver-map/" style="font-size: 10px;">naver map</a> <a href="/tags/neural-network/" style="font-size: 12.31px;">neural network</a> <a href="/tags/numpy/" style="font-size: 11.54px;">numpy</a> <a href="/tags/object-minimum-bounding-box/" style="font-size: 10px;">object minimum bounding box</a> <a href="/tags/open-API/" style="font-size: 10px;">open API</a> <a href="/tags/pairplot/" style="font-size: 10px;">pairplot</a> <a href="/tags/pandas/" style="font-size: 16.15px;">pandas</a> <a href="/tags/paraview/" style="font-size: 10.77px;">paraview</a> <a href="/tags/patches/" style="font-size: 10px;">patches</a> <a href="/tags/person-correlation/" style="font-size: 10px;">person correlation</a> <a href="/tags/pipeline/" style="font-size: 13.08px;">pipeline</a> <a href="/tags/polygon/" style="font-size: 11.54px;">polygon</a> <a href="/tags/powerpoint/" style="font-size: 10px;">powerpoint</a> <a href="/tags/presentation/" style="font-size: 17.69px;">presentation</a> <a href="/tags/principal-component-analysis/" style="font-size: 10px;">principal component analysis</a> <a href="/tags/probability/" style="font-size: 10.77px;">probability</a> <a href="/tags/proj/" style="font-size: 10px;">proj</a> <a href="/tags/pycon/" style="font-size: 10px;">pycon</a> <a href="/tags/pysolar/" style="font-size: 10.77px;">pysolar</a> <a href="/tags/python/" style="font-size: 20px;">python</a> <a href="/tags/pytorch/" style="font-size: 13.08px;">pytorch</a> <a href="/tags/qgis/" style="font-size: 10.77px;">qgis</a> <a href="/tags/raster/" style="font-size: 10.77px;">raster</a> <a href="/tags/regplot/" style="font-size: 10px;">regplot</a> <a href="/tags/ridge-map/" style="font-size: 10px;">ridge-map</a> <a href="/tags/ridgeplot/" style="font-size: 10.77px;">ridgeplot</a> <a href="/tags/roi/" style="font-size: 10px;">roi</a> <a href="/tags/savgol/" style="font-size: 10px;">savgol</a> <a href="/tags/scatter-density/" style="font-size: 10px;">scatter-density</a> <a href="/tags/sciencedirect/" style="font-size: 10px;">sciencedirect</a> <a href="/tags/scipy/" style="font-size: 10px;">scipy</a> <a href="/tags/scopus/" style="font-size: 10px;">scopus</a> <a href="/tags/script/" style="font-size: 10px;">script</a> <a href="/tags/seaborn/" style="font-size: 17.69px;">seaborn</a> <a href="/tags/segmentation/" style="font-size: 10px;">segmentation</a> <a href="/tags/set/" style="font-size: 10px;">set</a> <a href="/tags/shadow/" style="font-size: 10px;">shadow</a> <a href="/tags/shapefile/" style="font-size: 10.77px;">shapefile</a> <a href="/tags/signal-processing/" style="font-size: 12.31px;">signal processing</a> <a href="/tags/sklearn/" style="font-size: 13.85px;">sklearn</a> <a href="/tags/spines/" style="font-size: 10px;">spines</a> <a href="/tags/statistics/" style="font-size: 13.85px;">statistics</a> <a href="/tags/streamgraph/" style="font-size: 10px;">streamgraph</a> <a href="/tags/subplots/" style="font-size: 10.77px;">subplots</a> <a href="/tags/t-SNE/" style="font-size: 10px;">t-SNE</a> <a href="/tags/tensorflow/" style="font-size: 11.54px;">tensorflow</a> <a href="/tags/text/" style="font-size: 10px;">text</a> <a href="/tags/text-mining/" style="font-size: 10.77px;">text mining</a> <a href="/tags/translation/" style="font-size: 11.54px;">translation</a> <a href="/tags/uncertainty/" style="font-size: 10px;">uncertainty</a> <a href="/tags/unicode/" style="font-size: 10px;">unicode</a> <a href="/tags/vector/" style="font-size: 10px;">vector</a> <a href="/tags/vesta/" style="font-size: 10px;">vesta</a> <a href="/tags/visualization/" style="font-size: 19.23px;">visualization</a> <a href="/tags/wsl/" style="font-size: 10px;">wsl</a> <a href="/tags/x-window/" style="font-size: 10px;">x-window</a> <a href="/tags/xception/" style="font-size: 10px;">xception</a> <a href="/tags/youth/" style="font-size: 10px;">youth</a>
        </div>
    </div>


            
        
    </div>
</aside>

                </div>
            </div>
        </div>
        <footer id="footer">
    <div class="container">
        <div class="container-inner">
            <a id="back-to-top" href="javascript:;"><i class="icon fa fa-angle-up"></i></a>
            <div class="credit">
                <h1 class="logo-wrap">
                    <a href="/" class="logo"></a>
                </h1>
                <p>&copy; 2022 Jehyun Lee</p>
                
                <p>Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>. Theme by <a href="https://github.com/ppoffice" target="_blank">PPOffice</a></p>
                
            </div>
            <div class="footer-plugins">
              
    


            </div>
        </div>
    </div>
</footer>

    </div>
    
    
    <script>
    var disqus_shortname = 'hexo-theme-hueman';
    
    
    var disqus_url = 'https://jehyunlee.github.io/2022/06/09/Python-DL-10-skorch_callback3/';
    
    (function() {
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/count.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
    </script>





    
        
<script src="/libs/lightgallery/js/lightgallery.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-thumbnail.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-pager.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-autoplay.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-fullscreen.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-zoom.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-hash.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-share.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-video.min.js"></script>

    
    
        
<script src="/libs/justified-gallery/jquery.justifiedGallery.min.js"></script>

    
    
        <script type="text/x-mathjax-config">
            MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
        </script>
        
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML.js"></script>

    

    
    



<!-- Custom Scripts -->

<script src="/js/main.js"></script>


</body>
</html>
